[
  {
    "file_name": "aisp_0101.png",
    "caption": "A Venn diagram illustrating the intersection of hardware, software, and algorithms, emphasizing their collaborative role in AI systems performance engineering.",
    "description": "A Venn diagram illustrating the intersection of hardware, software, and algorithms, emphasizing their collaborative role in AI systems performance engineering.",
    "chapter": "The AI Systems Performance Engineer",
    "page_context": "An AI systems performance engineer commands top salaries—and for very good reasons. Our work has a clear impact on the bottom line. We blend expertise across hardware, software, and algorithms. We must understand low-level OS considerations, memory hierarchies, networking fundamentals, and multiple languages like Python and C++, as well as different AI frameworks and libraries such as PyTorch, OpenAI’s Triton, and NVIDIA’s Compute Unified Device Architecture (CUDA). ... On any given day, an AI systems performance engineer might be examining low-level GPU kernel efficiency, optimizing OS thread scheduling, analyzing memory access patterns, increasing network throughput efficiency, or debugging distributed training algorithms. Key responsibilities of an AI systems performance engineer include benchmarking, profiling, debugging, optimizing, scaling, and managing resources efficiently. And while performance engineers may specialize in a combination of hardware, software, and algorithms, the point is that these specializations need to be codesigned together (see Figure 1-1). As such, it’s good to understand their trade-offs and how they affect one another. ... ###### Figure 1-1. Codesigning hardware, software, and algorithms ... ## Benchmarking and Profiling"
  },
  {
    "file_name": "aisp_0102.png",
    "caption": "Bar chart comparing NVIDIA GB200 NVL72's training performance per GPU to an equivalent NVIDIA Hopper, showing 2.2x, 2.5x, and 2.6x improvements across different tasks.",
    "description": "Bar chart comparing NVIDIA GB200 NVL72's training performance per GPU to an equivalent NVIDIA Hopper, showing 2.2x, 2.5x, and 2.6x improvements across different tasks.",
    "chapter": "Transparency and Reproducibility",
    "page_context": "Open efforts like DeepSeek’s Open Infra Index provide valuable baselines and tools. They provide real-world performance measurements on various AI hardware setups and encourage apples-to-apples comparisons and reproducibility. Similarly, the MLPerf open benchmark suite provides a standard for reproducibly comparing training and inference performance across hardware and software setups. ... Industry benchmarks such as MLPerf have quantified these kinds of codesigned optimizations across hardware generations. In MLPerf Training v5.0 (2025), a Blackwell-based NVIDIA GB200 NVL72 system produced up to 2.6× higher training throughput per GPU over an equivalent Hopper system, as shown in Figure 1-2. In MLPerf Inference v5.0 (2025), the Blackwell NVL72 achieved about 3.4× higher inference throughput per GPU over an equivalent Hopper cluster due to higher per-GPU performance and the much larger NVLink domain. These results are shown in Figure 1-3. ... ###### Figure 1-2. NVIDIA GB200 NVL72 MLPerf Training v5.0 per-GPU throughput improvement over an equivalent NVIDIA Hopper cluster (source: https://oreil.ly/Ao1l8) ... ![Bar chart comparing relative per-GPU performance improvements of NVIDIA GB200 NVL72 over H200 NVL8 in offline and server scenarios, indicating increases of 2.8× and 3.4×, respectively.](images/aisp_0103.png)"
  },
  {
    "file_name": "aisp_0103.png",
    "caption": "Bar chart comparing relative per-GPU performance improvements of NVIDIA GB200 NVL72 over H200 NVL8 in offline and server scenarios, indicating increases of 2.8× and 3.4×, respectively.",
    "description": "Bar chart comparing relative per-GPU performance improvements of NVIDIA GB200 NVL72 over H200 NVL8 in offline and server scenarios, indicating increases of 2.8× and 3.4×, respectively.",
    "chapter": "Transparency and Reproducibility",
    "page_context": "![Bar chart comparing NVIDIA GB200 NVL72's training performance per GPU to an equivalent NVIDIA Hopper, showing 2.2x, 2.5x, and 2.6x improvements across different tasks.](images/aisp_0102.png) ... ###### Figure 1-2. NVIDIA GB200 NVL72 MLPerf Training v5.0 per-GPU throughput improvement over an equivalent NVIDIA Hopper cluster (source: https://oreil.ly/Ao1l8) ... ###### Figure 1-3. NVIDIA GB200 NVL72 MLPerf Inference v5.0 per-GPU throughput improvement over an equivalent NVIDIA Hopper cluster (source: https://oreil.ly/V-jze) ... In later chapters, we’ll reference some of these open benchmarks to support various performance tuning concepts. For instance, when discussing GPU kernel optimizations, we will reference DeepSeek’s published profiles showing how their custom kernels achieved near-peak memory bandwidth utilization on NVIDIA GPUs."
  },
  {
    "file_name": "aisp_0104.png",
    "caption": "Diagram illustrating DeepSeek-V3’s expert routing mechanism, showcasing the interaction between the shared expert, routed experts, and the router in processing tokens.",
    "description": "Diagram illustrating DeepSeek-V3’s expert routing mechanism, showcasing the interaction between the shared expert, routed experts, and the router in processing tokens.",
    "chapter": "DeepSeek Scales to ~680-Billion Parameter Models Despite US Export Hardware Restrictions in China",
    "page_context": "By designing custom CUDA kernels to bypass some of the default NCCL communication collectives, DeepSeek was able to coordinate data transfers in tandem with ongoing computations. This keeps the GPUs efficiently utilized despite their reduced interconnect bandwidth. This kind of communication/computation overlap is a theme we will revisit throughout the rest of the book. ... This innovative engineering paid off, as DeepSeek-V3 was trained to completion at a fraction of the GPU time (and cost) of similarly sized frontier models from OpenAI, Meta, DeepMind, and others. This is a fraction of the resources that many assumed were necessary to train a model of this scale using a more capable cluster. ... ###### Figure 1-4. DeepSeek-V3’s expert routing ... DeepSeek reports that V3’s performance approaches GPT-4’s on several standardized benchmarks, including language understanding, reading comprehension, and reasoning. It even matches or slightly exceeds GPT-4 on some tests. These comparisons were based on standardized tests used across the industry. This implies that an open MoE model can rival the best closed models—despite using less capable hardware."
  },
  {
    "file_name": "aisp_0201.png",
    "caption": "NVIDIA Grace Blackwell Superchip module featuring a central Grace CPU and two Blackwell GPUs, designed for integrated high-speed communication and shared memory.",
    "description": "NVIDIA Grace Blackwell Superchip module featuring a central Grace CPU and two Blackwell GPUs, designed for integrated high-speed communication and shared memory.",
    "chapter": "The CPU and GPU Superchip",
    "page_context": "NVIDIA’s approach to scaling AI starts at the level of a single, combined CPU + GPU superchip module. Beginning with the Hopper generation, NVIDIA started packaging an ARM-based CPU together with one or more GPUs in the same unit, tightly linking them with a high-speed interface. The result is a single module that behaves like a unified computing engine. ... The first implementation of the superchip was Grace Hopper (GH200), which pairs one Grace CPU with one Hopper GPU. Next came the Grace Blackwell (GB200) Superchip, which pairs one Grace CPU with two Blackwell GPUs in the same package. The Grace CPU sits in the center of the module, surrounded by two Blackwell GPU dies, as shown in Figure 2-1. ... ###### Figure 2-1. NVIDIA Grace Blackwell Superchip module containing one Grace CPU (center) and two Blackwell B200 GPUs (top left and right) on a single module with a shared unified memory space and connected by a custom high-speed link called NVLink-C2C (chip-to-chip) ... In a traditional system, the CPU and GPU have separate memory pools and communicate over a relatively slow bus (like PCIe), which means data has to be copied back and forth. NVIDIA’s superchip eliminates that barrier by connecting the CPU and GPUs with a custom high-speed link called NVLink-C2C (chip-to-chip)."
  },
  {
    "file_name": "aisp_0202.png",
    "caption": "Diagram of NVIDIA's Blackwell dual-die multichip module design, highlighting two reticle-sized dies, fast memory, and a high bandwidth interface for unified GPU operation.",
    "description": "Diagram of NVIDIA's Blackwell dual-die multichip module design, highlighting two reticle-sized dies, fast memory, and a high bandwidth interface for unified GPU operation.",
    "chapter": "NVIDIA Blackwell “Dual-Die” GPU",
    "page_context": "While this section dives into the details of the dual-die architecture, the rest of the book will refer to Blackwell’s two combined GPU dies collectively as just the “Blackwell GPU.” ... This chiplet approach splits what would normally be one enormous GPU into smaller GPU dies—linking them together with a superfast, on-package die-to-die interconnect. Why do this? Because a single monolithic die is limited by manufacturing because there’s a limit to how large you can make a chip on silicon. By combining two physical GPU dies into a single module, NVIDIA can double the total transistor budget for the module. ... ###### Figure 2-2. Blackwell dual-die multichip module (MCM) design ... For the Blackwell B200 MCM, each GPU die has about 104 billion transistors and 96 GB HBM3e memory. The combined GPU module has around 208 billion transistors and 192 (180 usable) GB total memory per B200 GPU. By comparison, the Hopper H100 GPU had ~80 billion transistors and 80 GB HBM3 (versus Blackwell’s HBM3e) memory. As such, Blackwell’s B200 more than doubles transistor count and ~2.4× increases memory size."
  },
  {
    "file_name": "aisp_0203.png",
    "caption": "Bar graph showing the relative throughput increase of FP8 and FP4 compared to FP16, with FP8 at 2x and FP4 at 4x.",
    "description": "Bar graph showing the relative throughput increase of FP8 and FP4 compared to FP16, with FP8 at 2x and FP4 at 4x.",
    "chapter": "NVIDIA GPU Tensor Cores and Transformer Engine",
    "page_context": "An entire NVL72 rack (72 GPUs) has a theoretical Tensor Core throughput over 1.4 exaFLOPS (that’s 1.4 × 1018) in 4-bit precision. This is a mind-boggling number that puts this single rack in the realm of the world’s fastest supercomputers—albeit at low FP4 precision. Even if real-world workloads don’t always hit that peak, the capability is there, which is astonishing. ... Modern GPUs use a TE that adds NVFP4 support together with improved scaling and calibration. In practice, you adopt TE by using its kernels and modules in frameworks such as PyTorch. This way, FP8 and NVFP4 are applied when they preserve accuracy. This is not a fully automatic per-layer decision in all frameworks. ... ###### Figure 2-3. Relative speedup of FP8 and FP4 compared to FP16 ... Advanced techniques include dynamically changing the precision for each layer of a neural network during training and inference. The goal is to use the lowest precision that will still preserve model accuracy for each of those layers. For example, the TE might keep the first layers of a neural net in FP16 since early layers can be sensitive to noise. But, based on heuristics, it could decide to use FP8 or FP4 for later layers that are more tolerant—or for giant embedding matrices where high precision isn’t as critical."
  },
  {
    "file_name": "aisp_0204.png",
    "caption": "Diagram comparing a CPU optimized for serial tasks with a GPU optimized for parallel tasks, illustrating their different core structures.",
    "description": "Diagram comparing a CPU optimized for serial tasks with a GPU optimized for parallel tasks, illustrating their different core structures.",
    "chapter": "Streaming Multiprocessor, Threads, and Warps",
    "page_context": "## Streaming Multiprocessor, Threads, and Warps ... Each Blackwell GPU, like its predecessors, consists of many streaming multiprocessors (SMs). Think of these like the “cores” of the GPU, as shown in Figure 2-4. ... ###### Figure 2-4. Comparing CPU cores to GPU cores (source: https://oreil.ly/003EH, https://oreil.ly/Z25Tf) ... Each SM contains a bunch of arithmetic units (for FP32, INT32, etc.), Tensor Cores for matrix math, load/store units for memory operations, and some special function units for things like transcendental math. The GPU also has its own small pool of superfast memory, including registers, shared memory, and L1 cache."
  },
  {
    "file_name": "aisp_0205.png",
    "caption": "Diagram showing the GPU memory hierarchy, including registers, L1/SMEM per SM, shared L2 cache, and global memory HBM.",
    "description": "Diagram showing the GPU memory hierarchy, including registers, L1/SMEM per SM, shared L2 cache, and global memory HBM.",
    "chapter": "Streaming Multiprocessor, Threads, and Warps",
    "page_context": "SMs execute many active warps in parallel to help cover the latency of a thread waiting on data accessed from global memory. Consider an SM having dozens of warps (hundreds of threads) in flight concurrently. If one warp is waiting on a memory fetch, another warp can run. This is called latency hiding. We will revisit latency hiding throughout the book. This is a very important performance-optimization tool to have in your tuning toolbox. ... A high-end GPU like Blackwell will have hundreds of SMs. Each SM is capable of running thousands of threads concurrently. This is how we get tens of thousands of active threads onto a single GPU. All those SMs share a 126 MB L2 cache, as we mentioned earlier, and share the memory controllers that connect to the HBM. The memory hierarchy contains registers (per thread) → shared memory (per thread block, on each SM) → L1 cache (per SM) → L2 cache (shared across all SMs on the GPU) → HBM memory (off chip), as shown in Figure 2-5. ... ###### Figure 2-5. GPU memory hierarchy ... For best performance, data needs to stay as high in that hierarchy as possible. If every operation went out to HBM even at 8 TB/s, the GPU would stall too often due to the increased latency of accessing off-chip memory. By keeping reusable data in SM local memory or L2 cache, the GPU can achieve enormous throughput. The Blackwell architecture’s doubling of cache and bandwidth is aimed exactly at keeping the GPU beast fed and happy."
  },
  {
    "file_name": "aisp_0206.png",
    "caption": "A 1U compute tray within the GB200/GB300 NVL72 showing two Grace Blackwell Superchips, illustrating the integration of CPUs and GPUs in a compact design.",
    "description": "A 1U compute tray within the GB200/GB300 NVL72 showing two Grace Blackwell Superchips, illustrating the integration of CPUs and GPUs in a compact design.",
    "chapter": "Ultrascale Networking Treating Many GPUs as One",
    "page_context": "NVIDIA provides a large rack configuration using GB200/GB300 Superchips called the NVL72 system. NVL72 stands for a system with 72 Blackwell GPUs—and 36 Grace CPUs—all interconnected with NVLink. This is essentially an AI supercomputer in a single rack. ... The GB200/GB300 NVL72 is built as 18 compute nodes in which each node contains two GB200/GB300 Superchips for a total of four Blackwell GPUs + two Grace CPUs per compute node, as shown in Figure 2-6. ... ###### Figure 2-6. A 1U compute tray within the GB200/GB300 NVL72 rack with two Grace Blackwell Superchips (source: developer.nvidia.com) ... Here, each superchip module has one Grace CPU and two Blackwell GPUs (each B200 is a dual-die MCM). The NVL72 has 18 of these trays linked together. By connecting the 18 compute nodes together, the GB200/GB300 NVL72 links 72 Blackwell GPUs (18 nodes × 4 GPUs) and 36 Grace CPUs (18 nodes × 2 CPUs) together to form a powerful, unified CPU-GPU cluster."
  },
  {
    "file_name": "aisp_0207.png",
    "caption": "Diagram of an NVLink Switch tray showing components such as NVLink5 connectors, Blackwell GPUs, Grace CPU, LPDDR5X memory, and various power and data connectors.",
    "description": "Diagram of an NVLink Switch tray showing components such as NVLink5 connectors, Blackwell GPUs, Grace CPU, LPDDR5X memory, and various power and data connectors.",
    "chapter": "NVLink and NVSwitch",
    "page_context": "This is double the per-GPU NVLink bandwidth of the previous generation used by Hopper GPUs. The Hopper H100 uses 18 NVLink 4 ports but runs at half the speed of NVLink 5. Inter-GPU latency over NVLink is in the single-digit microsecond range. ... The GPUs are cabled in a network through NVSwitch chips. NVSwitch is essentially a switching chip similar to a network switch, but it’s built specifically for NVLink. This means any GPU can reach any other GPU through one switch stage in the NVLink Switch System at full bisection bandwidth. This one-stage property holds true within a single NVL72 rack because each GPU uses its 18 NVLink links to connect to the 18 NVSwitch chips, enabling a path through a single switch. Figure 2-7 shows an NVLink Switch tray used in NVL72. ... ###### Figure 2-7. One NVLink Switch tray inside the NVL72 (source: https://oreil.ly/h7seG) ... Each switch tray contains two NVSwitch chips and multiple high-speed ports. The NVL72 rack comprises 9 such switch trays and 18 compute trays, as shown in Figure 2-8."
  },
  {
    "file_name": "aisp_0208.png",
    "caption": "Diagram of the NVSwitch system in an NVL72 rack, showing the arrangement of switch trays, compute trays, and power shelves.",
    "description": "Diagram of the NVSwitch system in an NVL72 rack, showing the arrangement of switch trays, compute trays, and power shelves.",
    "chapter": "NVLink and NVSwitch",
    "page_context": "###### Figure 2-7. One NVLink Switch tray inside the NVL72 (source: https://oreil.ly/h7seG) ... Each switch tray contains two NVSwitch chips and multiple high-speed ports. The NVL72 rack comprises 9 such switch trays and 18 compute trays, as shown in Figure 2-8. ... ###### Figure 2-8. NVSwitch System of nine trays inside an NVL72 rack (source: https://oreil.ly/h7seG) ... Since each of the 9 switch trays contains two NVSwitch chips, the total is 18 NVSwitch chips in the NVL72 system. The network is arranged as a full crossbar such that every GPU is connected to every NVSwitch, and every NVSwitch is connected to every GPU. This provides a high-bandwidth path between any pair of GPUs."
  },
  {
    "file_name": "aisp_0209.png",
    "caption": "Diagram of an NVL72 architecture showing 4-GPU compute trays connected through NVSwitches, illustrating GPU-to-switch links and data flow within the system.",
    "description": "Diagram of an NVL72 architecture showing 4-GPU compute trays connected through NVSwitches, illustrating GPU-to-switch links and data flow within the system.",
    "chapter": "NVLink and NVSwitch",
    "page_context": "Since each of the 9 switch trays contains two NVSwitch chips, the total is 18 NVSwitch chips in the NVL72 system. The network is arranged as a full crossbar such that every GPU is connected to every NVSwitch, and every NVSwitch is connected to every GPU. This provides a high-bandwidth path between any pair of GPUs. ... Each switch tray exposes 144 NVLink ports to fully connect the 18 NVLink links on each GPU. Concretely, each GPU uses its 18 NVLink links to connect to the 18 NVSwitch chips (one link to each switch). This means any GPU can reach any other GPU in one hop (GPU → NVSwitch → GPU), with enormous bandwidth along the way. Figure 2-9 shows the full NVL72 architecture with 72 fully connected GPUs (36 GB200 superchips) and 18 NVSwitches. ... ###### Figure 2-9. Each GPU connects to each NVSwitch (one link for each switch) ... The aggregate bisection bandwidth across the entire 72-GPU network is about 130 TB/s within an NVL72 rack. For perspective, that is many times higher than even a top-end InfiniBand cluster of similar scale. The design exposes a fully connected, high-bandwidth fabric with a global address space across GPUs. This allows efficient collectives and one-sided operations while preserving explicit software control over synchronization and consistency."
  },
  {
    "file_name": "aisp_0210.png",
    "caption": "Diagram illustrating the NVIDIA network hardware setup for offloading computations, showing CPUs, DPUs, and GPUs interconnected via the NVLink and Quantum InfiniBand fabric through the NVIDIA switch system.",
    "description": "Diagram illustrating the NVIDIA network hardware setup for offloading computations, showing CPUs, DPUs, and GPUs interconnected via the NVLink and Quantum InfiniBand fabric through the NVIDIA switch system.",
    "chapter": "In-Network Aggregations with NVIDIA SHARP",
    "page_context": "## In-Network Aggregations with NVIDIA SHARP ... Another hardware-enabled optimization is NVIDIA Scalable Hierarchical Aggregation and Reduction Protocol (SHARP). For NVLink Switch System racks, in-network reductions use SHARP engines integrated into NVSwitch ASICs to offload reductions and other collectives in-network (see Figure 2-10). ... ###### Figure 2-10. Offloading computations to the NVIDIA network hardware using SHARP reduction engines in NVSwitch ... The NVSwitch fabric combines partial results without the data needing to funnel through the GPUs. By offloading collective computations from the GPUs to the switch hardware itself, SHARP allows the GPUs to focus on more complex computations, lowers collective latencies, reduces the overall volume of data traversing the network, and increases system efficiency."
  },
  {
    "file_name": "aisp_0301.png",
    "caption": "Diagram illustrating a common set of frameworks, libraries, compilers, runtimes, and tools, such as PyTorch, CUDA, and cuDNN, used to develop and productionize modern LLM workloads.",
    "description": "Diagram illustrating a common set of frameworks, libraries, compilers, runtimes, and tools, such as PyTorch, CUDA, and cuDNN, used to develop and productionize modern LLM workloads.",
    "chapter": "NVIDIA Software Stack",
    "page_context": "Running a multi-petaFLOP GPU cluster[]()[]() involves more than just writing high-level PyTorch, TensorFlow, or JAX code. There is a whole software stack underpinning GPU operations, and each layer can affect performance. [Figure 3-1](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch03.html#ch03_figure_1_1757308027885881) shows[]() a common set of frameworks, libraries, compilers, runtimes, and tools used to develop and productionize modern LLM workloads, including PyTorch, cuDNN, cuBLAS, CUTLASS, CUDA C++, `nvcc`, and the CUDA Runtime API (e.g., CUDA tools, driver, etc.). ... In addition, the NVIDIA GPU and CUDA ecosystem embraces Python libraries and allows you to create CUDA kernels in Python using frameworks like OpenAI’s Triton domain-specific language (DSL) and NVIDIA’s Warp framework—as well as NVIDIA’s CUDA Python, cuTile, and CUTLASS libraries. ... ###### Figure 3-1. Common set of frameworks, libraries, compilers, runtimes, and tools used to develop and productionize modern LLM workloads ... ## GPU Driver"
  },
  {
    "file_name": "aisp_0302.png",
    "caption": "Diagram showing the compilation process of a CUDA program using nvcc into PTX and x86 code, and then targeting specific GPUs like GF100, GK110, and GP100.",
    "description": "Diagram showing the compilation process of a CUDA program using nvcc into PTX and x86 code, and then targeting specific GPUs like GF100, GK110, and GP100.",
    "chapter": "CUDA Forward and Backward Compatibility Across GPU Hardware Generations",
    "page_context": "## CUDA Forward and Backward Compatibility Across GPU Hardware Generations ... An important feature of NVIDIA’s GPU programming model is its compatibility across hardware generations. When you compile CUDA code, the resulting binary includes virtual, or intermediate, PTX code as well as physical device code (e.g., ARM, x86, GPU instructions), as shown in Figure 3-2. ... ###### Figure 3-2. Using nvcc to compile a CUDA program into PTX—and ultimately the low-level instructions for the GPU target device ... This allows newer GPUs to just-in-time (JIT) compile the PTX so your program runs on future architectures—and allows newer GPUs to execute older binary code for prior architectures. This compatibility is achieved through NVIDIA’s fatbinary model, which contains PTX for future-proofing and CUBIN, or architecture-specific CUDA device code binaries, for known architectures."
  },
  {
    "file_name": "aisp_0303.png",
    "caption": "Diagram showing the flow of operations from PyTorch Python API through various layers to GPU kernel execution.",
    "description": "Diagram showing the flow of operations from PyTorch Python API through various layers to GPU kernel execution.",
    "chapter": "PyTorch and Higher-Level AI Frameworks",
    "page_context": "The PyTorch compiler stack consists of TorchDynamo, AOT Autograd, and a backend like TorchInductor or Accelerated Linear Algebra (XLA), which automatically capture and optimize your models. TorchInductor is the most common backend, and it uses OpenAI’s Triton under the hood. Triton fuses kernels and performs kernel autotuning for your specific GPU and system environment, as we’ll cover in Chapter 14. ... When you perform operations on PyTorch tensors using GPUs, they are moved from the CPU to the GPU in what appears to be a single Python call. However, this single call is actually translated into a series of calls to the CUDA runtime utilizing various CUDA libraries, as shown in Figure 3-3. ... ###### Figure 3-3. Flow from PyTorch code to GPU device ... When you perform matrix multiplications, for example, PyTorch delegates these tasks to libraries such as cuBLAS. cuBLAS is part of the CUDA Toolkit and optimized for GPU execution. Behind the scenes, PyTorch ensures that operations like forward and backward passes are executed using low-level, optimized CUDA functions and libraries."
  },
  {
    "file_name": "aisp_0304.png",
    "caption": "Diagram showing the connection of GPUs to NUMA nodes, with GPUs 0-3 linked to NUMA node 0 and GPUs 4-7 to NUMA node 1, illustrating system architecture for training processes.",
    "description": "Diagram showing the connection of GPUs to NUMA nodes, with GPUs 0-3 linked to NUMA node 0 and GPUs 4-7 to NUMA node 1, illustrating system architecture for training processes.",
    "chapter": "NUMA Awareness and CPU Pinning",
    "page_context": "To explicitly specify NUMA-affinity, you need to “pin” processes or threads to specific CPUs that are connected to the same NUMA node as the GPU. This type of CPU affinity is called CPU pinning. Suppose you have eight GPUs in a node, with four GPUs connected to NUMA node 0 and the other four to NUMA node 1. ... If you launch eight training processes, one per GPU, you should bind each training process to a CPU core—or set of CPU cores—connected to the same NUMA node as the GPUs. In this case, GPUs 0–3 are connected to NUMA node 0 and GPUs 4–7s are connected to NUMA node 1’s cores, as shown in Figure 3-4. ... ###### Figure 3-4. Eight GPUs in a node, with four GPUs connected to NUMA node 0 and the other four to NUMA node 1 ... This way, when a CPU process wants to feed data to GPU 4, it should be running on a CPU connected to NUMA node 1 since GPU 4 is connected to NUMA node 1. []()[]()Linux provides tools to do this, including `numactl --cpunodebind=<node> --⁠membind=​<node>`, which launches a process pinned to the given NUMA node."
  },
  {
    "file_name": "aisp_0305.png",
    "caption": "Diagram illustrating data flow in virtual memory, highlighting the use of pinned memory to improve GPU data transfer efficiency.",
    "description": "Diagram illustrating data flow in virtual memory, highlighting the use of pinned memory to improve GPU data transfer efficiency.",
    "chapter": "NUMA-Friendly Memory Allocation and Memory Pinning",
    "page_context": "In fact, this is the basis of NVIDIA’s GPUDirect technologies such as GPUDirect RDMA, which allows NICs like InfiniBand to directly exchange data with GPU memory. Similarly, GPUDirect Storage (GDS) allows NVMe drives to stream data into GPU memory without extra CPU overhead. ... Deep learning frameworks provide options to use pinned memory for data loaders. []()[]()[]()[]()[]()For example, PyTorch’s `DataLoader` has a flag `pin_memory=True`, which, when true, means the batches loaded will be placed in pinned RAM, as shown in [Figure 3-5](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch03.html#ch03_figure_5_1757308027885970). ... ###### Figure 3-5. Pinned memory (aka page-locked or nonpageable) is a type of memory that cannot be swapped out to disk ... Memory pinning speeds up[]() the `tensor.to(device)` operations because the CUDA driver doesn’t have to pin pages on the fly. It’s especially beneficial when you are using large batch sizes or reading a lot of data in each iteration. Many practitioners have noticed that just turning on `pin_memory=True` in PyTorch can improve performance up to 10%–20% by reducing data transfer bottlenecks and increasing host-to-device transfer throughput."
  },
  {
    "file_name": "aisp_0306.png",
    "caption": "Diagram showing GPU schedule where Process A and Process B alternate, creating idle gaps and resulting in about 70% overall utilization.",
    "description": "Diagram showing GPU schedule where Process A and Process B alternate, creating idle gaps and resulting in about 70% overall utilization.",
    "chapter": "GPU Persistence Mode",
    "page_context": "If you enable MPS for these inference jobs, the GPUs can interleave their work so that while one job is waiting on memory, another job’s kernel might fill the GPU, etc. The result is higher overall GPU utilization. In practice, if two processes each use 40% of a GPU, with MPS you might see the GPU at 80%–90% utilization serving both. ... For instance, two training processes that each would take one hour on their own—on the same GPU, running sequentially—can run together with MPS and finish in a bit over one hour total in parallel instead of two hours sequentially. For instance, two training processes that each would take one hour on their own—on the same GPU, running sequentially—can run together with MPS. In this case, they would finish in a bit more than one hour total in parallel instead of two hours sequentially. The speedup from MPS can approach a near-doubling when kernels and memory bandwidth from concurrent clients complement one another. To visualize, imagine Process A and Process B each launching kernels periodically without MPS. The GPU schedule might look like A-B-A-B with gaps in between while each one waits, as shown in Figure 3-6. ... ###### Figure 3-6. GPU alternates between running Process A’s kernels and Process B’s kernels and creates idle gaps in which one process is waiting while the other is active ... With MPS, the schedule overlaps A and B so that whenever A isn’t using some parts of the GPU, B’s work can use them simultaneously, and vice versa. This overlapping eliminates idle gaps, as shown in Figure 3-7."
  },
  {
    "file_name": "aisp_0307.png",
    "caption": "Diagram showing processes A and B with overlapping GPU utilization, achieving around 90% combined efficiency by minimizing idle gaps.",
    "description": "Diagram showing processes A and B with overlapping GPU utilization, achieving around 90% combined efficiency by minimizing idle gaps.",
    "chapter": "GPU Persistence Mode",
    "page_context": "###### Figure 3-6. GPU alternates between running Process A’s kernels and Process B’s kernels and creates idle gaps in which one process is waiting while the other is active ... With MPS, the schedule overlaps A and B so that whenever A isn’t using some parts of the GPU, B’s work can use them simultaneously, and vice versa. This overlapping eliminates idle gaps, as shown in Figure 3-7. ... ###### Figure 3-7. Reducing idle gaps for processes A and B using MPS ... Setting up MPS involves running an MPS control daemon (`nvidia-cuda-mps-control`), which then launches an MPS server process that brokers GPU access. On modern GPUs, MPS is more streamlined as clients (the processes) can talk directly to the hardware with minimal interference from the compute node itself."
  },
  {
    "file_name": "aisp_0308.png",
    "caption": "Diagram showing seven MIG slices on a modern GPU, illustrating the allocation of system, control, data, L2 cache, and DRAM resources for each instance.",
    "description": "Diagram showing seven MIG slices on a modern GPU, illustrating the allocation of system, control, data, L2 cache, and DRAM resources for each instance.",
    "chapter": "GPU Persistence Mode",
    "page_context": "Modern GPUs can be partitioned at the hardware level into multiple instances using MIG. MIG is a form of virtualization but done in hardware. This way, the overhead is very low—maybe a few percent—due to the loss of some flexibility. ... If one instance is idle, it can’t lend its resources to another, as they are hard partitioned. MIG allows a GPU to be sliced into as many as seven smaller logical GPUs—each with its own dedicated portion of memory and compute units, or SMs, as shown in Figure 3-8. ... ###### Figure 3-8. Seven MIG slices on a modern GPU ... By convention, NVIDIA’s MIG profile naming[]() uses the prefix `<X>g to` denote the number of compute slices between 1 (min) and 7 (max) on modern GPUs. Each slice number represents a number of SM groups allocated to that partition. Each SM group is roughly a 1/7 slice of the total number of SMs."
  },
  {
    "file_name": "aisp_0401.png",
    "caption": "Diagram comparing sequential and overlapped communication and computation processes across multiple CUDA streams, illustrating improved efficiency with overlapping.",
    "description": "Diagram comparing sequential and overlapped communication and computation processes across multiple CUDA streams, illustrating improved efficiency with overlapping.",
    "chapter": "Overlapping Communication and Computation (Pipelining)",
    "page_context": "Overlapping communication and computation, or pipelining, plays a key role in building efficient training and inference systems at scale. In these environments, it’s important to keep GPUs busy and spend less time waiting for data. ... The main idea is to ensure that data transfers occur concurrently with ongoing computations so that when one task finishes, the results needed for the next stage are already in progress or have been delivered. Modern frameworks such as PyTorch support asynchronous operations so that collective communication (e.g., all-reduce of gradients) can run alongside compute tasks. This reduces idle GPU time (see Figure 4-1)—and improves overall system throughput. ... ###### Figure 4-1. Overlapping host-to-device (H2D) and device-to-host (D2H) communication with computation on multiple CUDA streams 0–3 ... CUDA-based libraries exploit the power of multiple CUDA streams. While one stream executes compute-heavy matrix multiplications, another handles communication tasks such as aggregating gradients. As each layer of a neural network finishes its computation, the previous layer’s outputs are already on their way for aggregation or further processing. This overlapping ensures that the system produces results without unnecessary waiting periods and maintains a steady flow of data."
  },
  {
    "file_name": "aisp_0402.png",
    "caption": "Diagram illustrating the four components of NVIDIA’s Magnum IO platform under CUDA: Storage I/O, Network I/O, In-network compute, and I/O management.",
    "description": "Diagram illustrating the four components of NVIDIA’s Magnum IO platform under CUDA: Storage I/O, Network I/O, In-network compute, and I/O management.",
    "chapter": "NVIDIA Magnum IO Optimization Stack",
    "page_context": "# NVIDIA Magnum IO Optimization Stack ... Magnum IO, NVIDIA’s overarching I/O acceleration platform, brings together a range of technologies to speed up data movement, access, and management across GPUs, CPUs, storage, and network interfaces. There are four key components of the Magnum IO architecture spanning storage, network, in-network computing, and I/O management, as shown in Figure 4-2. ... ###### Figure 4-2. Four components of NVIDIA’s Magnum IO acceleration platform ... Here is a description of the four components in Figure 4-2:"
  },
  {
    "file_name": "aisp_0403.png",
    "caption": "Diagram illustrating GPU-to-GPU direct data transfer with RoCE, showing the bypass of CPUs and system memory for efficient data movement.",
    "description": "Diagram illustrating GPU-to-GPU direct data transfer with RoCE, showing the bypass of CPUs and system memory for efficient data movement.",
    "chapter": "High-Speed, Low-Overhead Data Transfers with RDMA",
    "page_context": "The reduced throughput would show up in the profiler as an order-of-magnitude reduction in throughput. But you have to be aware of this possibility and continuously monitor your system for these types of subtle fallbacks. ... NVIDIA’s RDMA implementation for GPUs is called GPUDirect RDMA. GPUDirect RDMA lets an RDMA‐capable NIC such as InfiniBand and RDMA over Converged Ethernet (RoCE) perform direct memory access (DMA) to and from the GPU’s device memory across two servers—bypassing host CPU and system RAM entirely. A data transfer with RoCE is shown in Figure 4-3. ... ###### Figure 4-3. GPU-to-GPU direct data transfer with RoCE ... By registering GPU buffers with the NIC, GPUDirect RDMA enables one‐sided RDMA reads and writes between remote GPUs. This minimizes both latency and CPU overhead in multinode training."
  },
  {
    "file_name": "aisp_0404.png",
    "caption": "Diagram comparing network configurations: one without direct NIC, causing a Gen5 bottleneck, and one with direct NIC, allowing full throughput between GPU and NIC.",
    "description": "Diagram comparing network configurations: one without direct NIC, causing a Gen5 bottleneck, and one with direct NIC, allowing full throughput between GPU and NIC.",
    "chapter": "Tuning Multinode Connectivity",
    "page_context": "Some servers have multiple network interfaces (NICs). NCCL can stripe traffic across multiple NICs (called <em>multirail</em>) to increase bandwidth. []()[]()[]()[]()But you may need to set some environment variables like `NCCL_NSOCKS_PERTHREAD` and `NCCL_SOCKET_NTHREADS` to optimize this. We’ll discuss these in more detail in a bit. Just make sure that each NIC is on a different subnet and that NCCL can discover both. With proper setup, using two 800 Gbps NICs in parallel, for instance, gives an aggregate of 1.6 Tbps for NCCL traffic. And four such NIC links (e.g., two dual-port NICs) can achieve ~3.2 Tbps. ... Favor high-bandwidth, multirail NIC configurations that give each GPU or small groups of GPUs sufficient dedicated network bandwidth. Physically, NICs attach using PCIe to the host CPU or to a DPU. With modern GPU systems, NCCL supports GPU-initiated networking with InfiniBand GPUDirect Async (IBGDA) and the direct NIC path, as shown in Figure 4-4. This lets the GPU drive full-bandwidth RDMA without CPU intervention. ... ###### Figure 4-4. Bypassing CPU bottlenecks with direct connectivity between GPUs and NICs ... A common pitfall is a mismatch in network configuration that causes a fallback to a slower path. If RDMA is not working due to a misconfiguration, for instance, NCCL might be using TCP on a 100 Gbps Ethernet network but getting only a fraction of that due to kernel overhead. Even worse, if the cluster’s high-speed network is misidentified, traffic might go over a slower management network running only 10 Gbps Ethernet without the user realizing. Tools like NCCL’s debugging output and network interface counters (`ibstat`, `ifstat`) can help verify which interface is being used more heavily. For modern systems with large 200–400 Gbps paths, dropping to 10 Gbps would cause a severe bottleneck.[]()[]()"
  },
  {
    "file_name": "aisp_0405.png",
    "caption": "Diagram illustrating the hierarchy of memory management in NVIDIA Dynamo Distributed KV Cache Manager, showing the transition from GPU memory to shared network storage for optimal cache offloading.",
    "description": "Diagram illustrating the hierarchy of memory management in NVIDIA Dynamo Distributed KV Cache Manager, showing the transition from GPU memory to shared network storage for optimal cache offloading.",
    "chapter": "NVIDIA’s NIXL and Disaggregated Inference",
    "page_context": "NIXL is a core component of NVIDIA’s open source Dynamo inference engine. NIXL streamlines one-to-one and one-to-few data transfers such as moving a key-value (KV) cache (shared by the disaggregated stages) with minimal latency and overhead. It complements NCCL, which is mainly used for many-to-many collectives. ... NIXL has a consistent asynchronous API for moving data across GPUs, CPUs, SSDs, and shared network storage. It always picks the fastest path for the placement of each cache chunk of data being moved. This hierarchy is shown in Figure 4-5 in the context of NVIDIA Dynamo’s KV Cache Manager, which uses NIXL to choose the fastest path available for each KV cache transfer. ... ###### Figure 4-5. NVIDIA Dynamo Distributed KV Cache Manager offloads less frequently accessed KV cache to more economical memory hierarchies (source: https://oreil.ly/nsxdl) ... When scaling LLM inference, it’s important to efficiently transfer large data caches (e.g., transformer’s attention KV cache) between peers in a cluster of GPUs, CPUs, compute nodes, and racks. For example, with NIXL, an inference engine can offload a large (e.g., 100 GB) KV cache from a GPU to a peer using NVLink/InfiniBand with minimal overhead. This frees up the GPU to handle new requests. This is critical when serving LLMs with large context windows."
  },
  {
    "file_name": "aisp_0406.png",
    "caption": "Diagram comparing traditional and disaggregated serving models for inference processes, highlighting separate handling of prefill and decode stages across GPU clusters to improve efficiency and memory bandwidth.",
    "description": "Diagram comparing traditional and disaggregated serving models for inference processes, highlighting separate handling of prefill and decode stages across GPU clusters to improve efficiency and memory bandwidth.",
    "chapter": "Separate Prefill and Decode Inference Stages",
    "page_context": "The first stage, prefill, is often compute bound as it uses many matrix multiplications to build the KV cache from the incoming request data (aka prompt). The second stage, decode, is often memory-throughput bound, as it needs to gather the model weights from GPU HBM memory to calculate the next set of tokens (aka completion or response). ... This prefill/decode split is implemented in common inference engines vLLM, SGLang, and NVIDIA’s Dynamo and TensorRT-LLM. The prefill (prompt ingestion) creates the KV cache, and the decode (generation) uses this cache. NIXL specifically accelerates the transfer of the KV cache between nodes in this workflow. Figure 4-6 compares the traditional “monolithic” serving model to the “disaggregated” serving model in which two stages run on different GPU-based compute nodes to increase scale and maximize throughput. ... ###### Figure 4-6. Disaggregated serving separates prefill and decode stages onto different GPU clusters (source: https://oreil.ly/nsxdl) ... Here, the traditional setup is shown on the top, in which each GPU node handles both the prefill (compute-bound) and decode (memory-bound, I/O-bound) phases. On the bottom, the disaggregated serving configuration places the prefill workers in the GPU cluster and the decode workers in another GPU cluster."
  },
  {
    "file_name": "aisp_0407.png",
    "caption": "Diagram illustrating the NIXL architecture, highlighting its ability to transfer data between various storage types using a unified API and backend.",
    "description": "Diagram illustrating the NIXL architecture, highlighting its ability to transfer data between various storage types using a unified API and backend.",
    "chapter": "NIXL Asynchronous API with Callbacks",
    "page_context": "From a developer’s perspective, NIXL offers a straightforward API. You post a transfer request with a pointer to the data and a destination—either GPUs, CPUs, or storage targets like Amazon S3. NIXL will transfer that data as fast as possible. ... For instance, a NIXL transfer request could send a KV cache segment to another GPU, a CPU host memory buffer, or even an object storage service. And it can do this all within the same API, as shown in Figure 4-7. ... ###### Figure 4-7. NIXL architecture (source: https://oreil.ly/nsxdl) ... This modular design means that NIXL can adopt future transports as well. For example, it can incorporate upcoming protocols or faster storage-class memory without changing the user-facing API. Under the hood, NIXL coordinates the data movement using whatever backend is appropriate."
  },
  {
    "file_name": "aisp_0408.png",
    "caption": "Diagram comparing traditional x86-based GPU servers with PCIe bottleneck and NVIDIA Grace Hopper superchip using NVLink-C2C with unified virtual memory.",
    "description": "Diagram comparing traditional x86-based GPU servers with PCIe bottleneck and NVIDIA Grace Hopper superchip using NVLink-C2C with unified virtual memory.",
    "chapter": "KV Cache Offloading with NIXL",
    "page_context": "The design motivation for NIXL is closely related to best practices for handling large memory for LLM inference. If GPUs don’t have enough memory to hold the entire KV cache for a long sequence or multiturn conversation, NIXL allows the inference server (e.g., NVIDIA Dynamo) to offload the KV cache to CPU memory—or even NVMe SSD—and bring it back as needed. ... NIXL, in conjunction with the KV Cache Manager in Dynamo, for instance, can manage this transfer hierarchy efficiently. Consider NVIDIA’s Grace Hopper and Grace Blackwell Superchips with a huge amount of unified CPU and GPU memory shared over the fast NVLink interconnect (see Figure 4-8). ... ###### Figure 4-8. ARM-based Grace Hopper Superchip architecture leverages the 900 GB/s NVLink-C2C and overcomes traditional PCIe bottlenecks (source: https://oreil.ly/zf6rF) ... The inference server can quickly offload a large KV cache to the large CPU memory to free up the limited GPU HBM. This yields a huge boost in inference performance. Specifically, a PCIe-based x86 + H100 system can improve time-to-first-token (TTFT) latency by as much as 14× for long input sequences compared to recomputing the cache. This speedup is shown in Figure 4-9."
  },
  {
    "file_name": "aisp_0409.png",
    "caption": "Graph illustrating how KV cache offloading enhances time-to-first-token latency, showing up to a 14× speedup for x86-based NVIDIA H100 GPU systems with longer input sequences.",
    "description": "Graph illustrating how KV cache offloading enhances time-to-first-token latency, showing up to a 14× speedup for x86-based NVIDIA H100 GPU systems with longer input sequences.",
    "chapter": "KV Cache Offloading with NIXL",
    "page_context": "###### Figure 4-8. ARM-based Grace Hopper Superchip architecture leverages the 900 GB/s NVLink-C2C and overcomes traditional PCIe bottlenecks (source: https://oreil.ly/zf6rF) ... The inference server can quickly offload a large KV cache to the large CPU memory to free up the limited GPU HBM. This yields a huge boost in inference performance. Specifically, a PCIe-based x86 + H100 system can improve time-to-first-token (TTFT) latency by as much as 14× for long input sequences compared to recomputing the cache. This speedup is shown in Figure 4-9. ... ###### Figure 4-9. Measure 14× TTFT speedup with KV cache offloading for an x86-based NVIDIA H100 GPU system on large input sequence lengths compared to recalculating it from scratch (source: https://oreil.ly/zf6rF) ... Furthermore, with its 900 GB/s NVLink-C2C interconnect, the ARM-based Grace Hopper Superchip delivers 2× faster TTFT latency compared to the non-superchip, x86-based H100 version described previously. This speedup is shown in Figure 4-10."
  },
  {
    "file_name": "aisp_0410.png",
    "caption": "Graph showing a 2x speedup in time-to-first token speedup for KV cache offloading, with the ARM-based GH200 compared to the x86-H100 across varying cached input sequence lengths.",
    "description": "Graph showing a 2x speedup in time-to-first token speedup for KV cache offloading, with the ARM-based GH200 compared to the x86-H100 across varying cached input sequence lengths.",
    "chapter": "KV Cache Offloading with NIXL",
    "page_context": "Furthermore, with its 900 GB/s NVLink-C2C interconnect, the ARM-based Grace Hopper Superchip delivers 2× faster TTFT latency compared to the non-superchip, x86-based H100 version described previously. This speedup is shown in Figure 4-10. ... These are impressive gains enabled by codesigning NIXL software with NVIDIA superchip hardware. And NIXL, designed exactly with those numbers in mind, makes offloading the KV cache a viable option by keeping transfer costs low. KV cache offloading is a key part of large-scale inference deployments, as we’ll see in an upcoming chapter—especially for massive LLMs where memory capacity is a limiting factor. ... ###### Figure 4-10. 2× speedup in TTFT for the ARM-based Grace Hopper Superchip compared to the x86-based H100 GPU system due to KV cache offloading with the 900 GB/s NVLink-C2C interconnect (source: https://oreil.ly/zf6rF) ... As models get bigger and workloads get more complex, having a library like NIXL to efficiently move large blobs of data asynchronously is critical. For performance engineers, if your use case involves moving large amounts of data between stages (e.g., pipeline parallelism) and other components (e.g., GPUs or storage) in a system, consider whether NCCL is sufficient or whether a specialized solution like NIXL is an option to optimize that flow of data."
  },
  {
    "file_name": "aisp_0501.png",
    "caption": "Diagram comparing VAST Data's network architecture with and without GPU Direct Storage (GDS), highlighting reduced CPU utilization and improved data flow efficiency with GDS.",
    "description": "Diagram comparing VAST Data's network architecture with and without GPU Direct Storage (GDS), highlighting reduced CPU utilization and improved data flow efficiency with GDS.",
    "chapter": "Using NVIDIA GDS",
    "page_context": "Many storage vendors like WekaIO, DDN, VAST, Cloudian, etc., have released GDS-aware solutions or plugins so their systems can deliver data using RDMA directly into GPU memory. This ecosystem support means GDS can be used by enterprise network-attached storage (NAS) and parallel filesystems out of the box. ... Reports from VAST Data show a 20% boost in read throughput using GDS on certain AI workloads. In their case, using GDS on a single A100 GPU achieved 20% higher read throughput for sequential reads, which pushed significantly closer to the 100 Gb/s link capacity per NIC when applicable. Figure 5-1 shows the architecture with and without GDS. ... ###### Figure 5-1. VAST Data’s network architecture with GDS versus without GDS ... Here on the left, we see traditional staged DMA that copies through host memory. On the right is a direct GPU pull using GDS that bypasses host memory copies and reduces CPU utilization. A report by VAST measured a 20% read-throughput boost on an NVIDIA Ampere A100 GPU and a 30%+ increase on a Hopper H100 GPU due to its higher NIC bandwidth and greater CPU burden."
  },
  {
    "file_name": "aisp_0502.png",
    "caption": "Diagram showing the components of 3FS including FoundationDB, meta service, storage service, cluster manager, and their connection over an RDMA network, highlighting client interactions through RDMA device, kernel model, and FUSE.",
    "description": "Diagram showing the components of 3FS including FoundationDB, meta service, storage service, cluster manager, and their connection over an RDMA network, highlighting client interactions through RDMA device, kernel model, and FUSE.",
    "chapter": "DeepSeek’s Fire-Flyer File System",
    "page_context": "3FS mirrors the trend of codesigning storage specifically for AI. This is similar to NVIDIA’s GDS, which is designed to work with high-performance parallel filesystems to achieve similar direct-GPU throughput. ... 3FS consists of four key components: cluster manager, metadata service, storage service, and client. These are interconnected over an RDMA-capable fabric like InfiniBand or RoCE to minimize CPU involvement and host-side copies. These components and connections are shown in Figure 5-2. ... ###### Figure 5-2. Components of DeepSeek’s Fire-Flyer File System (3FS) (source: https://oreil.ly/xD3id) ... 3FS is a Linux-based filesystem, which allows compatibility with existing applications while leveraging RDMA reads for direct GPU-accessible data transfers. Metadata is sharded and replicated across multiple nodes for scale-out performance. Data paths bypass the OS page cache entirely to maintain optimal throughput."
  },
  {
    "file_name": "aisp_0601.png",
    "caption": "Diagram illustrating a simple CUDA programming flow between CPU and GPU, showing the sequence of loading data, copying to GPU, executing the kernel, copying results back, and using the results on the CPU.",
    "description": "Diagram illustrating a simple CUDA programming flow between CPU and GPU, showing the sequence of loading data, copying to GPU, executing the kernel, copying results back, and using the results on the CPU.",
    "chapter": "Understanding GPU Architecture",
    "page_context": "# Understanding GPU Architecture ... Unlike CPUs, which optimize for low-latency single-thread performance, GPUs are throughput‐optimized processors built to run thousands of threads in parallel. A simple CUDA programming flow between the CPU and GPU is shown in Figure 6-1. ... ###### Figure 6-1. Simple CUDA programming flow ... Initially, the host loads data into CPU memory. It then copies the data from the CPU to the GPU memory. After calling the GPU kernel with the data in GPU memory, the CPU copies the results back from GPU memory to CPU memory. Now the results live back on the CPU for further processing."
  },
  {
    "file_name": "aisp_0602.png",
    "caption": "Diagram illustrating the structure of a Blackwell Streaming Multiprocessor (SM) with four independent warp schedulers, each capable of issuing one warp instruction per cycle, featuring INT32, FP32, and Tensor Core units.",
    "description": "Diagram illustrating the structure of a Blackwell Streaming Multiprocessor (SM) with four independent warp schedulers, each capable of issuing one warp instruction per cycle, featuring INT32, FP32, and Tensor Core units.",
    "chapter": "Understanding GPU Architecture",
    "page_context": "Within a Blackwell SM, multiple warp schedulers issue instructions to the available pipelines; four independent warp schedulers allow up to four warps to issue instructions to the available pipelines on every cycle. Furthermore, each scheduler supports dual-issue capable of issuing two independent instructions (e.g., one arithmetic and one memory operation) per warp. Note that the dual-issue must come from the same warp—and not across warps. ... In the best case, one warp from each scheduler can issue an instruction concurrently each cycle, allowing four warps to execute in parallel per cycle. This further boosts throughput when instruction mixing is utilized, as shown in Figure 6-2. ... ###### Figure 6-2. Blackwell SMs contain four independent warp schedulers, each capable of issuing one warp instruction per cycle with dual-issue of one math and one memory operation per scheduler ... Here, each SM is subdivided into four independent scheduling partitions—each with its own warp scheduler and dispatch logic. You can think of the SM as four “mini-SMs” sharing on-chip resources. This lets the hardware pick ready warps and issue instructions from up to four different warps each clock cycle."
  },
  {
    "file_name": "aisp_0603.png",
    "caption": "Diagram illustrating the hierarchy of CUDA workloads: threads are executed by cores, thread blocks by streaming multiprocessors, and kernel grids by the entire GPU unit.",
    "description": "Diagram illustrating the hierarchy of CUDA workloads: threads are executed by cores, thread blocks by streaming multiprocessors, and kernel grids by the entire GPU unit.",
    "chapter": "Threads, Warps, Blocks, and Grids",
    "page_context": "## Threads, Warps, Blocks, and Grids ... CUDA structures parallel work into a three-level hierarchy—threads, thread blocks (aka cooperative thread arrays [CTAs]), and grids—to balance programmability with massive throughput. At the lowest level, each thread executes your kernel code. You group threads into thread blocks of up to 1,024 threads each on modern GPUs. Thread blocks form a grid when you launch the kernel, as seen in Figure 6-3. ... ###### Figure 6-3. Threads, thread blocks (aka CTAs), and grids ... By sizing your grid appropriately, you can scale to millions of threads without changing your kernel logic. CUDA’s runtime (and frameworks like PyTorch) handle scheduling and distribution across all SMs. Figure 6-4 shows another view of the thread hierarchy, including the CPU-based host, which invokes a CUDA kernel running on the GPU device."
  },
  {
    "file_name": "aisp_0604.png",
    "caption": "Diagram illustrating the thread hierarchy, showing how the CPU-based host launches kernels on the GPU device, with thread blocks organized within grids.",
    "description": "Diagram illustrating the thread hierarchy, showing how the CPU-based host launches kernels on the GPU device, with thread blocks organized within grids.",
    "chapter": "Threads, Warps, Blocks, and Grids",
    "page_context": "###### Figure 6-3. Threads, thread blocks (aka CTAs), and grids ... By sizing your grid appropriately, you can scale to millions of threads without changing your kernel logic. CUDA’s runtime (and frameworks like PyTorch) handle scheduling and distribution across all SMs. Figure 6-4 shows another view of the thread hierarchy, including the CPU-based host, which invokes a CUDA kernel running on the GPU device. ... ###### Figure 6-4. View of thread hierarchy, including the CPU-based host, which launches a kernel running on the GPU device ... Traditionally, threads from different thread blocks could not work with one another directly. However, modern GPU architectures and CUDA versions support thread block clusters. Threadblock clusters are groups of thread blocks that can communicate with one another across SMs."
  },
  {
    "file_name": "aisp_0605.png",
    "caption": "Diagram illustrating the architecture of hardware-supported DSMEM, showing shared memory connections between multiple SMs in a thread block cluster.",
    "description": "Diagram illustrating the architecture of hardware-supported DSMEM, showing shared memory connections between multiple SMs in a thread block cluster.",
    "chapter": "Threads, Warps, Blocks, and Grids",
    "page_context": "Traditionally, threads from different thread blocks could not work with one another directly. However, modern GPU architectures and CUDA versions support thread block clusters. Threadblock clusters are groups of thread blocks that can communicate with one another across SMs. ... Specifically, within a thread block cluster, threads in different thread blocks can access one another’s shared memory and use hardware-supported, cluster-scoped barriers. These allow for much larger compute operations, including matrix multiplies, which are very common in today’s massive LLM workloads. Thread block clusters share a distributed shared-memory (DSMEM) address space between SMs that participate in the thread block cluster, as shown in Figure 6-5. ... ###### Figure 6-5. Hardware-supported DSMEM used in thread block clusters containing multiple thread blocks ... DSMEM is a hardware feature that links the shared-memory banks of all SMs into a thread block cluster over a fast on-chip interconnect. With DSMEM, the SMs share a combined multi-SM distributed shared-memory pool. This unification allows threads in different blocks to read, write, and atomically update one another’s shared buffers at on-chip speeds—and without using global memory bandwidth."
  },
  {
    "file_name": "aisp_0606.png",
    "caption": "Diagram illustrating thread synchronization within a block using __syncthreads() between two code sections, highlighting the need to minimize synchronization points.",
    "description": "Diagram illustrating thread synchronization within a block using __syncthreads() between two code sections, highlighting the need to minimize synchronization points.",
    "chapter": "Threads, Warps, Blocks, and Grids",
    "page_context": "We’ll cover advanced topics like thread block clusters and DSMEM in Chapter 10. These are an extremely important addition to modern GPU processing—and very important for an AI systems performance engineer to understand. For this chapter, our focus remains on intrablock shared-memory optimizations. ... Within each thread block, threads share data using low-latency on-chip shared memory and synchronize with `__syncthreads()`. Because each barrier incurs overhead, you should minimize synchronization points, as shown in [Figure 6-6](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch06.html#ch06_figure_6_1757308039123612). ... ###### Figure 6-6. Synchronizing all threads within a thread block between two sections of code ... The goal is to minimize synchronization points. However, the GPU hardware will attempt to hide long-latency events such as global-memory loads, cache fills, and pipeline stalls by rapidly switching among warps."
  },
  {
    "file_name": "aisp_0607.png",
    "caption": "Diagram illustrating the execution of warps with a warp scheduler and instruction dispatch units, showing how instructions for different warps advance over time.",
    "description": "Diagram illustrating the execution of warps with a warp scheduler and instruction dispatch units, showing how instructions for different warps advance over time.",
    "chapter": "Threads, Warps, Blocks, and Grids",
    "page_context": "The goal is to minimize synchronization points. However, the GPU hardware will attempt to hide long-latency events such as global-memory loads, cache fills, and pipeline stalls by rapidly switching among warps. ... Thread blocks are subdivided into warps of 32 threads that execute in lockstep under the SIMT model using a warp scheduler. This is shown in Figure 6-7. ... ###### Figure 6-7. Warps (32 threads) advance as a whole with instructions managed by the warp scheduler ... Keeping more warps in flight is known as high occupancy on the SM. When your CUDA code allows high occupancy, it means that when one warp stalls, another is ready to run. This keeps the GPU’s compute units busy."
  },
  {
    "file_name": "aisp_0608.png",
    "caption": "Diagram showing warp divergence on the left with split execution paths, and warp uniformity on the right with synchronized threads.",
    "description": "Diagram showing warp divergence on the left with split execution paths, and warp uniformity on the right with synchronized threads.",
    "chapter": "Threads, Warps, Blocks, and Grids",
    "page_context": "Thread blocks execute independently and in no guaranteed order. This allows the GPU scheduler to dispatch them across all SMs and fully exploit hardware parallelism. This grid–block–warp hierarchy guarantees that your CUDA kernels will run unmodified on future GPU architectures with more SMs and threads. ... Throughput also hinges on warp execution efficiency. Threads in a warp must follow the same control-flow path and perform coalesced memory accesses. []()[]()If some threads diverge such that one branch takes the `if` path and others take the `else` path, the warp serializes execution, processing each branch path sequentially. This is called <em>warp divergence</em>, and it’s shown in [Figure 6-8](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch06.html#ch06_figure_8_1757308039123643).[]()[]()[]()[]()[]()[]()[]() ... ###### Figure 6-8. SIMT warp divergence (left) versus uniformity (right) ... By masking inactive lanes and running extra passes to cover each branch, warp divergence multiplies the overall execution time by the number of branches. We’ll dive deeper into warp divergence in Chapter 8—as well as ways to detect, profile, and mitigate it."
  },
  {
    "file_name": "aisp_0609.png",
    "caption": "Diagram illustrating the hierarchical structure and hardware limits of threads on a Blackwell GPU, showing hundreds of thousands of threads distributed across multiple levels including streaming multiprocessors, thread blocks, and warps.",
    "description": "Diagram illustrating the hierarchical structure and hardware limits of threads on a Blackwell GPU, showing hundreds of thousands of threads distributed across multiple levels including streaming multiprocessors, thread blocks, and warps.",
    "chapter": "Choosing Threads-per-Block and Blocks-per-Grid Sizes",
    "page_context": "These hardware limits affect how many blocks/warps can be active on an SM at once. This is a measurement of occupancy, as we introduced earlier. Smaller blocks might enable higher occupancy if they allow more concurrent warps to run concurrently on the SM. ... It’s important to understand the relative scale and hardware thread limits for your GPU generation, including number of threads, thread blocks, warps, and SMs. Figure 6-9 shows the relative scale of these resources, including their limits. ... ###### Figure 6-9. Relative scale and hardware limits for threads on a Blackwell GPU ... Table 6-2 summarizes these GPU limits for the Blackwell B200 GPU. The rest of the limits are available on NVIDIA’s website. (Other GPU generations will have different limits, so be sure to check the exact specifications for your system.)"
  },
  {
    "file_name": "aisp_0610.png",
    "caption": "Diagram of GPU memory hierarchy showing the relationship between CPU host memory, DRAM, L1 and L2 caches, and shared memory within Streaming Multiprocessors (SM), along with NVLINK and PCIe connections.",
    "description": "Diagram of GPU memory hierarchy showing the relationship between CPU host memory, DRAM, L1 and L2 caches, and shared memory within Streaming Multiprocessors (SM), along with NVLINK and PCIe connections.",
    "chapter": "Understanding GPU Memory Hierarchy",
    "page_context": "In reality, however, the GPU provides a multilevel memory hierarchy and helps balance capacity and speed. The hierarchy includes registers, shared memory, caches, global memory, and a specialized TMEM on Blackwell GPUs and beyond. TMEM, discussed in more detail in a bit, is a dedicated ~256 KB per-SM on-chip memory used by Blackwell’s 5th-generation Tensor Core instructions (`tcgen05.*`). It isn’t directly pointer-addressable from CUDA C++. Instead, data movement is orchestrated by TMA hardware (global memory ↔ SMEM) and tcgen05 Tensor Core data-movement instructions (SMEM ↔ TMEM implicitly using tensor descriptors. ... Global memory (HBM or DRAM) is large, off-chip, and relatively slow. Registers are tiny, on-chip, and extremely fast. L1 cache, L2 cache, and shared memory are somewhere in between. The benefit of caching and shared memory is that they hide the relatively long latency of accessing the large off-chip memory stores. A high-level view of the GPU memory hierarchy (including the CPU) is shown in Figure 6-10. ... ###### Figure 6-10. GPU memory hierarchy, including the CPU ... TMEM is a dedicated 256 KB per-SM buffer that transparently communicates with the Tensor Cores at tens of terabytes per second of bandwidth. This reduces the Tensor Core’s reliance on global memory. Figure 6-11 shows TMEM servicing the Tensor Cores—along with SMEM—to compute a C = A × B matrix multiply."
  },
  {
    "file_name": "aisp_0611.png",
    "caption": "Diagram showing the interaction between TMEM, SMEM, Registers, and Tensor Cores for matrix multiplication C = A × B.",
    "description": "Diagram showing the interaction between TMEM, SMEM, Registers, and Tensor Cores for matrix multiplication C = A × B.",
    "chapter": "Understanding GPU Memory Hierarchy",
    "page_context": "###### Figure 6-10. GPU memory hierarchy, including the CPU ... TMEM is a dedicated 256 KB per-SM buffer that transparently communicates with the Tensor Cores at tens of terabytes per second of bandwidth. This reduces the Tensor Core’s reliance on global memory. Figure 6-11 shows TMEM servicing the Tensor Cores—along with SMEM—to compute a C = A × B matrix multiply. ... ###### Figure 6-11. TMEM and SMEM servicing the Tensor Cores for C = A × B matrix multiply ... Here, operand B is sourced from SMEM. Operand A is in TMEM (although it may be in SMEM, as well). The accumulator is in TMEM, as well. Tiles flow from global memory to SMEM through L2 cache using TMA (e.g., `cuda::memcpy_async`). Operands move between SMEM and TMEM implicitly through Tensor Core instructions such as unified matrix-multiply-accumulate (UMMA) and `tcgen05.mma`."
  },
  {
    "file_name": "aisp_0612.png",
    "caption": "Diagram illustrating the connection between a thread and its corresponding per-thread local memory, emphasizing local memory spillover when registers are exceeded.",
    "description": "Diagram illustrating the connection between a thread and its corresponding per-thread local memory, emphasizing local memory spillover when registers are exceeded.",
    "chapter": "Understanding GPU Memory Hierarchy",
    "page_context": "On Blackwell, every thread begins its journey at the register file, a tiny on-SM SRAM array that holds each thread’s local variables with essentially zero added latency. Each SM houses 64 K 32-bit registers (256 KB total), but the hardware exposes at most 255 registers per thread. ... Because reads and writes complete in a single cycle and contend with almost nothing else, register bandwidth can reach tens of terabytes per second per SM. However, if your kernel needs more registers—either through many thread-local variables or compiler temporaries—the overflow spills into local memory, mapped to off-chip DRAM, and incurs hundreds to over a thousand cycles of latency. This local memory is shown in Figure 6-12. ... ###### Figure 6-12. Local memory per thread ... One step up is a unified L1/data cache and shared-memory block. This is 256 KB of on-SM SRAM per SM that you can dynamically split between user-managed shared memory (up to 228 KB per block) using `cudaFuncSetAttribute()` with `cudaFuncAttributePreferredSharedMemoryCarveout` to select the memory carveout on architectures like Blackwell with unified L1/Texture/Shared Memory. The maximum dynamic shared memory per block is 227 KB (CUDA reserves 1KB per block), and the total allocatable shared memory per SM is also bounded by this limit."
  },
  {
    "file_name": "aisp_0613.png",
    "caption": "Diagram illustrating the relationship between a thread block and its dedicated per-block shared memory.",
    "description": "Diagram illustrating the relationship between a thread block and its dedicated per-block shared memory.",
    "chapter": "Understanding GPU Memory Hierarchy",
    "page_context": "One step up is a unified L1/data cache and shared-memory block. This is 256 KB of on-SM SRAM per SM that you can dynamically split between user-managed shared memory (up to 228 KB per block) using `cudaFuncSetAttribute()` with `cudaFuncAttributePreferredSharedMemoryCarveout` to select the memory carveout on architectures like Blackwell with unified L1/Texture/Shared Memory. The maximum dynamic shared memory per block is 227 KB (CUDA reserves 1KB per block), and the total allocatable shared memory per SM is also bounded by this limit. ... Accesses here cost roughly 20–30 cycles, but if you design your thread blocks to avoid bank conflicts, you can achieve terabytes-per-second throughput. Thread-block shared memory is shown in Figure 6-13. ... ###### Figure 6-13. Thread-block shared memory ... TMEM is a dedicated on-chip memory per SM (256 KB on Blackwell) used by Tensor Core–specific operations and instructions including unified matrix-multiply-accumulate and `tcgen05`, discussed in [Chapter 10](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch10.html#ch10_intra_kernel_pipelining_warp_specialization_and_1757308054839290). It is not a normal pointer addressable space in CUDA C++. Instead, transfers are orchestrated with the Tensor Memory Accelerator (TMA) using descriptors. This frees the developer from having to manually manage data flow with the Tensor Cores. Some arithmetic operands, for instance, reside in shared memory, while the <span class=\"keep-together\">accumulator</span> resides in TMEM. TMA is then responsible for moving the data between global memory, shared memory, and TMEM memory to perform the <span class=\"keep-together\">computations</span>."
  },
  {
    "file_name": "aisp_0614.png",
    "caption": "Diagram showing the connection between Kernel 0 and Kernel 1 to per-device global memory.",
    "description": "Diagram showing the connection between Kernel 0 and Kernel 1 to per-device global memory.",
    "chapter": "Understanding GPU Memory Hierarchy",
    "page_context": "The global memory tier, local spill space and HBM, live off-chip. Any spilled registers or oversized automatic arrays reside in local memory, paying full DRAM latency (hundreds to more than 1,000 cycles) despite HBM3e’s ~8 TB/s bandwidth. ... For Blackwell, the HBM3e tier provides up to 180 GB of device-wide storage at ~8 TB/s total. However, its high latency makes it the slowest link in the chain. Per-device global memory is shown in Figure 6-14. ... ###### Figure 6-14. Per-device global memory, or HBM ... Using tools like Nsight Compute to track spills and cache hit rates, you can keep your kernels operating as close as possible to the on-chip peaks of this memory hierarchy. These tools can help you orchestrate data effectively through registers, shared/L1, constant cache, and L2 cache. Modern GPUs like Blackwell allow kernel developers to exploit the memory hierarchy by using L2 caches and unified L1/shared memory to buffer and coalesce accesses to HBM, as we’ll soon see."
  },
  {
    "file_name": "aisp_0615.png",
    "caption": "Diagram illustrating the point of coherency in GPU threads, highlighting five levels: Self (thread), Block, Cluster, Device, and System.",
    "description": "Diagram illustrating the point of coherency in GPU threads, highlighting five levels: Self (thread), Block, Cluster, Device, and System.",
    "chapter": "Understanding GPU Memory Hierarchy",
    "page_context": "The Blackwell B200 presents as a single GPU built with a unified, global address space. However, it’s made up of two reticle-limited dies connected by a 10 TB/s chip-to-chip interconnect. Each die is connected to four HBM3e stacks for a total of eight HBM3e stacks. From a developer’s perspective, however, HBM memory access is uniform across this combined address space, but it’s worth understanding the low-level details of this architecture. ... The point of coherency (PoC) for the different levels in the memory hierarchy depends on your needs and the level at which the threads are communicating. It typically happens at the following levels: thread, thread-block (aka CTA), thread block cluster (aka CTA cluster), device, or system, as shown in Figure 6-15. ... ###### Figure 6-15. Point-of-memory coherency for your GPU threads ... In summary, it’s important to understand the GPU’s memory hierarchy and target each level appropriately. By doing so, you can structure your CUDA kernels to maximize data locality, hide memory-access latency, increase occupancy, and fully leverage Blackwell’s massive parallel compute capabilities, as we’ll explore in a bit. First, let’s discuss NVIDIA’s unified memory, which is important to understand given the unified CPU-GPU superchip designs of Grace Hopper and Grace Blackwell."
  },
  {
    "file_name": "aisp_0616.png",
    "caption": "Diagram illustrating automatic page migrations in Unified Memory, showing data transfers between CPU and GPU physical memory via NVLink-C2C for improved performance.",
    "description": "Diagram illustrating automatic page migrations in Unified Memory, showing data transfers between CPU and GPU physical memory via NVLink-C2C for improved performance.",
    "chapter": "Unified Memory",
    "page_context": "cudaMemAdvise(ptr, size, cudaMemAdviseSetReadMostly, gpuId); ... ``` ... Here, you can use `PreferredLocation` to tell the driver where you’ll mostly use the data, and `ReadMostly` when it’s largely read-only, as shown in [Figure 6-18](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch06.html#ch06_figure_18_1757308039123803). ... ###### Figure 6-16. Automatic page migrations with CPU-GPU Unified Memory ... ![Diagram illustrating the use of cudaMemPrefetchAsync() to transfer data from CPU memory to GPU memory, highlighting asynchronous memory prefetching to avoid GPU stalls.](images/aisp_0617.png)"
  },
  {
    "file_name": "aisp_0617.png",
    "caption": "Diagram illustrating the use of cudaMemPrefetchAsync() to transfer data from CPU memory to GPU memory, highlighting asynchronous memory prefetching to avoid GPU stalls.",
    "description": "Diagram illustrating the use of cudaMemPrefetchAsync() to transfer data from CPU memory to GPU memory, highlighting asynchronous memory prefetching to avoid GPU stalls.",
    "chapter": "Unified Memory",
    "page_context": "![Diagram illustrating automatic page migrations in Unified Memory, showing data transfers between CPU and GPU physical memory via NVLink-C2C for improved performance.](images/aisp_0616.png) ... ###### Figure 6-16. Automatic page migrations with CPU-GPU Unified Memory ... ###### Figure 6-17. Streaming data from CPU to GPU over NVLink-C2C with cudaMemPrefetchAsync() ... ![Diagram illustrating the use of cudaMemAdvise with cudaPreferredLocation to set preferred memory locations between CPU and GPU for optimization.](images/aisp_0618.png)"
  },
  {
    "file_name": "aisp_0618.png",
    "caption": "Diagram illustrating the use of cudaMemAdvise with cudaPreferredLocation to set preferred memory locations between CPU and GPU for optimization.",
    "description": "Diagram illustrating the use of cudaMemAdvise with cudaPreferredLocation to set preferred memory locations between CPU and GPU for optimization.",
    "chapter": "Unified Memory",
    "page_context": "![Diagram illustrating the use of cudaMemPrefetchAsync() to transfer data from CPU memory to GPU memory, highlighting asynchronous memory prefetching to avoid GPU stalls.](images/aisp_0617.png) ... ###### Figure 6-17. Streaming data from CPU to GPU over NVLink-C2C with cudaMemPrefetchAsync() ... ###### Figure 6-18. Specifying “preferred location” to tell the CUDA driver how the data is mostly used (e.g., ReadMostly for largely read-only workloads) ... You can also call the following to let a second GPU map those pages without triggering migrations at launch:"
  },
  {
    "file_name": "aisp_0619.png",
    "caption": "Diagram illustrating parallel vector addition, showing elements from Vector A and Vector B being added to produce Vector C.",
    "description": "Diagram illustrating parallel vector addition, showing elements from Vector A and Vector B being added to produce Vector C.",
    "chapter": "This launches N tiny GPU operations serially",
    "page_context": "This code effectively uses the GPU like a scalar, nonparallel processor. It achieves very low occupancy similar to the previous native `addSequential` CUDA C++ code. ... Let’s optimize the CUDA kernel and PyTorch code to implement a parallel version of the vector add operation. Figure 6-19 shows how a vectorized add operation works. ... ###### Figure 6-19. Vectorized addition operating happening in parallel across elements in the vectors ... In the following CUDA C++ code, we launch enough threads to cover all elements (`<<< (N+255)/256, 256 >>>`) so that 256 threads per block process <em>N</em> elements in parallel across however many blocks are needed:"
  },
  {
    "file_name": "aisp_0620.png",
    "caption": "Diagram comparing parallel and sequential timelines, showing continuous GPU activity in the parallel version versus idle gaps in the sequential version.",
    "description": "Diagram comparing parallel and sequential timelines, showing continuous GPU activity in the parallel version versus idle gaps in the sequential version.",
    "chapter": "Parallel add",
    "page_context": "The parallel kernel running within multiple warps, in particular, benefits from warp-level latency hiding. While one warp is waiting for a memory load, another warp can be executing the add computation, while yet another could be fetching the next data, etc. We’ll explore many techniques to hide memory latency in the upcoming chapters. ... In the sequential kernel, there are no other warps to run while one is waiting, so the hardware pipelines often sit idle. The timeline is one long series of operations with idle gaps during memory waits. In the parallel version, those gaps are filled by other warps’ work, so the GPU is busy continuously. The comparison is shown in Figure 6-20. ... ###### Figure 6-20. Parallel versus sequential timeline comparison ... Here, the sequential timeline is one long series of operations with idle gaps during memory waits. In the parallel version, those gaps are filled by other warps’ work, so the GPU is busy continuously."
  },
  {
    "file_name": "aisp_0621.png",
    "caption": "Diagram of the roofline model for the Blackwell GPU, showing peak compute performance at 80 TFLOPS, peak memory bandwidth at 8 TB/s, and the example kernel at an arithmetic intensity of 0.083 FLOPS/byte, indicating a memory-bound regime.",
    "description": "Diagram of the roofline model for the Blackwell GPU, showing peak compute performance at 80 TFLOPS, peak memory bandwidth at 8 TB/s, and the example kernel at an arithmetic intensity of 0.083 FLOPS/byte, indicating a memory-bound regime.",
    "chapter": "Roofline Model: Compute-Bound or Memory-Bound Workloads",
    "page_context": "Here, we see that the ridge point for the Blackwell GPU is the sustained FLOPs/sec divided by the sustained HBM bandwidth. Here, it’s the intersection point shown at 10 FLOPs/byte. Our example kernel’s arithmetic intensity is to the left along the slanted, memory-bandwidth diagonal at 0.083 FLOPs/byte. As such, this kernel lies on the slanted, memory-bandwidth ceiling of the roofline. This confirms that it is memory bound. ... To make this kernel less memory bound (and thus more compute bound), you can increase its arithmetic intensity by doing more work per byte of data. This will move the kernel to the right, which pushes performance up toward the compute roofline. ... ###### Figure 6-21. Roofline model for a Blackwell-class GPU (~80 TFLOPs/sec FP32, ~8 TB/s HBM3e) showing our kernel’s point and the ~10 FLOPs/byte arithmetic intensity ridge ... One simple way to make the kernel less memory bound is to use lower-precision data. For instance, if you used 16-bit floats (FP16) instead of 32-bit (FP32), you’d halve the bytes transferred per operation and instantly double the FLOPs/byte intensity."
  },
  {
    "file_name": "aisp_0622.png",
    "caption": "Diagram showing synchronous data transfer and computation occurring sequentially, contrasted with asynchronous transfer overlapping with computation.",
    "description": "Diagram showing synchronous data transfer and computation occurring sequentially, contrasted with asynchronous transfer overlapping with computation.",
    "chapter": "Roofline Model: Compute-Bound or Memory-Bound Workloads",
    "page_context": "For instance, you can use NVTX to label regions as “memory copy” or “kernel execution” and see them in the Nsight Systems timeline. This is incredibly useful to confirm overlapping host-device transfers with compute, as discussed earlier. ... For instance, to verify the overlap, you can mark the start/end of both the data transfer and kernel calls with NVTX markers. Nsight Systems will show these NVTX ranges on a timeline, making it easy to see overlap. With asynchronous memory copies (`cudaMemcpyAsync`), the data transfer overlaps with kernel execution on the GPU (see [Figure 6-22](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch06.html#ch06_figure_22_1757308039123878)), comparing a synchronous and asynchronous memory <span class=\"keep-together\">transfer</span>. ... ###### Figure 6-22. Synchronous (sequential) and asynchronous (overlapping) data transfers with kernel computations ... If you expect overlap but see the copies and kernels running sequentially versus parallel, then it’s something like an unwanted default-stream synchronization. Otherwise, a missing pinned-memory buffer is likely preventing true overlap."
  },
  {
    "file_name": "aisp_0701.png",
    "caption": "Diagram illustrating the difference between perfectly coalesced and misaligned memory access in GPU threads, highlighting the efficiency of coalesced access with full-width loads versus fragmented access in misaligned scenarios.",
    "description": "Diagram illustrating the difference between perfectly coalesced and misaligned memory access in GPU threads, highlighting the efficiency of coalesced access with full-width loads versus fragmented access in misaligned scenarios.",
    "chapter": "Coalesced Versus Uncoalesced Global Memory Access",
    "page_context": "For example, if a warp’s first thread starts at an address that isn’t 128-byte aligned, the warp’s memory request will cross a cache-line boundary, often resulting in two 128-byte transactions instead of one. In that case the warp may fetch an extra sector beyond the optimal four sectors, for a total of five sectors across the two lines. This is a waste of bandwidth. Whether a misaligned, contiguous 128-byte warp load touches 5× 32 B sectors or 8× 32 B sectors depends on the start offset. Aligned accesses keep it to 4× 32 B sectors. ... In the coalesced case, however, threads load from consecutive addresses combined into a single wide transaction. Figure 7-1 compares coalesced and uncoalesced memory access. ... ###### Figure 7-1. Comparing coalesced versus uncoalesced memory access pattern ... In kernel code, this problem typically appears as strided or irregular indexing such that each thread reaches into different cache lines. When a kernel’s threads fetch data with strided or irregular indices, the GPU issues many small, uncoalesced global‐memory transactions rather than a handful of full‐width loads."
  },
  {
    "file_name": "aisp_0702.png",
    "caption": "Diagram illustrating the difference between an array of structures (AoS) and a structure of arrays (SoA) in data organization.",
    "description": "Diagram illustrating the difference between an array of structures (AoS) and a structure of arrays (SoA) in data organization.",
    "chapter": "Coalesced Versus Uncoalesced Global Memory Access",
    "page_context": "In Nsight Compute, the Memory Workload Analysis section will show lower Global Memory Load Efficiency, higher DRAM sector read counts, and average sectors per request above 4.0 when uncoalesced patterns are present. This is because more sectors are being fetched than a properly coalesced memory access pattern, and it indicates that you’re wasting bandwidth by fetching mostly unused bytes. And DRAM throughput percentage will remain well below peak. This confirms that your warp is spending cycles stalled on memory rather than driving the ALUs. ... To break out of this memory‐bound regime, you can reorganize your data so that each warp’s 32 threads load contiguous elements. Either index the array with `input[idx]` where `idx = blockIdx.x * blockDim.x + threadIdx.x` or switch to a structure‐of‐arrays (SoA) layout so that thread `i` always touches element `i`. The difference between an array of structures (AoS) and an SoA is shown in [Figure 7-2](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch07.html#ch07_figure_2_1757308047295166). ... ###### Figure 7-2. Array of structures (AoS) versus structure of arrays (SoA) ... Once you make the change, the hardware will automatically combine the warp’s global memory loads into fewer, wider transactions with more usable (less wasted) data being returned. The Nsight Compute counters will immediately show improvement."
  },
  {
    "file_name": "aisp_0703.png",
    "caption": "Diagram illustrating coalesced, strided, and random memory access patterns with arrows representing different access methods.",
    "description": "Diagram illustrating coalesced, strided, and random memory access patterns with arrows representing different access methods.",
    "chapter": "Usage",
    "page_context": "This PyTorch snippet uses `torch.index_select` with a strided index pattern, which causes the underlying GPU gather kernel to perform uncoalesced loads. Specifically, a warp of 32 threads will access addresses that are `stride * 4 bytes` apart. ... This does not allow a single wide transaction and instead generates 32 separate loads. Each thread loads a value from `inp` that is far apart in memory from the value loaded by the next thread, which prevents coalescing. [Figure 7-3](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch07.html#ch07_figure_3_1757308047295185) shows the coalesced versus strided access pattern—as well as random access. ... ###### Figure 7-3. Coalesced, strided, and random memory access patterns ... After running the C++ and PyTorch codes on a GPU, we measure performance metrics. In the uncoalesced version, each warp’s memory request is broken into 8 separate 32-byte sectors on average. Since each 128-byte cache line fetch is split into 4 separate 32-byte sectors (more on this number 4 in a bit), the access pattern spans two lines per warp to retrieve the 8 separate 32-byte sectors."
  },
  {
    "file_name": "aisp_0704.png",
    "caption": "Diagram illustrating conflict-free memory bank accesses by threads, showing direct alignment of threads to banks without overlap.",
    "description": "Diagram illustrating conflict-free memory bank accesses by threads, showing direct alignment of threads to banks without overlap.",
    "chapter": "Avoid Shared-Memory Bank Conflicts",
    "page_context": "On modern NVIDIA GPUs,[]()[]()[]() including Blackwell, shared memory has 32 banks with a 4-byte bank width (i.e., addresses map `mod 32`). As such, a warp that strides by 128 B (32 floats × 4 B) maps all threads to the same bank. If multiple threads in a warp access the same bank, a <em>bank conflict</em> occurs. This forces the memory accesses to serialize, which negates the speed advantage of shared memory. ... In code, bank conflicts often occur when threads access a shared-memory array with a stride that causes them to fall into the same bank. Figure 7-4 shows two examples of conflict-free memory bank accesses. ... ###### Figure 7-4. No bank conflicts ... Here, there are no two threads accessing the same memory bank concurrently. This is ideal. Figure 7-5 shows examples of both a 2-way and 16-way bank conflict."
  },
  {
    "file_name": "aisp_0705.png",
    "caption": "Diagram illustrating 2-way and 16-way bank conflicts, showing how multiple threads interact with memory banks, causing access issues.",
    "description": "Diagram illustrating 2-way and 16-way bank conflicts, showing how multiple threads interact with memory banks, causing access issues.",
    "chapter": "Avoid Shared-Memory Bank Conflicts",
    "page_context": "###### Figure 7-4. No bank conflicts ... Here, there are no two threads accessing the same memory bank concurrently. This is ideal. Figure 7-5 shows examples of both a 2-way and 16-way bank conflict. ... ###### Figure 7-5. 2-way and 16-way bank conflicts ... Here, multiple threads are accessing the same memory bank, which will cause conflicts and impact performance. A classic example of a bank conflict is a naive matrix transpose that uses a 32 × 32 shared memory tile. If 32 threads each read `tile[i][threadIdx.x]` such that the same column index `(threadIdx.x)` is read across <span class=\"keep-together\">different</span> rows (`i`), all 32 threads in the warp will each access memory addresses that lie in the same shared-memory bank, causing a 32-way bank conflict."
  },
  {
    "file_name": "aisp_0706.png",
    "caption": "Diagram showing how adding padding to a shared memory array reduces bank conflicts by offsetting memory accesses across different banks.",
    "description": "Diagram showing how adding padding to a shared memory array reduces bank conflicts by offsetting memory accesses across different banks.",
    "chapter": "Avoid Shared-Memory Bank Conflicts",
    "page_context": "Always choose your stride and data layout so that threads in the same warp hit different banks and avoid that serializing bottleneck. ... The solution is to adjust data layouts in shared memory to avoid conflicts. []()[]()A common technique is <em>padding</em> shared arrays so that each row, or each memory-access pattern, maps to different banks. For instance, if you have a 32 × 32 shared tile, you can declare it as `[32][33]` by adding one extra padding column so that each row now occupies 33 floats. This extra 1-element offset means that when thread `k` of a warp accesses `tile[i][k]`, successive rows start at addresses that shift across shared-memory banks. This keeps all threads from hitting the same bank, as shown in [Figure 7-6](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch07.html#ch07_figure_6_1757308047295229). ... ###### Figure 7-6. Avoiding bank conflicts with padding ... By changing the stride to 33, no two threads in a warp will contend for the same bank when accessing a given column. This eliminates what would have been a 32-way bank conflict."
  },
  {
    "file_name": "aisp_0707.png",
    "caption": "Diagram illustrating the TMA unit's role within a streaming multiprocessor (SM) for asynchronous data transfer from global memory to shared memory.",
    "description": "Diagram illustrating the TMA unit's role within a streaming multiprocessor (SM) for asynchronous data transfer from global memory to shared memory.",
    "chapter": "Asynchronous Memory Prefetching and Tensor Memory Accelerator",
    "page_context": "On Blackwell, for instance, a full DRAM round-trip is on the order of hundreds of cycles before any computation can begin. To hide that latency, we need to overlap data transfer with compute. This overlap is what hides most of the DRAM latency. ... CUDA’s Pipeline API together with the Tensor Memory Accelerator (TMA) hardware engine take this idea to the thread-block level. Instead of having each warp use the SM’s load and store (LD/ST) units to fetch data from global memory, you can invoke the TMA engine to asynchronously fetch an entire tile from global memory into shared memory, as shown in Figure 7-7. ... ###### Figure 7-7. TMA asynchronously fetching data from global HBM into shared memory ... To start the TMA transfer, you can[]() call `cuda::memcpy_async()`. On modern GPU architectures, `cuda::memcpy_async()` together with `cuda::pipeline` exposes the hardware engines for asynchronous global to shared transfers. This includes the TMA when available. This will use TMA’s on-chip DMA engine to perform the asynchronous bulk data transfer. This is implemented in CUDA as follows:"
  },
  {
    "file_name": "aisp_0801.png",
    "caption": "Screenshot of the Nsight Systems interface showing a detailed timeline of CPU and GPU activity, including process and kernel utilization, relevant for understanding performance profiling.",
    "description": "Screenshot of the Nsight Systems interface showing a detailed timeline of CPU and GPU activity, including process and kernel utilization, relevant for understanding performance profiling.",
    "chapter": "Nsight Systems Timeline View",
    "page_context": "In addition, the Nsight Systems GUI lets you interactively inspect the timeline. It visualizes CPU threads, GPU kernels, and even user-defined NVTX ranges with zoom and pan features for detailed analysis (see Figure 8-1). ... Remember that NVTX annotations[]() are essential for complex applications. Use NVTX ranges in your code to mark regions of interest. You can then use Nsight Systems to capture range profiles. And while the CUDA profiler Start and Stop API is supported for capture control, NVTX ranges are the recommended mechanism for framework workflows. For instance, you can use `NVTX` range push/pop, available using `torch.cuda.nvtx` in PyTorch, to label phases such as “forward pass” and “backprop.” This makes the Nsight Systems timeline far more interpretable since the profiler can capture the performance-critical iterations and clearly delineate key computation <span class=\"keep-together\">segments</span>. ... ###### Figure 8-1. Nsight Systems interactive UI (source: https://oreil.ly/YEiWS) ... ## Profiling and Tuning the Data Pipeline"
  },
  {
    "file_name": "aisp_0802.png",
    "caption": "Roofline chart from Nsight Compute showing the relationship between performance in FLOP/s and arithmetic intensity in FLOP/byte, with boundaries for memory bandwidth and peak performance.",
    "description": "Roofline chart from Nsight Compute showing the relationship between performance in FLOP/s and arithmetic intensity in FLOP/byte, with boundaries for memory bandwidth and peak performance.",
    "chapter": "Nsight Compute and Roofline Analysis",
    "page_context": "Nsight Compute includes a Roofline analysis section as well as automated guidance. This can plot your kernel’s achieved FLOPS against hardware rooflines—and even highlight if you are near the memory bandwidth or compute limits. ... Remember that the memory-versus-compute distinction is quantified using the Roofline model, which plots kernel performance against hardware ceilings for memory bandwidth and compute throughput. Nsight Compute now directly provides Roofline analysis, showing each kernel’s achieved GFLOPS relative to peak and its arithmetic intensity (FLOPS per byte). A kernel falling below the memory roof indicates memory-bound behavior, while one near the compute roof is ALU bound. Use the Roofline section in Nsight Compute to obtain arithmetic intensity (FLOPs/byte) and FLOPs by precision directly. Compare this to the hardware’s theoretical peak FLOPS per byte, you can see how far below the roofline the kernel is operating. An example Roofline chart is shown in Figure 8-2. ... ###### Figure 8-2. Roofline chart shown in the Nsight Compute UI (https://oreil.ly/wUbIz) ... It’s often effective to first use Nsight Systems to find hot kernels or bottleneck operations on the timeline. Then, you can zoom into each of these kernels with Nsight Compute to perform a more fine-grained analysis and diagnosis. This two-step workflow, moving from a system-wide view to a kernel-level deep dive is a common approach to handling complex GPU performance debugging and tuning."
  },
  {
    "file_name": "aisp_0803.png",
    "caption": "A Chrome tracing visualization from the PyTorch profiler showing detailed execution steps of a model, illustrating how Kineto's integration captures and organizes execution data for performance analysis.",
    "description": "A Chrome tracing visualization from the PyTorch profiler showing detailed execution steps of a model, illustrating how Kineto's integration captures and organizes execution data for performance analysis.",
    "chapter": "PyTorch Profiler and Visualization Tools",
    "page_context": "When using high-level frameworks[]()[]()[]()[]()[]()[]()[]() like PyTorch, the `torch.profiler` [API](https://oreil.ly/GlT7l) can collect similar performance metrics during model training/inference. The PyTorch profiler uses the Kineto library to actually perform the data collection under the hood. ... [Kineto](https://oreil.ly/Rwwwh) integrates with CUDA’s Performance Tools Interface ([CUPTI](https://oreil.ly/oO6Ti)) backend to capture[]()[]()[]()[]() operator-wise execution times, GPU kernel launches, memory copies, and hardware counter metrics. It also integrates with Linux `perf` to record CPU events. Kineto merges all of this information into a coherence, time-ordered trace, which can be visualized using the PyTorch profiler UI, Nsight Systems GUI, or just a Chrome browser (e.g., Chrome tracing format). An example Chrome tracing visualization is shown in [Figure 8-3](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch08.html#ch08_figure_3_1757308048846258). ... ###### Figure 8-3. Chrome tracing visualization generated by the PyTorch profiler ... PyTorch profiler allows you to identify bottleneck operations in your model code directly. For example, you can profile a training loop with `torch.profiler.profile(..., with_flops=True, profile_memory=True)` to record memory usage and to estimate FLOPs for supported operators such as matrix multiplication. (Note: These are formula-based estimates at the operator level rather than per-kernel hardware counters.) Such integration makes it easier to bridge the gap between PyTorch model code and low-level CUDA performance analysis."
  },
  {
    "file_name": "aisp_0804.png",
    "caption": "Diagram illustrating how long scoreboard stalls occur due to high-latency memory loads from global memory into registers and shared memory.",
    "description": "Diagram illustrating how long scoreboard stalls occur due to high-latency memory loads from global memory into registers and shared memory.",
    "chapter": "Memory-Related Stalls",
    "page_context": "## Memory-Related Stalls ... If a kernel is waiting on global memory loads, Nsight Compute reports a high percentage of “Stall: Long Scoreboard” cycles. The scoreboard tracks each warp’s outstanding memory requests, so Long Scoreboard indicates warps frequently waiting on the high latency of global DRAM loads, as shown in Figure 8-4. ... ###### Figure 8-4. Long Scoreboard stall caused by waiting on high-latency global memory accesses (e.g., waiting for data fetched from device memory into registers, or for spilled local memory to be written to/from device memory) ... Similarly, a Short Scoreboard stall is caused by waiting on memory transfers between shared memory and the registers. This is shown in Figure 8-5."
  },
  {
    "file_name": "aisp_0805.png",
    "caption": "Diagram illustrating a Short Scoreboard stall caused by delays in data transfer between shared memory and registers.",
    "description": "Diagram illustrating a Short Scoreboard stall caused by delays in data transfer between shared memory and registers.",
    "chapter": "Memory-Related Stalls",
    "page_context": "###### Figure 8-4. Long Scoreboard stall caused by waiting on high-latency global memory accesses (e.g., waiting for data fetched from device memory into registers, or for spilled local memory to be written to/from device memory) ... Similarly, a Short Scoreboard stall is caused by waiting on memory transfers between shared memory and the registers. This is shown in Figure 8-5. ... ###### Figure 8-5. Short Scoreboard stall caused by high-latency data transfers between shared memory and the registers ... Similarly, metrics labeled “Stall: Memory Throttle” mean the load/store pipelines are saturated so that no additional memory requests can be issued because the hardware’s memory queues are full. “Stall: Not Selected” means that although warps are eligible to issue, they must wait for free memory transaction slots before they can proceed. Whenever these memory-related stall reasons dominate your stall profile, it is a clear sign the kernel is memory bound."
  },
  {
    "file_name": "aisp_0806.png",
    "caption": "Diagram illustrating the balance between resource usage and parallelism for occupancy, showing low occupancy with complex algorithms and high occupancy with simple algorithms.",
    "description": "Diagram illustrating the balance between resource usage and parallelism for occupancy, showing low occupancy with complex algorithms and high occupancy with simple algorithms.",
    "chapter": "Tuning Occupancy",
    "page_context": "Occupancy is the measure of how many threads, or warps, are active on an SM relative to the hardware’s maximum capacity. Higher occupancy, or more warps in flight, allows the GPU to better hide latency. This is because when one warp stalls waiting on a memory load, for instance, the scheduler can quickly switch to run another warp. ... More warps per SM generally means the GPU’s pipelines stay busier, and fewer cycles are wasted waiting on memory. Occupancy tuning is the practice of adjusting your kernel launch parameters and resource usage (e.g., registers and shared memory) to maximize useful parallelism on each SM, as shown in Figure 8-6. ... ###### Figure 8-6. Tuning occupancy by balancing resource usage (e.g., registers per thread and shared memory) with parallelism (e.g., number of threads and warps) ... Remember that the goal of occupancy tuning is to keep enough warps active to fully utilize the SM’s pipelines and hide long‐latency operations. In an ideal case, you would achieve 100% occupancy, filling all available warp slots. For instance, there are 64 warp slots, or 2,048 threads, available in each Blackwell B200 (compute capability 10.0) SM."
  },
  {
    "file_name": "aisp_0807.png",
    "caption": "Diagram illustrating divergent and nondivergent warp execution, showing how warps execute 'if' and 'else' branches sequentially, resulting in idle threads and reduced throughput.",
    "description": "Diagram illustrating divergent and nondivergent warp execution, showing how warps execute 'if' and 'else' branches sequentially, resulting in idle threads and reduced throughput.",
    "chapter": "Causes of Warp Divergence",
    "page_context": "In SIMT execution, the warp must execute both paths serially: first, all threads that take the `if` branch execute while the others in that warp are masked off (idle). Then the threads that took `else` execute while the first group idles, as shown in [Figure 8-7](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch08.html#ch08_figure_7_1757308048846327). ... During the divergent sections, effectively only half, or some fraction, of the warp is doing useful work. This reduces overall throughput. In the case of a 50/50 split between `if` and `else`, the warp’s active utilization drops to 50% for that portion of code. If the split is 1 thread versus 31 threads, then 31/32 threads will be idle in one of the subbranches. ... ###### Figure 8-7. Divergent versus nondivergent warp execution ... If your kernel contains multiple divergence points or if each divergent branch carries heavier work, removing those branches can compound these gains. In the ideal case (e.g., a 50/50 split branch), removing that divergent branch can nearly double throughput up to ~2× speedup. Eliminating multiple heavy divergence paths can compound these gains."
  },
  {
    "file_name": "aisp_0808.png",
    "caption": "Diagram showing instruction-level parallelism (ILP) with overlapping load, multiply, and add units, illustrating how multiple instructions are issued and processed concurrently by different warps over time.",
    "description": "Diagram showing instruction-level parallelism (ILP) with overlapping load, multiply, and add units, illustrating how multiple instructions are issued and processed concurrently by different warps over time.",
    "chapter": "Exposing Instruction-Level Parallelism",
    "page_context": "You can rearrange or unroll your code so that each thread issues multiple independent operations (e.g., memory loads and arithmetic instructions) before consuming their results. This way, the GPU keeps its execution units busy while earlier instructions are still pending. ... Leveraging ILP allows a single warp to issue certain independent instructions back-to-back, which improves latency hiding. For instance, a thread might load data and then perform unrelated arithmetic while waiting for that load to complete. Figure 8-8 shows an example of multiple instructions overlapping during each cycle. ... ###### Figure 8-8. Overlapping with ILP ... By unrolling your loop body, you turn what was once “load → multiply → store” each iteration into a sequence that loads and multiplies multiple elements before looping back. For example, instead of:"
  },
  {
    "file_name": "aisp_0809.png",
    "caption": "Diagram illustrating the architecture of a streaming multiprocessor with four warp schedulers, each capable of dispatching up to two instructions per cycle.",
    "description": "Diagram illustrating the architecture of a streaming multiprocessor with four warp schedulers, each capable of dispatching up to two instructions per cycle.",
    "chapter": "Warp Scheduling and Dual Issue Instructions",
    "page_context": "## Warp Scheduling and Dual Issue Instructions ... Each SM contains multiple warp schedulers. For example, Blackwell SMs have four warp schedulers. Each scheduler can issue up to two independent instructions per cycle, as shown in Figure 8-9. ... ###### Figure 8-9. Each warp scheduler can dispatch up to two instructions per cycle called “dual issue” ... In practice, this means that a single warp can overlap multiple independent operations across successive cycles—and sometimes even in the same cycle if they target different pipelines such as one math, one memory, or one special-function unit (SFU) pipeline. This means that multiple instructions can be in flight and progressing through the pipeline simultaneously. This overlap is the ILP that we discussed earlier."
  },
  {
    "file_name": "aisp_1001.png",
    "caption": "Diagram illustrating a two-stage producer-consumer pattern using the CUDA Pipeline API, showcasing the overlap of memory loads and computations with synchronized stages.",
    "description": "Diagram illustrating a two-stage producer-consumer pattern using the CUDA Pipeline API, showcasing the overlap of memory loads and computations with synchronized stages.",
    "chapter": "Cooperative Tiling and Double-Buffering with the CUDA Pipeline API",
    "page_context": "## Cooperative Tiling and Double-Buffering with the CUDA Pipeline API ... You can implement the traditional[]()[]()[]()[]() double‐buffered tiling pattern using the C++ Pipeline API by instantiating a two‐stage pipeline to overlap memory loads and computations. Specifically, you can declare a two-stage `cuda::pipeline_shared_state​<cuda::thread_scope_block, 2>` object, which is scoped to a specific thread block using cooperative groups (discussed in a bit). This is essentially a producer-consumer pattern, as shown in [Figure 10-1](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch10.html#ch10_figure_1_1757308054816426). ... ###### Figure 10-1. Two-stage producer-consumer pattern with the CUDA Pipeline API ... The key CUDA Pipeline API calls are shown next. They are followed by an implementation of a double-buffered, cooperative tiling kernel using this API to demonstrate modern CUDA techniques that align with hardware features:"
  },
  {
    "file_name": "aisp_1002.png",
    "caption": "Diagram illustrating the producer-consumer implementation to hide DRAM latency during Tensor Core computations, showing the sequence of loading and computing operations across two output tiles.",
    "description": "Diagram illustrating the producer-consumer implementation to hide DRAM latency during Tensor Core computations, showing the sequence of loading and computing operations across two output tiles.",
    "chapter": "Cooperative Tiling and Double-Buffering with the CUDA Pipeline API",
    "page_context": "The two stages in the pipeline overlap global-memory loads with computations. In the first, Stage 0, one warp in the thread block issues an asynchronous prefetch for the next tile into shared memory. The prefetch issues cooperative `cuda::memcpy_async` copies that lower to per thread `cp.async` into shared memory. ... While Stage 0 is producing (loading) the data in one warp, the remaining warps in the thread block are consuming (computing) the loaded data in the second stage, 1. This simple producer-consumer implementation hides DRAM latency with ongoing computations, as shown in Figure 10-2. ... ###### Figure 10-2. Hiding global DRAM load (L) latency with (C) compute using a producer–consumer pipeline ... This pattern can raise SM utilization and reduce kernel time, but whether you are compute bound or memory bound depends on the operation, tile sizes, and overlap efficiency. Even highly optimized attention kernels like FlashAttention-3 report around 75% percent of peak FP16 FLOPs due to practical limits in overlap and data movement."
  },
  {
    "file_name": "aisp_1003.png",
    "caption": "Diagram illustrating a nonwarp specialized kernel where each warp performs both data loading and computation tasks sequentially, separated by a barrier.",
    "description": "Diagram illustrating a nonwarp specialized kernel where each warp performs both data loading and computation tasks sequentially, separated by a barrier.",
    "chapter": "Warp Specialization and the Producer-Consumer Model",
    "page_context": "## Warp Specialization and the Producer-Consumer Model ... Warp specialization extends double buffering by assigning operations to warps that use different hardware, such as data movement (e.g., TMA) and compute (e.g., Tensor Cores). This is in contrast to reusing the same warps for both loading data and computing, as shown in Figure 10-3. ... ###### Figure 10-3. Nonwarp specialized kernel with each warp performing a mix of both data loading and compute (source: https://oreil.ly/WZDbM) ... This type of specialization allows each set of warps to have their own instruction sequences. As such, instructions are issued and executed continuously without being interrupted by other types of operations. Specifically, warp specialization lets you assign one set of “producer” or “memory” warps to prefetch tiles asynchronously using `cuda::memcpy_async`. Then all other “consumer” or “compute” warps perform the computations, as shown in [Figure 10-4](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch10.html#ch10_figure_4_1757308054816499)."
  },
  {
    "file_name": "aisp_1004.png",
    "caption": "Diagram illustrating warp specialization with distinct instruction sequences for producer and consumer roles, highlighting data loading and computation phases separated by barriers.",
    "description": "Diagram illustrating warp specialization with distinct instruction sequences for producer and consumer roles, highlighting data loading and computation phases separated by barriers.",
    "chapter": "Warp Specialization and the Producer-Consumer Model",
    "page_context": "Here, four warps are assigned the producer role, while the remaining eight warps are assigned the consumer role. Like most producer-consumer patterns, you can assign a different number of warps for the producer and consumer. ... Because each warp has its own scheduler, the GPU can issue a load instruction, a math instruction, and a write instruction—all in the same cycle from different warps using different warp schedulers. So an SM with multiple schedulers can issue a memory instruction from Warp 0, a math instruction from Warp 1, and so on, in one cycle. ... ###### Figure 10-4. Warp-specialized kernel with one set of warps for loading data and all other warps for computations (source: https://oreil.ly/WZDbM) ... This effectively creates a thread-block-level multi-issue scenario across warps. This is not possible with single-warp double buffering because a single warp’s scheduler can issue only one instruction per cycle."
  },
  {
    "file_name": "aisp_1005.png",
    "caption": "Diagram illustrating a three-role warp-specialized pipeline with warps designated for data loading, computing, and storing, showcasing synchronization barriers and data flow.",
    "description": "Diagram illustrating a three-role warp-specialized pipeline with warps designated for data loading, computing, and storing, showcasing synchronization barriers and data flow.",
    "chapter": "Warp Specialization and the Producer-Consumer Model",
    "page_context": "This warp-specialized pipeline squeezes out the idle cycles that a single-warp, double-buffered, sequential load-and-compute loop cannot address. Warp specialization’s efficient overlap of data transfer and computation increases GPU utilization—especially for long-running loops and persistent kernels. In these cases, the overhead of role coordination and data handoff is amortized over many iterations. ... A paper on warp scheduling demonstrated that warp specialization can achieve nearly perfect overlap of memory and compute. In this case, the GPU kernel had distinct memory and compute phases such that memory and compute took turns being the bottleneck. By applying warp specialization, their workload transformed into a state in which both the SM’s memory subsystem and compute units were simultaneously busy almost the entire time. ... ###### Figure 10-5. Three-role warp-specialized pipeline configuration with one set of warps for loading data, another set for compute, and another set for data storing (source: https://oreil.ly/xs7YN) ... The profiling showed that before using warp specialization, their L2 bandwidth utilization and Tensor Core utilization were out of phase. After warp specialization, L2 bandwidth and Tensor Core utilization became in-phase. This resulted in much higher effective throughput and showed that warp specialization can squeeze out the last bits of idle time—even for a well-tuned asynchronous pipeline that might be left on the table."
  },
  {
    "file_name": "aisp_1006.png",
    "caption": "Diagram illustrating the ping-pong architecture with three-role warp-specialized kernel, showing the interaction between producer and consumer warpgroups over time with TMA load, MMA, and epilogue stages.",
    "description": "Diagram illustrating the ping-pong architecture with three-role warp-specialized kernel, showing the interaction between producer and consumer warpgroups over time with TMA load, MMA, and epilogue stages.",
    "chapter": "Warp Specialization and the Producer-Consumer Model",
    "page_context": "The profiling showed that before using warp specialization, their L2 bandwidth utilization and Tensor Core utilization were out of phase. After warp specialization, L2 bandwidth and Tensor Core utilization became in-phase. This resulted in much higher effective throughput and showed that warp specialization can squeeze out the last bits of idle time—even for a well-tuned asynchronous pipeline that might be left on the table. ... Another warp specialization pattern is a modification of the three-role warp-specialized pipeline. It assigns a set of warps to the memory loader as before, but then uses two sets of consumer warps that “ping-pong” between the roles of compute and memory-storer. This three-role warp-specialization architecture is exposed in CUTLASS as [gemm_tma_warpspecialized_pingpong](https://oreil.ly/xs7YN) and shown in [Figure 10-6](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch10.html#ch10_figure_6_1757308054816535). ... ###### Figure 10-6. Ping-pong architecture with three-role warp-specialized kernel (source: https://oreil.ly/xs7YN) ... Here, the consumer MMAs are overlapping and include a small amount of post-MMA wrap-up, or epilogue, processing. This per-MMA epilogue cleanup is required before launching the next MMA. Specifically, the epilogue can include accumulating, scaling, writing back to global memory, or shuffling results to another warp. Additionally, the epilogue can perform housekeeping like advancing tile pointers, updating loop counters, and signaling that this tile is done so the next TMA load or MMA can kick off."
  },
  {
    "file_name": "aisp_1007.png",
    "caption": "Diagram illustrating a block-wide barrier where threads synchronize between register files and shared memory, shown by arrows through different stages labeled \"Load\" and \"MMA.\"",
    "description": "Diagram illustrating a block-wide barrier where threads synchronize between register files and shared memory, shown by arrows through different stages labeled \"Load\" and \"MMA.\"",
    "chapter": "Using CUDA Pipeline API for Warp Specialization",
    "page_context": "By comparison, the Pipeline API maintains per-stage state internally. When a producer warp finishes its asynchronous copy and calls `pipe.producer_commit`, only the warps that call `pipe.consumer_wait` will block until the data is ready. Other warps in the block can continue running any work that does not depend on that stage. In practice, the CUDA Pipeline API reduces idle time and decreases stalled warps because it eliminates the need to pause the entire block with a barrier. With pipelines you coordinate producer and consumer handoffs at a finer granularity than a hand-coded `async-copy` sequence (e.g., PTX `cp.async + __syncthreads()`). ... You can implement warp specialization with the CUDA Pipeline API in a three-role pattern. A loader warp produces inputs for a compute warp, the compute warp consumes those inputs and produces results, and a storer warp consumes those results and writes them out. The pipeline object is block scoped, and it tracks the stage order internally. ... ###### Figure 10-7. Block-wide barrier prevents threads from proceeding until they synchronize and load the new data ... Here is an example of a warp-specialized, three-role kernel that computes tiles of data using a loader warp (`warp_id 0`), compute warp (`warp_id 1`), and storer warp (`warp_id 2`):"
  },
  {
    "file_name": "aisp_1008.png",
    "caption": "Bar chart comparing decoding throughput of Llama-1B for batch-size 1 across different GPUs, showing megakernels have higher throughput than VLLM and SGLang.",
    "description": "Bar chart comparing decoding throughput of Llama-1B for batch-size 1 across different GPUs, showing megakernels have higher throughput than VLLM and SGLang.",
    "chapter": "Megakernels for Inference",
    "page_context": "## Megakernels for Inference ... Additionally, a modern approach to persistent kernels originating from large-scale inference is called a megakernel. A megakernel fuses entire sequences of operations across layers—and even across GPUs—into a single large kernel. As shown in Figure 10-8, persistent megakernels have shown to reduce latency by 1.2× to 6.7× versus traditional per-layer launches by eliminating repeated kernel launch overhead. ... ###### Figure 10-8. Decode throughput improvement with megakernels relative to vLLM and SGLang (source: https://oreil.ly/2aZiF) ... ## Persistent Kernels and Warp Specialization"
  },
  {
    "file_name": "aisp_1009.png",
    "caption": "Diagram illustrating synchronization across various granularities of threads using cooperative groups, with different sections showing synchronization points labeled \"Sync.\"",
    "description": "Diagram illustrating synchronization across various granularities of threads using cooperative groups, with different sections showing synchronization points labeled \"Sync.\"",
    "chapter": "Cooperative Groups",
    "page_context": "# Cooperative Groups ... Cooperative groups let you define and synchronize groups of threads at arbitrary granularities. For example, you can create groups with individual threads, warps, tiles, blocks, and clusters, as shown in Figure 10-9. ... ###### Figure 10-9. Synchronizing across different granularities of threads using cooperative groups ... Cooperative groups provide safe, reusable collectives like sync, broadcast, and reduce. This is in contrast to using ad hoc synchronization barriers. []()[]()Normally, threads can only synchronize within their own block using `__syncthreads()`—and there is no built-in global barrier for the entire grid, for example."
  },
  {
    "file_name": "aisp_1010.png",
    "caption": "Diagram illustrating how multiple thread block clusters are coscheduled on the same GPU processing cluster (GPC), ensuring efficient communication among SMs within the cluster.",
    "description": "Diagram illustrating how multiple thread block clusters are coscheduled on the same GPU processing cluster (GPC), ensuring efficient communication among SMs within the cluster.",
    "chapter": "Thread Block Clusters and Distributed Shared Memory",
    "page_context": "A cooperative group is a software-level abstraction that provides an API that lets you carve up kernel threads into arbitrary collectives for synchronization and data movement. This includes warps, tiles, thread blocks—even entire grids or multidevice grids. ... In contrast, a thread block cluster (or cooperative thread array [CTA] cluster), is a hardware-level hierarchy. It grants a subset of SMs to your cooperative grid—and leaves the remainder free for other kernels to use. This mitigates the risk of one kernel monopolizing the GPU. The GPU guarantees the thread blocks will be coscheduled on the same GPU processing cluster (GPC), as shown in Figure 10-10. ... ###### Figure 10-10. Multiple thread block clusters are guaranteed to be coscheduled on the same GPC or GPC partition ... A GPC is a collection of nearby SMs. The GPU schedules thread blocks onto GPCs similar to how it schedules threads of a thread block onto the same SM."
  },
  {
    "file_name": "aisp_1011.png",
    "caption": "Diagram illustrating thread clusters with four CTAs (CTA 0 to CTA 3), showing how tiles A and B are loaded into the clusters for synchronized processing.",
    "description": "Diagram illustrating thread clusters with four CTAs (CTA 0 to CTA 3), showing how tiles A and B are loaded into the clusters for synchronized processing.",
    "chapter": "Thread Block Clusters and Distributed Shared Memory",
    "page_context": "The GPU provides distributed shared memory (DSMEM), discussed in the next section, for the thread block cluster to use across those blocks. It also supports a cluster-level barrier using the Cluster Group API (`cluster.sync()`). ... This cluster-level barrier lets you synchronize only a subset of thread blocks without blocking the entire GPU. Thread block clusters let you launch a cooperative kernel that subdivides the grid into smaller groups of blocks, as shown in Figure 10-11. ... ###### Figure 10-11. For these four (2 × 2) thread clusters, each tile of A and B is loaded into two thread blocks simultaneously (source: https://oreil.ly/kEZsv) ... Within each group, calling `cluster.sync()` provides a local barrier. This lets blocks inside a cluster share data through dedicated on-chip resources without monopolizing every SM. On modern GPUs, you can use DSMEM, which allows thread block clusters to share a contiguous region of on-chip SRAM. This enables low‐latency communication between blocks in the same cluster with native hardware support."
  },
  {
    "file_name": "aisp_1012.png",
    "caption": "Diagram illustrating thread block swizzling, showing how tiles from matrices A and B are accessed in a single wave to enhance L2 cache utilization and improve memory access efficiency.",
    "description": "Diagram illustrating thread block swizzling, showing how tiles from matrices A and B are accessed in a single wave to enhance L2 cache utilization and improve memory access efficiency.",
    "chapter": "Thread Block Swizzling",
    "page_context": "In a straightforward grid launch, thread blocks process tiles in strict row-major or column-major order. This can cause early blocks to evict data that later blocks will need—resulting in poor reuse and extra memory traffic. Instead, you want tiles A and B within a single wave, which can be read out of L2 cache. ... To work around this inefficiency, you can use thread block swizzling. Similar to using swizzling to optimize memory access and avoid shared-memory bank conflicts, you can use thread block swizzling to avoid assigning tiles in the inefficient row-major and column-major order, as shown in Figure 10-12. ... ###### Figure 10-12. Thread block swizzling to read tiles A and B in a single wave out of L2 cache ... Thread block swizzling lets tiles of both A and B matrices, needed by the same wave, stay in L2 for maximum reuse. When applied to persistent and tiled GEMM workloads, this type of swizzling can produce double-digit performance gains by reducing memory misses and bandwidth pressure. Thread-block swizzling is a simple yet powerful pattern-reordering technique that aligns kernel launch order with cache locality."
  },
  {
    "file_name": "aisp_1013.png",
    "caption": "Diagram illustrating a thread block pair, showing how two thread blocks, each with multiple Tensor Cores, are combined to facilitate data sharing and processing.",
    "description": "Diagram illustrating a thread block pair, showing how two thread blocks, each with multiple Tensor Cores, are combined to facilitate data sharing and processing.",
    "chapter": "Thread Block Pair",
    "page_context": "## Thread Block Pair ... With modern NVIDIA, GPUs let you coschedule exactly two thread blocks in a cluster, or a thread block pair (aka CTA pair), across SMs within a single GPC. By grouping thread blocks in a cluster (e.g., a 2-block cluster, as shown in Figure 10-13), kernels that share data can use TMA to move tiles into each block’s shared memory. ... ###### Figure 10-13. Thread block pair combines two thread blocks ... A single thread block might lack the registers or shared‐memory capacity to process a very large tile (for example, a 256 × 256 matrix subtile) by itself. By pairing two thread blocks on nearby SMs within a GPC and using DSMEM, those two blocks can split the work on one large tile yet share data through a unified shared‐memory region, as shown in Figure 10-14."
  },
  {
    "file_name": "aisp_1014.png",
    "caption": "Diagram illustrating a thread block pair loading matrix tiles for an A * B matrix multiply, with shared and tensor memory highlighted for efficient collaboration.",
    "description": "Diagram illustrating a thread block pair loading matrix tiles for an A * B matrix multiply, with shared and tensor memory highlighted for efficient collaboration.",
    "chapter": "Thread Block Pair",
    "page_context": "###### Figure 10-13. Thread block pair combines two thread blocks ... A single thread block might lack the registers or shared‐memory capacity to process a very large tile (for example, a 256 × 256 matrix subtile) by itself. By pairing two thread blocks on nearby SMs within a GPC and using DSMEM, those two blocks can split the work on one large tile yet share data through a unified shared‐memory region, as shown in Figure 10-14. ... ###### Figure 10-14. Thread block pair (aka CTA pair) loading tiles as operands for an A * B matrix multiply (source: https://oreil.ly/kEZsv) ... Here, each thread block in the pair can load a fraction of the operand tile (e.g., 128 × 16) into its on-chip SMEM for the matrix multiply. In addition, each thread block holds part of the accumulator (e.g., 128 × 256) in Tensor Memory (TMEM). This allows the two thread blocks in the CTA pair to collaborate on a single tile, as shown in Figure 10-15."
  },
  {
    "file_name": "aisp_1015.png",
    "caption": "Diagram illustrating the paired-thread block mode in a cluster with Tensor Cores and shared memory, showing operand matrices and accumulators stored in TMEM and SMEM for matrix multiplication.",
    "description": "Diagram illustrating the paired-thread block mode in a cluster with Tensor Cores and shared memory, showing operand matrices and accumulators stored in TMEM and SMEM for matrix multiplication.",
    "chapter": "Thread Block Pair",
    "page_context": "###### Figure 10-14. Thread block pair (aka CTA pair) loading tiles as operands for an A * B matrix multiply (source: https://oreil.ly/kEZsv) ... Here, each thread block in the pair can load a fraction of the operand tile (e.g., 128 × 16) into its on-chip SMEM for the matrix multiply. In addition, each thread block holds part of the accumulator (e.g., 128 × 256) in Tensor Memory (TMEM). This allows the two thread blocks in the CTA pair to collaborate on a single tile, as shown in Figure 10-15. ... ###### Figure 10-15. Thread block pair with Tensor Cores and TMEM ... Thread block pairs allow larger matrix multiplies that span two physical SMs. Using a pair of thread blocks effectively doubles the tile size since each SM handles half of the tile’s data."
  },
  {
    "file_name": "aisp_1016.png",
    "caption": "Diagram illustrating data sharing between streaming multiprocessors (SMs) in a cluster using DSMEM and shared memory.",
    "description": "Diagram illustrating data sharing between streaming multiprocessors (SMs) in a cluster using DSMEM and shared memory.",
    "chapter": "Thread Block Pair",
    "page_context": "Thread block pairs allow larger matrix multiplies that span two physical SMs. Using a pair of thread blocks effectively doubles the tile size since each SM handles half of the tile’s data. ... The SM hardware shares operand data between the SMs using DSMEM. DSMEM reduces duplicate loads, improves data reuse, and increases arithmetic intensity. Figure 10-16 shows this data sharing between SMs in a thread block cluster using DSMEM. ... ###### Figure 10-16. Sharing data between SMs in a thread block cluster using DSMEM ... CUDA provides enhanced multicast barrier and synchronization primitives for these multi-SM operations. The pair synchronizes with each other using the lightweight `cluster.sync()` call. This way, when one SM finishes using a tile, the adjacent SM in the CTA pair can consume it."
  },
  {
    "file_name": "aisp_1017.png",
    "caption": "Diagram illustrating the multicast of A and B tiles into shared memory across four CTAs in a cluster.",
    "description": "Diagram illustrating the multicast of A and B tiles into shared memory across four CTAs in a cluster.",
    "chapter": "Reducing Global Memory Traffic with Thread Block Clusters",
    "page_context": "The TMA engine supports a multicast copy mode that feeds directly into DSMEM when thread blocks belong to the same cluster. A single TMA transfer from global memory can place data into each participating block’s shared memory simultaneously. This avoids redundant DRAM fetches. ... With TMA multicast, the GPU ensures that L2‐cached data is broadcast into the shared memory of each cluster member (thread block) in one pass. As a result, you avoid repeated global‐memory loads of the same tile. This improves bandwidth utilization and shrinks DRAM traffic—especially when many blocks need the same input, as shown in Figure 10-17. ... ###### Figure 10-17. For these four (2 × 2) thread clusters, each tile is loaded once and multicast into the shared memory of all CTAs in each cluster (source: https://oreil.ly/kEZsv) ... Here, the TMA engine performs a single multicast from global memory into DSMEM, broadcasting the tile to every thread block’s SMEM in the cluster and eliminating redundant DRAM reads. TMA multicast is configured through a tensor-map descriptor and issued as `cp.async.bulk.tensor` targeting `shared::cluster`. Fortunately, higher-level libraries like CUTLASS/cuTe and Triton generate these multicast operations, tensor-map descriptors, and bulk tensor copies for you. Specifically, these libraries can issue the relevant [PTX](https://oreil.ly/uFO4C) instructions including `cp.async.bulk.tensor` with a tensor-map operand. They can issue PTX instructions that target `shared::cluster` for multicast. In these cases, the hardware delivers the tile to each thread block’s SMEM."
  },
  {
    "file_name": "aisp_1018.png",
    "caption": "Diagram showing the SM-to-SM network facilitating data exchange between two thread block clusters, each utilizing SMEM within a cluster.",
    "description": "Diagram showing the SM-to-SM network facilitating data exchange between two thread block clusters, each utilizing SMEM within a cluster.",
    "chapter": "Reducing Global Memory Traffic with Thread Block Clusters",
    "page_context": "In short, by multicasting shared data, thread block clusters allow multiple blocks to reuse data at on-chip speeds. This leads to substantial speedups for memory-bound workloads. ... It’s important to note that both DSMEM and L2 operate in parallel. This provides two “lanes” for interblock data sharing. This dual‐path design combines DSMEM’s ultrafast cluster-wide communication with L2’s broader caching coverage. In other words, DSMEM access is bypassing the L2 cache for cluster-local addresses. Instead, DSMEM uses the dedicated SM-to-SM network, as shown in Figure 10-18. ... ###### Figure 10-18. DSMEM uses an SM-to-SM thread block cluster-local network between two thread block clusters for its data exchange ... Remote DSMEM accesses are routed using the thread block cluster interconnect and are distinct from global-memory traffic. For example, when a thread block pulls a tile using DSMEM, it uses low-latency shared‐memory transfers. This ensures that interthread block data sharing in a thread block cluster is as fast as on-chip shared memory."
  },
  {
    "file_name": "aisp_1101.png",
    "caption": "Diagram showing the sequence of five kernels launched from the CPU to two GPU streams, illustrating the concurrent execution of operations in CUDA streams.",
    "description": "Diagram showing the sequence of five kernels launched from the CPU to two GPU streams, illustrating the concurrent execution of operations in CUDA streams.",
    "chapter": "Overlapping Kernel Execution with CUDA Streams",
    "page_context": "# Overlapping Kernel Execution with CUDA Streams ... A CUDA stream is a sequence of operations—kernel launches, memory copies, and memory allocations—that execute in the order they are issued. Consider launching 5 kernels from the CPU onto the GPU using 2 streams, as shown in Figure 11-1. ... ###### Figure 11-1. Launching five kernels from the CPU onto the two streams running on the GPU ... Here, we see `ker_A` and `ker_B` are running on stream 2, while `ker_1`, `ker_2`, and `ker_3` are running on stream 1. All kernels may overlap with one another—and across CUDA streams—as long as hardware resources permit."
  },
  {
    "file_name": "aisp_1102.png",
    "caption": "Diagram illustrating the non-sequential issuance and execution of kernels across different queues, showing overlapping host-to-device, compute, and device-to-host operations to achieve a three-way overlap with a runtime of 5.",
    "description": "Diagram illustrating the non-sequential issuance and execution of kernels across different queues, showing overlapping host-to-device, compute, and device-to-host operations to achieve a three-way overlap with a runtime of 5.",
    "chapter": "Using Streams to Overlap Compute with Data Transfers",
    "page_context": "When compute and memory throughput are saturated, you’ll start seeing two concurrent operations each running at 50%, for instance, since they are both contending for the same resource. You can profile GPU utilization to identify these saturation thresholds. ... For example, consider an AI model training or inference workload that breaks work into batches. Here, you would launch a kernel on batch 0 in stream 0 at the same time that stream 1 invokes `cudaMemcpyAsync()` to copy batch 1 from the host to device, as shown in [Figure 11-2](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch11.html#ch11_figure_2_1757308065262467). ... ###### Figure 11-2. Timeline of three-way overlap ... On modern GPUs with at least two copy engines (`deviceProp.asyncEngineCount()`), you can extend this to a three-way overlap such that stream 0 runs the kernel for batch 0, stream 1 copies batch 1 host to device, and stream 2 writes the results of the previous batch back to the host. This extends to additional streams. This pattern hides data‐transfer latency behind computation, and vice versa, keeping all of the GPU’s engines busy and minimizing idle time."
  },
  {
    "file_name": "aisp_1103.png",
    "caption": "Diagram illustrating stream-ordered memory allocation using cudaMallocAsync and cudaFreeAsync, showing how memory blocks are allocated from a pool without halting stream execution.",
    "description": "Diagram illustrating stream-ordered memory allocation using cudaMallocAsync and cudaFreeAsync, showing how memory blocks are allocated from a pool without halting stream execution.",
    "chapter": "Using CUDA Streams and Stream-Ordered Memory Allocator with LLMs",
    "page_context": "The solution is to use the stream‐ordered allocator with `cudaMallocAsync()` and `cudaFreeAsync()`. These APIs enqueue the work of allocating and freeing regions of device memory at the stream level. As such, they synchronize only at the stream <span class=\"keep-together\">level—and</span> not the device level. ... For instance, consider a stream that needs a scratch buffer of 16 MB for attention on a batch of input data. It would invoke `cudaMallocAsync(&scratchPtr, scratchBytes, stream1)`, which records this allocation request in its operation queue but does not force any other stream to wait, as shown in [Figure 11-3](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch11.html#ch11_figure_3_1757308065262485). ... ###### Figure 11-3. Stream-ordered memory allocation ... The other streams continue launching kernels, copying data, or doing whatever they were doing—even while stream 1’s allocation is in flight. And once the CUDA runtime has reserved the memory behind the scenes, stream 1 can make progress again and launch the attention kernel into that newly allocated region—all without halting any other streams."
  },
  {
    "file_name": "aisp_1104.png",
    "caption": "Diagram showing concurrent execution of multiple GPU kernels across different CUDA streams from separate CPU threads, illustrating overlap in kernel launches.",
    "description": "Diagram showing concurrent execution of multiple GPU kernels across different CUDA streams from separate CPU threads, illustrating overlap in kernel launches.",
    "chapter": "Modern Per-Thread Default Stream",
    "page_context": "With PTDS, any kernel launch, copy, or allocation without an explicit stream parameter goes into a thread‐local queue. Only commands within the same host thread’s default stream serialize, and they never impose an implicit global barrier on streams belonging to other threads. ... In short, by enabling per-thread default streams, the legacy default-stream synchronization barrier is removed. Each host thread’s default stream never waits on other threads’ streams. This allows full overlap of multiple kernel launches across threads. And if you issue kernel launches (or memory copies) from different CPU threads without specifying an explicit stream, the operations will overlap on the GPU whenever resources permit. This is shown in Figure 11-4. ... ###### Figure 11-4. Timeline showing multiple GPU kernels running concurrently across separate CUDA streams issued from different threads on their respective default streams with PTDS enabled ... # Default Versus Explicit (Nondefault) Streams"
  },
  {
    "file_name": "aisp_1105.png",
    "caption": "A diagram illustrating how cudaStreamSynchronize() causes the CPU to wait for synchronization of operations in a stream, blocking subsequent CPU tasks until GPU tasks complete.",
    "description": "A diagram illustrating how cudaStreamSynchronize() causes the CPU to wait for synchronization of operations in a stream, blocking subsequent CPU tasks until GPU tasks complete.",
    "chapter": "Fine-Grained Synchronization with Events and Callbacks",
    "page_context": "Even when multiple streams and DMA engines overlap, there are times when one stream’s operation must wait for another. Consider a pair of producer-consumer streams. The producer stream 0 is loading and preparing data for the consumer stream 1 to process. ... In this case, it is tempting to synchronize these streams on the host side and block the CPU with a full `cudaDeviceSynchronize()`. However, this will block the CPU until all streams and all operations on the GPU complete. This is very bad for performance. You can also use `cudaStreamSynchronize()`, as shown in [Figure 11-5](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch11.html#ch11_figure_5_1757308065262530), but this will block until all operations in the stream’s queue complete. ... ###### Figure 11-5. Using cudaStreamSynchronize() will block the CPU until all operations in the stream have synchronized ... Instead, you can use CUDA events to provide a much finer-grained synchronization mechanism for streams. With CUDA events, you record a `cudaEvent_t` in the stream that produces data and then have the consumer stream wait for that event."
  },
  {
    "file_name": "aisp_1106.png",
    "caption": "Diagram showing CUDA event synchronization between GPU and CPU streams, highlighting cudaEventRecord and cudaStreamWaitEvent processes along a timeline.",
    "description": "Diagram showing CUDA event synchronization between GPU and CPU streams, highlighting cudaEventRecord and cudaStreamWaitEvent processes along a timeline.",
    "chapter": "Fine-Grained Synchronization with Events and Callbacks",
    "page_context": "Instead, you can use CUDA events to provide a much finer-grained synchronization mechanism for streams. With CUDA events, you record a `cudaEvent_t` in the stream that produces data and then have the consumer stream wait for that event. ... For instance, you would launch a producer kernel on stream 0 and call `cuda​E⁠vent​Re⁠cord(doneEvent, stream0)` when the data is ready to be consumed. In stream 1, you would then call `cudaStreamWaitEvent(stream1, doneEvent, 0)` before launching the consumer kernel. In this way, only stream 1 is stalled waiting for the event to be recorded—the host thread and all other streams will continue executing, as shown in [Figure 11-6](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch11.html#ch11_figure_6_1757308065262547). ... ###### Figure 11-6. Using CUDA events to synchronize in a fine-grained manner ... In addition to using CUDA events for interstream coordination,[]()[]()[]()[]()[]()[]() you can also use them to communicate between the GPU device and the CPU host. To do this, you would register a host callback with `cudaLaunchHostFunc()` from the host."
  },
  {
    "file_name": "aisp_1107.png",
    "caption": "Diagram illustrating synchronization of CUDA streams, showing dependencies and event-based communication between Streams A-D, ensuring correct processing order.",
    "description": "Diagram illustrating synchronization of CUDA streams, showing dependencies and event-based communication between Streams A-D, ensuring correct processing order.",
    "chapter": "Using CUDA Events for Cross-Stream Synchronization",
    "page_context": "This event-based synchronization is heavily used in deep learning frameworks to overlap gradient computations with all-reduce communications. The compute stream records an event when gradients are ready, and a communication stream waits on that event to start an NCCL operation, thereby overlapping communication with remaining computation. ... In short, CUDA events are lightweight and optimized for device signaling. A stream records an event when it reaches a specific point in its command queue. And other streams can efficiently poll/wait for it. They let us orchestrate complex dependency graphs across streams without forcing global waits. And they provide the necessary control to implement pipelined execution in multistream LLM workloads. ... ###### Figure 11-7. Synchronizing CUDA streams with events (source: https://oreil.ly/MynOA) ... NVIDIA continues to improve event-time granularity and reduce event-recording overhead. As such, events are a good choice for profiling since they record event timestamps in the GPU timeline and can measure execution time, etc."
  },
  {
    "file_name": "aisp_1108.png",
    "caption": "Diagram illustrating a four-GPU ring setup for chunked all-reduce, showing directional data flow between GPUs.",
    "description": "Diagram illustrating a four-GPU ring setup for chunked all-reduce, showing directional data flow between GPUs.",
    "chapter": "Multi-GPU Compute and Data Transfer Overlap with CUDA Streams",
    "page_context": "PyTorch and NCCL both use dedicated, high-priority streams to interleave communication with compute-intensive operations. This way, they don’t get delayed behind large compute kernels. ... NCCL will choose a ring or tree algorithm based on NVLink or PCIe topology. Consider a four-GPU ring, as shown in Figure 11-8, with four chunks (1–4). ... ###### Figure 11-8. Chunked all-reduce with four GPUs in a ring ... In this ring all-reduce, each GPU sends chunk i to its neighbor (k → k+1) while receiving chunk i−1 from its other neighbor (k−1 → k), using device kernels to move and reduce chunks over NVLink or PCIe."
  },
  {
    "file_name": "aisp_1109.png",
    "caption": "Diagram showing how Kernel A triggers Kernel B using PDL, allowing partial overlap of their executions, with Kernel B waiting for synchronization from Kernel A's completion.",
    "description": "Diagram showing how Kernel A triggers Kernel B using PDL, allowing partial overlap of their executions, with Kernel B waiting for synchronization from Kernel A's completion.",
    "chapter": "Programmatic Dependent Launch",
    "page_context": "Another type of inter-kernel pipelining and communication is called Programmatic Dependent Launch (PDL). PDL lets a kernel trigger another kernel’s execution directly on the device using the same CUDA stream—and without involving the CPU. For instance, Kernel A, the primary kernel, can trigger Kernel B, the secondary kernel, which is waiting on a signal from Kernel A. ... This trigger can happen even before Kernel A finishes execution. To do this, it uses `cudaTriggerProgrammaticLaunchCompletion()`, as shown in [Figure 11-9](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch11.html#ch11_figure_9_1757308065262607). Here, Kernel B is waiting on Kernel A with a call to `cudaGridDependencySynchronize()`. ... ###### Figure 11-9. Using PDL to launch Kernel B from Kernel A—partially overlapping Kernel B’s execution with Kernel A’s epilogue (in the same CUDA stream) ... Using the constructs provided by PDL, Kernel A can signal Kernel B to execute during Kernel A’s epilogue (e.g., wrap-up) phase. This way, Kernel A can execute alongside Kernel B for a bit of time. It’s important to note that Kernel A should not trigger Kernel B to execute until Kernel A has produced and synchronized all data (e.g., L2/shared memory/global memory) needed by Kernel B."
  },
  {
    "file_name": "aisp_1110.png",
    "caption": "Diagram illustrating a warp-specialized, multistage pipeline using PDL and thread block clusters for an optimized GEMM implementation with TMA multicast.",
    "description": "Diagram illustrating a warp-specialized, multistage pipeline using PDL and thread block clusters for an optimized GEMM implementation with TMA multicast.",
    "chapter": "Combining PDL and Thread Block Clusters with Warp Specialization",
    "page_context": "Thread block clusters enable groups of thread blocks to coordinate prefetch (e.g., multicast) and perform dynamic load balancing across SMs. This way, both producer and consumer tasks are evenly distributed cluster-wide. ... For example, you can overlap the movement of model weights (data transfer) with GEMMs (compute) inside the same kernel so that one tile is being computed while the next tile is in flight. A warp-specialized, multistage software pipeline (stages 0…N) can coordinate these roles using PDL and mbarrier primitives, as shown in Figure 11-10. ... ###### Figure 11-10. Warp-specialized, multistage pipeline with PDL and thread block clusters and TMA multicast for a high-performance, inter-kernel and intra-kernel GEMM implementation ... Here is a CUDA C++ example that shows how to combine PDL, thread block clusters, and warp specialization with a simple TMA-style async copy + compute pipeline. Specifically, the primary kernel, `primary_gemm`, uses `cudaTriggerProgrammati⁠c​LaunchCompletion()` to signal to the secondary kernel that all memory flushes have completed and the data is ready to be consumed. As such, it’s now safe for the secondary kernel, `secondary_gemm`, to continue from `cudaGridDependencySynchronize()`, as shown here:"
  },
  {
    "file_name": "aisp_1111.png",
    "caption": "Diagram illustrating the process of loading a tile from global memory into shared memory buffers of each SM in a thread block cluster using TMA multicast, featuring cuda::memcpy_async calls.",
    "description": "Diagram illustrating the process of loading a tile from global memory into shared memory buffers of each SM in a thread block cluster using TMA multicast, featuring cuda::memcpy_async calls.",
    "chapter": "Combining PDL and Thread Block Clusters with Warp Specialization",
    "page_context": "The dependent kernel then calls `cudaGridDependencySynchronize()`. This waits for the signal and the necessary memory flush from the primary kernel so that it can start executing in parallel with the primary kernel’s epilogue. ... Within each thread block, we use warp specialization to overlap data movement and computation. By splitting each block into a single “producer” warp and multiple “consumer” warps, the producer issues `cuda::memcpy_async` calls into shared memory. This uses TMA multicast to broadcast a single DMA transfer to all thread blocks in the cluster, as shown in [Figure 11-11](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch11.html#ch11_figure_11_1757308065262648). ... ###### Figure 11-11. TMA multicast: a leader CTA issues cp.async.bulk.tensor into DSMEM (cluster shared memory); the follower CTAs consume tiles using cluster.map_shared_rank; cuda::memcpy_async drives TMA ... While the producer warp is loading data with TMA multicast, the consumers spin on a C++ block-scope barrier (`cuda::barrier<cuda::thread_scope_block>`) before executing their matrix-multiply steps (`do_compute()`). This lets each tile’s load and its fused-multiply-add (FMA) operations interleave in a fine-grained, multistage software pipeline that hides global-memory latency inside the kernel."
  },
  {
    "file_name": "aisp_1201.png",
    "caption": "Diagram illustrating two threads performing atomic add operations on a histogram, highlighting the efficiency of on-chip atomic updates.",
    "description": "Diagram illustrating two threads performing atomic add operations on a histogram, highlighting the efficiency of on-chip atomic updates.",
    "chapter": "Atomic Counters",
    "page_context": "Atomic counters are the foundation for atomic queues, which allow dynamic work allocation. ... On modern GPUs, global atomics are serviced and serialized in the on-device L2 cache. This reduces latency versus DRAM round trips when the target line is resident. Atomic counters still incur latency and serialize under contention. But uncontended `atomicAdd` operations happen extremely quickly by remaining on-chip. An example of two threads incrementing an atomic is shown in [Figure 12-1](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch12.html#ch12_figure_1_1757308067464507). ... ###### Figure 12-1. Superfast, on-chip atomic-memory add operations across multiple threads in the context of a histogram computation ... However, `atomicAdd` does not come for free. It still has latency and, under contention, can serialize threads waiting on the same memory address. As such, the L2 needs to serialize the updates as well. This creates a hotspot, which needs to be optimized. []()[]()[]()[]()[]()Nsight Compute can help you quantify the cost."
  },
  {
    "file_name": "aisp_1202.png",
    "caption": "Diagram illustrating the use of an atomic counter and atomicAdd to dynamically allocate workloads, showing segments for completed work, work in progress, and new work reservations.",
    "description": "Diagram illustrating the use of an atomic counter and atomicAdd to dynamically allocate workloads, showing segments for completed work, work in progress, and new work reservations.",
    "chapter": "Atomic Queues",
    "page_context": "## Atomic Queues ... Let’s now use a global atomic counter[]()[]()[]()[]() to coordinate a dynamic work queue. The goal is to use the atomic counter and `atomicAdd` to balance arbitrary workloads across all SMs so that no thread, or warp, sits idle. An example of this dynamic work queue is shown in [Figure 12-2](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch12.html#ch12_figure_2_1757308067464538). ... ###### Figure 12-2. Using atomic counter and atomicAdd as a dynamic work queue to balance workloads across SMs and warps ... In the next code example (`computeKernel`), each thread computes a different number of iterations based on `idx % 256`. Threads with a small value for `idx % 256` do very little work, while threads with a large value for `idx % 256` will do a lot of work. As a result of this imbalance, threads finish at different times, and some SMs go idle waiting for the longest threads to complete. Here is the code that uses a static, uneven workload per thread:"
  },
  {
    "file_name": "aisp_1203.png",
    "caption": "Diagram showing a timeline comparison of kernel launches, illustrating higher CPU overhead without CUDA Graphs (top) and reduced overhead with CUDA Graphs (bottom).",
    "description": "Diagram showing a timeline comparison of kernel launches, illustrating higher CPU overhead without CUDA Graphs (top) and reduced overhead with CUDA Graphs (bottom).",
    "chapter": "CUDA Graphs",
    "page_context": "# CUDA Graphs ... When your pipeline consists of multiple kernels, copies, stream-event records, and callbacks, launching them one by one on the host every iteration still incurs CPU overhead. CUDA Graphs let you capture that entire workflow once and replay it repeatedly with essentially zero CPU overhead. Figure 12-3 compares kernel launches without (top) and with (bottom) CUDA Graphs. ... ###### Figure 12-3. Kernel-launch timeline without (top) and with (bottom) CUDA Graphs ... Why use CUDA Graphs? First, they cut down launch overhead. Multiple small kernels or copies can be launched with essentially one CPU call. Second, they enable better scheduling on the GPU. The work is submitted as a batch, so the CUDA driver can potentially reduce some internal latency between operations."
  },
  {
    "file_name": "aisp_1204.png",
    "caption": "Diagram illustrating PyTorch's use of static memory pools in CUDA Graphs, showing how memory allocations are managed across different graph stages.",
    "description": "Diagram illustrating PyTorch's use of static memory pools in CUDA Graphs, showing how memory allocations are managed across different graph stages.",
    "chapter": "Memory Pools for CUDA Graphs",
    "page_context": "One important consideration is memory management with CUDA Graphs. Memory operations inside a CUDA Graph obey the same rules as in CUDA streams. If you allocate GPU memory inside the capture, that allocation becomes part of the graph execution. ... You generally want to avoid allocating GPU memory inside your graph and preallocating memory outside of the graph. Many frameworks, such as PyTorch, use static memory pools with CUDA Graphs, as shown in Figure 12-4. The use of static memory pools keeps memory allocations from becoming part of the captured graph sequence. ... ###### Figure 12-4. PyTorch uses static memory pools for CUDA Graphs ... While CUDA Graphs won’t make an individual memory copy or kernel execution faster, they can automatically overlap independent data transfers and computations within the graph—similar to CUDA streams. This eliminates the per-iteration CPU scheduling and is made possible since the dependency graph is known upfront."
  },
  {
    "file_name": "aisp_1205.png",
    "caption": "Diagram showing a sequence of CUDA Graph operations with nodes for memory copy and kernels, illustrating dependencies among them.",
    "description": "Diagram showing a sequence of CUDA Graph operations with nodes for memory copy and kernels, illustrating dependencies among them.",
    "chapter": "Device-Initiated CUDA Graph Launch",
    "page_context": "Device-initiated graph launches keep data-driven workflows completely on the GPU. Your kernel is responsible for computing the decision conditions, not the CPU. As such, it can spawn the next graph directly, eliminate CPU round trips, and further reduce latency. ... Because the graph is already resident on the GPU and no CPU-GPU handshake is needed, device-initiated launches remove host scheduling from the critical path and can reduce end-to-end latency in host-bound loops. In practice, device-initiated CUDA graph launches have shown roughly 2× lower launch latency compared to equivalent host-side graph launches. And the overhead stays flat even as the graph grows in size or complexity. ... ###### Figure 12-5. Sequence of operations (nodes for kernels and data transfers) and their dependencies (edges) launched by a CUDA Graph ... Device-launch graph latency is not impacted by how many nodes or parallel branches are in the graph. This is in contrast to host-launch graph latency, which would increase with a larger graph due to CPU scheduling overhead."
  },
  {
    "file_name": "aisp_1206.png",
    "caption": "Diagram illustrating the sequential execution of graph environments, with G1 initiating tail launches for G2, G3, and G4 in order.",
    "description": "Diagram illustrating the sequential execution of graph environments, with G1 initiating tail launches for G2, G3, and G4 in order.",
    "chapter": "Device-Initiated CUDA Graph Launch",
    "page_context": "A graph can have up to 120 total fire-and-forget graphs during the course of its execution. ... By contrast, a device-initiated graph tail launch defers execution of the graph until the launching kernel reaches a synchronization point or completes. This effectively queues the graph to run after the current kernel as a continuation, as shown in Figure 12-6. ... ###### Figure 12-6. Tail launches enqueued by a given graph will execute one at a time, in order of when they were enqueued ... Tail launches are especially powerful when implementing GPU-resident work schedulers. A persistent “scheduler” kernel can tail-launch a graph, then relaunch itself once that graph finishes. This technique effectively creates a loop on the GPU without requiring host re-invocation. To relaunch itself, the kernel calls `cudaGetCurrent​Gra⁠ph​Exec()` to get a handle to its own executing graph. It then launches the graph using `cudaGraphLaunch(..., cudaStreamGraphTailLaunch)` to enqueue itself again."
  },
  {
    "file_name": "aisp_1207.png",
    "caption": "Diagram illustrating the sequence of tail launches enqueued from multiple execution environments, showing graphs G1, G2, X, Y, and G3.",
    "description": "Diagram illustrating the sequence of tail launches enqueued from multiple execution environments, showing graphs G1, G2, X, Y, and G3.",
    "chapter": "Device-Initiated CUDA Graph Launch",
    "page_context": "Tail launches are especially powerful when implementing GPU-resident work schedulers. A persistent “scheduler” kernel can tail-launch a graph, then relaunch itself once that graph finishes. This technique effectively creates a loop on the GPU without requiring host re-invocation. To relaunch itself, the kernel calls `cudaGetCurrent​Gra⁠ph​Exec()` to get a handle to its own executing graph. It then launches the graph using `cudaGraphLaunch(..., cudaStreamGraphTailLaunch)` to enqueue itself again. ... Additionally, tail graphs can perform additional tail launches. In this case, the new tail launches will execute before the previous graph’s tail launch, as shown in Figure 12-7. ... ###### Figure 12-7. Tail launches enqueued from multiple graphs ... You can have up to 255 pending tail launches enqueued in a CUDA Graph. However, when it comes to a self-tail-launch (e.g., a graph enqueues itself for relaunch), you can have only one pending self-tail-launch at a time."
  },
  {
    "file_name": "aisp_1208.png",
    "caption": "Diagram illustrating sibling graph launch, showing two parallel execution environments within a stream environment connected by a Launch(X) action.",
    "description": "Diagram illustrating sibling graph launch, showing two parallel execution environments within a stream environment connected by a Launch(X) action.",
    "chapter": "Device-Initiated CUDA Graph Launch",
    "page_context": "You can have up to 255 pending tail launches enqueued in a CUDA Graph. However, when it comes to a self-tail-launch (e.g., a graph enqueues itself for relaunch), you can have only one pending self-tail-launch at a time. ... A sibling launch is a variation of fire-and-forget in which the launched graph executes as a peer to the parent graph—instead of as a child. Additionally, the sibling runs in the parent’s stream environment. This means it runs immediately and independently but without delaying any tail launches of the parent graph, as shown in Figure 12-8. ... ###### Figure 12-8. Sibling graph launch in the parent’s stream environment ... For this mode, you can use `cudaGraphLaunch(graphExec, cudaStreamGraphFireAndForgetAsSibling)` to launch in “sibling mode.” This submits the graph as a sibling of the current graph’s execution environment."
  },
  {
    "file_name": "aisp_1209.png",
    "caption": "Diagram illustrating various types of conditional graph nodes, including If, If/else, While, and Switch, each with corresponding body graphs based on conditions.",
    "description": "Diagram illustrating various types of conditional graph nodes, including If, If/else, While, and Switch, each with corresponding body graphs based on conditions.",
    "chapter": "Conditional Graph Nodes",
    "page_context": "Repeatedly executes its body graph as long as the condition remains nonzero, checking again after each iteration. ... Holds N body graphs and executes the ith one when the condition equals i; if the condition ≥ N, it skips execution altogether. ... ###### Figure 12-9. Types of conditional graph nodes ... Next is an example showing how to create and populate an IF conditional node. Note the use of `cudaGraphSetConditional` to write the flag that controls the IF node. In this case, the condition checks if the sum is greater than a given threshold. This way, if the data meets the given criteria (`flag = 1u`), it runs the next subgraph. Otherwise, if the condition is not met, the conditional node does not run the subgraph:"
  },
  {
    "file_name": "aisp_1210.png",
    "caption": "Diagram showing GPU and CPU timelines where the GPU conditionally executes additional processing (body subgraph) if a condition is true, versus no execution when the condition is false.",
    "description": "Diagram showing GPU and CPU timelines where the GPU conditionally executes additional processing (body subgraph) if a condition is true, versus no execution when the condition is false.",
    "chapter": "Conditional Graph Nodes",
    "page_context": "After graph construction, call `cudaGraphInstantiate` to produce an executable graph object. To launch a graph from device code, you must instantiate it with the `cudaGraphInstantiateFlagDeviceLaunch` flag and upload it with `cudaGraphUpload` before any device-side launch. ... Launching with `cudaGraphLaunch` on a CUDA stream triggers execution of the upstream set-handle kernel, the conditional check, and then, if the flag was set, the body kernel. And all of this happens directly on the GPU, as shown in [Figure 12-10](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch12.html#ch12_figure_10_1757308067464680). ... ###### Figure 12-10. Additional processing (body subgraph) if condition is met ... We then synchronize the stream with `cudaStreamSynchronize` to wait for completion. Finally, we clean up by destroying the instantiated graph, the graph itself, and the stream."
  },
  {
    "file_name": "aisp_1211.png",
    "caption": "Diagram showing nested conditional graph nodes with a WHILE loop containing an IF statement, illustrating multilevel decision logic in CUDA.",
    "description": "Diagram showing nested conditional graph nodes with a WHILE loop containing an IF statement, illustrating multilevel decision logic in CUDA.",
    "chapter": "Conditional Graph Nodes",
    "page_context": "To minimize race conditions, it’s important to always set the condition in a single thread (e.g., `if (threadIdx.x == 0)`). And make sure that the preceding kernels flush memory to make the value visible before the conditional node executes. ... Conditional nodes can be nested as well. For instance, a WHILE node’s body can contain an IF node, as shown in Figure 12-11. This allows multilevel decision logic without requiring CPU hops. ... ###### Figure 12-11. Nested conditional graph nodes ... In short, you should use conditional graph nodes to keep decisions on the GPU, reduce CPU overhead, and express complex control flow directly in your CUDA Graph. Because graph creation costs can be amortized over many iterations, representing dynamic workflows entirely on-device can produce significant performance improvements."
  },
  {
    "file_name": "aisp_1212.png",
    "caption": "Diagram showing idle gaps due to sequential child kernel launches, with each launch followed by a corresponding execution phase, highlighting launch overhead periods.",
    "description": "Diagram showing idle gaps due to sequential child kernel launches, with each launch followed by a corresponding execution phase, highlighting launch overhead periods.",
    "chapter": "Dynamic Parallelism",
    "page_context": "} ... ``` ... In the host-driven version, the GPU runs `parentKernel` and then idles while the CPU prepares and launches each `childKernel` in turn. Note the explicit `cudaDeviceSynchronize()` calls after the parent and between children. These calls lead to idle gaps that should be eliminated, as shown in [Figure 12-12](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch12.html#ch12_figure_12_1757308067464709). ... ###### Figure 12-12. Idle gaps caused by child-kernel launches ... In contrast, the device-launched DP version lets the parent-kernel spawn its children on the device. This approach requires no host synchronization between the parent and child kernel launches. This way, the parent’s child-kernel launches implicitly queue the children and synchronize only at the end, as shown in the code here:"
  },
  {
    "file_name": "aisp_1213.png",
    "caption": "Diagram illustrating a timeline where dynamic parallelism eliminates idle gaps between kernel executions, showcasing increased GPU utilization and reduced launch overhead.",
    "description": "Diagram illustrating a timeline where dynamic parallelism eliminates idle gaps between kernel executions, showcasing increased GPU utilization and reduced launch overhead.",
    "chapter": "Dynamic Parallelism",
    "page_context": "Note that this dynamic parallelism version avoids any use of `cudaDeviceSynchronize()`. It relies on the implicit rule that a parent kernel does not complete until all its device-launched children complete, and the host simply waits on the stream. ... With the device-side DP approach, there are no idle gaps for CPU decision making. As such, it collapses the latency between dependent stages and keeps SMs busy end to end. This increases GPU utilization at the slight cost of a small amount of per-launch overhead incurred on the device, as you see in Figure 12-13’s timeline. ... ###### Figure 12-13. No gaps with device-side launch and dynamic parallelism ... In our simple two-child example, the host-driven version issues three separate launches (1 parent + 2 children). In this case, the GPU idles while the CPU decides when to launch each child kernel."
  },
  {
    "file_name": "aisp_1214.png",
    "caption": "Diagram of NVSHMEM one-sided communication illustrating data and signal exchange between sender and receiver for synchronization.",
    "description": "Diagram of NVSHMEM one-sided communication illustrating data and signal exchange between sender and receiver for synchronization.",
    "chapter": "Fine-Grained GPU-to-GPU Memory Sharing with NVSHMEM",
    "page_context": "In practice, avoid over-synchronizing. Use NVSHMEM’s fine-grained signals or point-to-point synchronization when possible. This is in contrast to always calling `nvshmem_barrier_all()`. ... Modern implementations of NVSHMEM provide efficiency improvements for these synchronization routines. However, they are still a synchronization point that can become a bottleneck if misused. NVSHMEM provides fine-grained primitives such as `nvshmem_wait_until` for waiting on device variables and [signal](https://oreil.ly/o3reh) operations like `nvshmem_signal_fetch`, `nvshmem_signal_wait_until`, or the `nvshmemx_signal_op` variants for point-to-point synchronization when only a subset of devices needs to coordinate. The low-level details showing NVSHMEM sharing data and synchronizing with signals between a sender and a receiver GPU are shown in [Figure 12-14](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch12.html#ch12_figure_14_1757308067464738). ... ###### Figure 12-14. NVSHMEM one-sided communication example ... NVSHMEM shines when workloads are irregular or data-dependent, such as graph algorithms, dynamic load balancing, and discrete‐event simulations. In these cases, static graphs and collectives are not sufficient."
  },
  {
    "file_name": "aisp_1215.png",
    "caption": "Diagram illustrating the bucketed all-reduce operation in two separate processes, highlighting the overlap of gradient reduction and computation steps.",
    "description": "Diagram illustrating the bucketed all-reduce operation in two separate processes, highlighting the overlap of gradient reduction and computation steps.",
    "chapter": "Capturing Multi-GPU Collectives with NCCL and CUDA Graphs",
    "page_context": "NCCL collectives are graph capture–compatible when all ranks capture and replay the same sequence with the same communicator. However, all ranks must capture and replay the same NCCL sequence with the same communicator. Mismatched communicators across graph replays will risk deadlock (at best, relatively easy to debug) or incorrect results (at worst, silent failure, and difficult to debug). Also, it’s recommended to capture preliminary warmup collectives, run them ahead of time to initialize communicators, and reuse the instantiated graph to achieve minimal steady-state latency. ... In practice, this bucketed all-reduce approach is standard in large-model training. By overlapping chunks of gradient reduction with computation of the next layers, you hide nearly all network time. An example of a bucketed all-reduce with DDP running in separate processes (process 1 and process 2) is shown in Figure 12-15. ... ###### Figure 12-15. Overlapping all-reduce with computation ... Modern libraries like PyTorch DDP implement variants of this approach automatically. But capturing a CUDA Graph can further reduce CPU overhead and provide more deterministic performance."
  },
  {
    "file_name": "aisp_1216.png",
    "caption": "Diagram illustrating data parallelism in GPU computing, showing each GPU performing forward and backward passes with gradient synchronization and model updates.",
    "description": "Diagram illustrating data parallelism in GPU computing, showing each GPU performing forward and backward passes with gradient synchronization and model updates.",
    "chapter": "Pattern for N-GPU Scaling",
    "page_context": "For example, 4 GPUs should ideally behave like a single GPU that is 4× faster—assuming you can keep all the GPUs fed with data in parallel (see Figure 12-16) and the host out of the loop. If you can do this, you should achieve near-linear speedups by properly overlapping communication computation. Without overlap, scaling will plateau once the communication time equals the computation time. ... As you increase GPUs, you need to use more aggressive pipelining of data transfers and computations—and use less CPU-side orchestration and synchronization. As such, you should offload more orchestration to the devices using asynchronous copies, NCCL collectives in graphs, and NVSHMEM’s PGAS primitives. This shifts even more responsibility to software. ... ###### Figure 12-16. Each GPU computes in parallel while exchanging data concurrently—no idle GPUs and no stalled data transfers ... Applying these techniques, you can eliminate the CPU bottleneck, saturate the fast interconnects, max out the compute FLOPS, and build truly low-latency multi-GPU pipelines. Next, let’s revisit roofline models in the context of dynamic and device-side scheduling and orchestration."
  },
  {
    "file_name": "aisp_1217.png",
    "caption": "Diagram showing the roofline model with two ceilings: a sloped memory bound ceiling indicating peak bandwidth and a flat compute bound ceiling indicating peak compute performance, related to the arithmetic intensity in FLOP/byte.",
    "description": "Diagram showing the roofline model with two ceilings: a sloped memory bound ceiling indicating peak bandwidth and a flat compute bound ceiling indicating peak compute performance, related to the arithmetic intensity in FLOP/byte.",
    "chapter": "Roofline-Guided Scheduling and Orchestration Decisions",
    "page_context": "At its heart, roofline boils down to operational arithmetic intensity, or the ratio of FLOPS performed to bytes moved. It consists of two hardware “ceilings”: memory roof (sloped) showing the peak throughput if you’re limited by bandwidth, and compute roof (flat) marking the peak arithmetic rate when you’re ALU‐bound, as shown in Figure 12-17. ... If your kernel lies near the memory roof (e.g., low FLOPS/byte) and is therefore memory bound, the best optimizations are those that hide or overlap memory transfers with computation. That means you should use asynchronous copies with CUDA streams—or even run multiple memory‐bound kernels concurrently. This way, you can better saturate different parts of the memory system. ... ###### Figure 12-17. Arithmetic intensity with two hardware ceilings: memory bound (e.g., data transform operation) and compute bound (e.g., matrix multiply) ... Kernel fusion helps only modestly for memory-bound workloads. It can shave off a number of intermediate global‐memory round trips. But the real gains come from masking latency and packing more loads/stores in flight."
  },
  {
    "file_name": "aisp_1301.png",
    "caption": "Diagram showing torch-compiled regions in pink and noncompiled regions in green, highlighting CompiledFunction and Aten operations.",
    "description": "Diagram showing torch-compiled regions in pink and noncompiled regions in green, highlighting CompiledFunction and Aten operations.",
    "chapter": "end of backward",
    "page_context": "The profiler can also highlight compiled versus noncompiled regions of the model. We’ll cover the PyTorch Compiler, graph breaks, and mechanisms to mitigate graph breaks later in this chapter—as well as the next chapter. ... For instance, when using `torch.compile`, the trace[]() will show events like `CompiledFunction` and indicate any graph breaks (see [Figure 13-1](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch13.html#ch13_figure_1_1757308071410752)). This helps pinpoint where the model fell back to eager execution, which will guide further optimizations. ... ###### Figure 13-1. Compiled (left and middle, pink) versus noncompiled (right, green) regions (source: https://oreil.ly/Z_fJG) ... After execution, we can examine the operation-level results by calling `prof.key_averages().table()` to print a concise table of the top operators by runtime. In the next code block, we request the top 10 operations sorted by their self CUDA time, which is the time spent in each operation’s own CUDA kernels excluding child operations spawned by the kernel. The top 10 operations by CUDA execution time are summarized in [Table 13-2](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch13.html#ch13_table_2_1757308071421735):"
  },
  {
    "file_name": "aisp_1302.png",
    "caption": "Diagram showing the shift from a memory-bound to a compute-bound kernel, highlighting increased arithmetic intensity and improved performance close to hardware limits.",
    "description": "Diagram showing the shift from a memory-bound to a compute-bound kernel, highlighting increased arithmetic intensity and improved performance close to hardware limits.",
    "chapter": "Kernel Roofline Analysis for General Matrix Multiply (GEMM)",
    "page_context": "The baseline metrics indicate a kernel that is memory bound since its execution is stalled by memory transfers. The result is a substantial amount of unused compute capacity, which further reinforces that this workload is not currently compute bound. The goal is to make this kernel more compute bound to take advantage of the large number of FLOPS available with this GPU. ... In the optimized version (e.g., fusing kernels, increasing arithmetic intensity, and reducing memory movement), the peak FLOPS increases to 85%, peak memory bandwidth drops to 40%, and occupancy increases to 80%. We effectively shifted the kernel from memory bound to compute bound—much closer to the hardware’s roofline limits, as shown in Figure 13-2. ... ###### Figure 13-2. Roofline chart before and after increasing arithmetic intensity of this kernel ... Up to this point, our profiling has focused on GPU performance. It’s also important to not waste time on the CPU or performing I/O. In the next section, we continue our profiling journey on the host side."
  },
  {
    "file_name": "aisp_1303.png",
    "caption": "Diagram comparing attention variant support with and without FlexAttention, highlighting enhanced capability with FlexAttention for diverse patterns.",
    "description": "Diagram comparing attention variant support with and without FlexAttention, highlighting enhanced capability with FlexAttention for diverse patterns.",
    "chapter": "PyTorch Optimized Attention Mechanisms",
    "page_context": "PyTorch’s high-level API `torch.nn.functional.scaled_dot_product_attention`, or SDPA, automatically[]()[]() uses the fastest available attention kernel for the given hardware (e.g., FlashAttention). Use this for a no-hassle speedup when your model’s attention pattern and dtype are supported by the selected backend (Flash, memory-efficient, or math). If it’s not supported, it will fall back to the standard attention implementation. ... A compiler-based approach for custom[]() sparsity patterns in attention. FlexAttention can be substantially faster for specific sparse attention patterns (e.g., block-sparse or sliding-window attention) by generating optimized kernels for these patterns, as shown in [Figure 13-3](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch13.html#ch13_figure_3_1757308071410804). Use FlexAttention for special cases that `scaled_dot_product_attention` does not support. ... ###### Figure 13-3. FlexAttention provides support for custom attention variants ... This is a counterpart to FlexAttention[]() that optimizes the decoding or text generation phases. FlexDecoding integrates with `torch.compile` and dynamic cache layouts. It uses compile-time optimizations for the decoder side of sequence generation, including KV caching efficiently across time steps. FlexDecoding can speed up autoregressive generation by reducing redundant compute during decoding. FlexDecoding is intended for LLM inference workloads, including those with long-generation sequences. It does not change training-time attention semantics."
  },
  {
    "file_name": "aisp_1304.png",
    "caption": "Diagram illustrating overlapping compute and data transfer processes, highlighting separate compute and transfer streams to maximize parallelism and efficiency.",
    "description": "Diagram illustrating overlapping compute and data transfer processes, highlighting separate compute and transfer streams to maximize parallelism and efficiency.",
    "chapter": "3) Run forward/backward on compute_stream",
    "page_context": "Specifically, while the model is processing a batch on the default stream, the next batch’s data transfer is already underway on the `transfer_stream`. Synchronizing with `compute_stream.wait_stream(transfer_stream)` before consuming the preloaded batch enforces correct ordering without a full device-wide barrier. And the `.to(device, non_blocking=True)` calls make sure that the copy uses asynchronous DMA-based copies that don’t block the calling CPU thread. ... Using `next(dataloader_iter, None)` gives explicit control over when transfers are enqueued versus when the kernel operations run. This makes sure one batch of data is moving on the transfer stream while another batch is executing on the compute stream, as shown in [Figure 13-4](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch13.html#ch13_figure_4_1757308071410822). ... ###### Figure 13-4. Overlapping compute and data transfer with dedicated compute and transfer streams ... Additionally, by pulling from `dataloader_iter` ahead of time and storing in `next_inputs next_labels`, this code separates batch loading (running on the CPU in `transfer_stream`) from batch processing (running on the GPU in `compute_stream`). This split means you always have one batch in flight for each stream. This decouples data loading from compute and maximizes overlap."
  },
  {
    "file_name": "aisp_1305.png",
    "caption": "Visualization of memory allocation during three PyTorch training iterations, highlighting forward and backward passes.",
    "description": "Visualization of memory allocation during three PyTorch training iterations, highlighting forward and backward passes.",
    "chapter": "Profiling and Tuning Memory in PyTorch",
    "page_context": "Large models can be limited by GPU memory capacity and memory bandwidth. Additionally, inefficient memory use such as memory fragmentation can hurt performance even if HBM capacity is sufficient. You can address memory issues on several fronts, including memory-allocator tuning, activation checkpointing, memory offloading, and input-pipeline optimization. ... Also, PyTorch has a memory profiler that’s built into `torch.profiler` by enabling `profile_memory=True`, as shown earlier. You can use this to find out which operations allocate a lot of memory—and try to address those operations first, as shown in the visualization in [Figure 13-5](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch13.html#ch13_figure_5_1757308071410839) generated by the PyTorch [memory visualizer tool](https://oreil.ly/tX6gA). ... ###### Figure 13-5. PyTorch memory profile visualization for three iterations of a forward and backward pass ... Also, NVIDIA’s Nsight System’s CUDA Memory Inspector can help visualize how memory fragmentation happens over time. Utilizing these can guide your memory-allocator tuning efforts, as we’ll explore next."
  },
  {
    "file_name": "aisp_1306.png",
    "caption": "Diagram illustrating the sharding of model parameters, gradients, and optimizer states across multiple GPUs using FSDP's ZeRO Stage-3 strategy.",
    "description": "Diagram illustrating the sharding of model parameters, gradients, and optimizer states across multiple GPUs using FSDP's ZeRO Stage-3 strategy.",
    "chapter": "FSDP Automatic Checkpointing and Offloading",
    "page_context": "## FSDP Automatic Checkpointing and Offloading ... PyTorch’s FSDP is a distributed parallelism strategy that shards model parameters, gradients, and activations across GPUs during model training. This reduces memory overhead during training—and lets you train larger models than you could without sharding. Technically, FSDP implements the ZeRO Stage-3 strategy to shard model states across GPUs, as shown in Figure 13-6. ... ###### Figure 13-6. FSDP shards model parameters, gradients, and optimizer states across the GPUs (ZeRO Stage-3) ... FSDP can automatically apply activation checkpointing and offload parameters/gradients under the hood. Simply wrap the model with `FSDP()`, then specify the <span class=\"keep-together\"><code>activation_checkpointing_policy</code></span> and `CPUOffload` parameters as shown here:"
  },
  {
    "file_name": "aisp_1307.png",
    "caption": "vLLM benchmark dashboard displaying performance metrics for language models, including median and p99 latencies, illustrating changes between specific commits.",
    "description": "vLLM benchmark dashboard displaying performance metrics for language models, including median and p99 latencies, illustrating changes between specific commits.",
    "chapter": "PyTorch HUD Performance Dashboard",
    "page_context": "For example, you might see that, after a certain date, throughput dropped while memory usage increased. This indicates a potential increase in memory fragmentation—or that a less efficient kernel was picked. HUD helps correlate these types of changes directly with GitHub commits. ... It’s useful to monitor HUD to gauge if upstream PyTorch changes might affect your model. If your model is not included directly in the HUD, a model with a similar architecture may serve as a proxy. Also, HUD is open source, so you can mimic it yourself for your own model, hardware, and environment. ... ###### Figure 13-7. PyTorch HUD dashboard (source: https://oreil.ly/JxLKJ) ... Each data point on the chart is generated by comparing a performance benchmark between a base commit and the latest commit on PyTorch’s `main` branch. The dashboard allows selecting the time range (e.g., last 7 days, 30 days) and granularity (hourly, daily, weekly) to zoom in or out to show trends over time."
  },
  {
    "file_name": "aisp_0901.png",
    "caption": "Diagram of a Roofline model showing performance in GFLOPS versus operational intensity in FLOPS per byte, illustrating memory and compute bound conditions.",
    "description": "Diagram of a Roofline model showing performance in GFLOPS versus operational intensity in FLOPS per byte, illustrating memory and compute bound conditions.",
    "chapter": "Chapter 9. Increasing CUDA Kernel Efficiency and Arithmetic Intensity",
    "page_context": "The goal is to push the kernel toward the compute-bound regime and leverage the GPUs increasing computational power. A Roofline performance model can properly guide your optimizations toward that goal. ... As shown in a previous chapter, a roofline chart uses one horizontal line to represent the hardware’s peak compute throughput (the roof)—and a diagonal line from the origin represents the peak achievable throughput limited by memory bandwidth. A kernel’s arithmetic intensity determines where it falls on the x-axis, and its performance can be compared against these ceilings, as shown in Figure 9-1. ... ###### Figure 9-1. Example Roofline model (GFLOP/s versus arithmetic intensity in FLOPs/byte ... A kernel with low arithmetic intensity, or few math operations per byte of data moved, will be memory bound. In this case, the kernel’s speed is capped by the hardware’s memory bandwidth, because the GPU spends most of its time waiting for data rather than crunching numbers."
  },
  {
    "file_name": "aisp_0902.png",
    "caption": "Diagram illustrating multilevel tiling in GPU memory, showing data transfer between global memory, shared memory, and registers.",
    "description": "Diagram illustrating multilevel tiling in GPU memory, showing data transfer between global memory, shared memory, and registers.",
    "chapter": "Multilevel Microtiling and Software Prefetching",
    "page_context": "Whenever you restructure code so that each element is loaded once and used tens or hundreds of times, like in the case of tiling, you multiply your FLOPs per byte ratio by the reuse factor. For instance, in a typical matrix multiply, a 32 × 32 tile of A and B produces 1,024 (1,024 = 32 × 32) independent multiplies for each element in shared memory. As such, the arithmetic intensity rises compared to fetching each element directly from DRAM for every operation. ... Beyond simple shared-memory tiling,[]()[]()[]()[]() you can further increase intensity and expose more ILP with multilevel tiling. With multilevel tiling, after staging a tile into shared memory, you have each thread load microtiles into registers using vectorized types such as `float4` and `<half2>`. This way, repeated operations happen entirely in registers. An example of multilevel tiling is shown in [Figure 9-2](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch09.html#ch09_figure_2_1757308049959058). ... ###### Figure 9-2. Multilevel tiling between global memory (DRAM), shared memory (SMEM), and registers ... This intra-SM reuse (register → SMEM → DRAM) reduces the working set at every level—and minimizes off-chip traffic. As always, be sure to coalesce global reads when filling shared memory and pad/swizzle shared data to avoid memory-bank conflicts, as we covered in Chapter 7."
  },
  {
    "file_name": "aisp_0903.png",
    "caption": "Diagram illustrating the use of Distributed Shared Memory (DSMEM) by Cooperative Group Arrays (CGAs), highlighting shared and individual memory setups for thread blocks (CTAs).",
    "description": "Diagram illustrating the use of Distributed Shared Memory (DSMEM) by Cooperative Group Arrays (CGAs), highlighting shared and individual memory setups for thread blocks (CTAs).",
    "chapter": "Tiling with Thread Block Clusters",
    "page_context": "# Tiling with Thread Block Clusters ... On modern GPUs, you can extend the tiling-reuse idea using CUDA thread-block clusters from Cooperative Groups (discussed in Chapter 10). These allow multiple thread blocks to share data using distributed shared memory (DSMEM), as shown in Figure 9-3. ... ###### Figure 9-3. DSMEM shared by CTAs (thread blocks) within a CGA ... We cover CGAs and thread block clusters in detail in the next chapter, but they’re worth mentioning here, as they can directly increase arithmetic intensity. For example, a cluster of four thread blocks can cooperatively load one tile using the Tensor Memory Accelerator (TMA) multicast feature, as shown in Figure 9-4, which uses four CTAs to demonstrate this mechanism."
  },
  {
    "file_name": "aisp_0904.png",
    "caption": "Diagram illustrating the multicast loading of tiles A and B into a 2x2 thread block cluster, showing simultaneous distribution to four CTAs.",
    "description": "Diagram illustrating the multicast loading of tiles A and B into a 2x2 thread block cluster, showing simultaneous distribution to four CTAs.",
    "chapter": "Tiling with Thread Block Clusters",
    "page_context": "###### Figure 9-3. DSMEM shared by CTAs (thread blocks) within a CGA ... We cover CGAs and thread block clusters in detail in the next chapter, but they’re worth mentioning here, as they can directly increase arithmetic intensity. For example, a cluster of four thread blocks can cooperatively load one tile using the Tensor Memory Accelerator (TMA) multicast feature, as shown in Figure 9-4, which uses four CTAs to demonstrate this mechanism. ... ###### Figure 9-4. For these four (2 × 2) thread block clusters, each tile of A and B is loaded into four CTAs (thread blocks) simultaneously using multicast (source: https://oreil.ly/EEO_O) ... Each tile is partitioned across the four thread blocks so that global memory traffic for that tile is amortized over the cluster. The tiles are fetched only once and reused by all four thread blocks."
  },
  {
    "file_name": "aisp_0905.png",
    "caption": "Diagram illustrating the process of compressing sparse weights by selecting nonzero data values and their indexes for efficient computation with input activations, resulting in output activations.",
    "description": "Diagram illustrating the process of compressing sparse weights by selecting nonzero data values and their indexes for efficient computation with input activations, resulting in output activations.",
    "chapter": "Structured Sparsity",
    "page_context": "On modern GPUs, 2:4 structured sparsity is accelerated in hardware by Sparse Tensor Cores and cuSPARSELt. 2:4 means that exactly two out of every four consecutive weights are nonzero. Creating this type of sparsity is sometimes called pruning. ... By pruning half of the weights into a 2:4 pattern, each memory load now delivers twice as many nonzero values that actually participate in multiplication. In other words, you are no longer fetching weights that turn out to be zero. As such, you are not wasting a matrix multiply operation on something that you know is zero, as shown in Figure 9-5. ... ###### Figure 9-5. 2:4 structured sparsity ... Structured sparsity is applied after a model is trained. The model is pruned and optimized for inference. Pruning and format conversion are done in software stacks such as cuSPARSELt and framework tooling. Note that the Transformer Engine accelerates supported sparse executions but does not enforce sparsity during conversion."
  },
  {
    "file_name": "aisp_0906.png",
    "caption": "Diagram illustrating the organization of shared memory and Tensor Memory within subpartitions, showing Tensor Cores, 32 lanes, and registers.",
    "description": "Diagram illustrating the organization of shared memory and Tensor Memory within subpartitions, showing Tensor Cores, 32 lanes, and registers.",
    "chapter": "Feeding Tensor Cores with TMEM and TMA",
    "page_context": "## Feeding Tensor Cores with TMEM and TMA ... At the heart of high‐throughput tensor computation is TMEM, a 256 KB SRAM buffer per SM. At a high level, programmers do not explicitly allocate or manage TMEM, however. TMEM is handled by the hardware or libraries when you use Tensor Core operations. TMEM is shown in Figure 9-6. ... ###### Figure 9-6. TMEM supports the Tensor Cores by accumulating partial results (instead of registers) ... Under the hood, Blackwell uses `tcgen05.mma` instructions that operate with TMEM for operand and accumulator storage.[]()[]()[]() CUTLASS and library kernels manage the required allocation and usage through the kernel configuration and Parallel Thread Execution (PTX) assembly. As such, the Transformer Engine uses TMEM for partial results. This reduces the MMA dependencies on registers."
  },
  {
    "file_name": "aisp_0907.png",
    "caption": "Diagram illustrating mixed-precision matrix multiply-accumulate (MMA) with FP16 inputs, full precision products, FP32 accumulation, and a final conversion to FP32 result.",
    "description": "Diagram illustrating mixed-precision matrix multiply-accumulate (MMA) with FP16 inputs, full precision products, FP32 accumulation, and a final conversion to FP32 result.",
    "chapter": "TF32 and Automatic Mixed Precision (PyTorch)",
    "page_context": "In PyTorch, these mixed-precision decisions are integrated into the compiler so you get optimal `dtype` selection (e.g., FP16/FP8 for compute, FP32 for accumulations) without requiring manual intervention. This is shown in [Figure 9-7](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch09.html#ch09_figure_7_1757308049959152) as a mixed-precision matrix multiply-accumulate (MMA). ... This automatic mixed-precision pipeline maximizes arithmetic intensity with minimal code changes. The fused Tensor Core kernels minimize round trips to HBM by staging and reusing data in shared memory (e.g., operands) and TMEM (e.g., accumulators). ... ###### Figure 9-7. Mixed-precision and matrix multiply-accumulate (MMA) ... When using structured sparsity, described earlier, or extreme low-precision (FP8/FP4), be sure to maintain a large enough batch size or tile granularity so that TMEM and Tensor Cores remain fully utilized. Small batches incur overhead, including format conversions, sparse index handling, irregular memory patterns, etc. This can reduce achieved speedups."
  },
  {
    "file_name": "aisp_0908.png",
    "caption": "Diagram comparing memory copy processes, showing how cp.async instructions bypass the register file and optionally L1 cache to load data directly into shared memory (SMEM).",
    "description": "Diagram comparing memory copy processes, showing how cp.async instructions bypass the register file and optionally L1 cache to load data directly into shared memory (SMEM).",
    "chapter": "Using CUTLASS for Optimal Arithmetic Intensity and Tensor Core Performance",
    "page_context": "Depending on the precision, a 256 × 512 tile would max out the 256 KB per-SM TMEM budget since 256 × 512 elements × 2 bytes per element = 256 KiB. And 256 × 256 elements × 4 bytes per element = 256 KB. Larger tiles improve per-tile throughput but reduce the number of concurrent tiles per SM. This can lead to underutilization on smaller GEMMs. In contrast, very small tiles sacrifice arithmetic intensity for parallelism. ... CUTLASS then emits asynchronous memory copies (`cp.async` or TMA) that stream each tile from DRAM into shared memory. The `cp.async` instruction stages data from global memory into shared memory without using per-thread registers (or optionally L1 cache), as shown in [Figure 9-8](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch09.html#ch09_figure_8_1757308049959168). The caching behavior is controlled using `cp.async` modifiers or by using TMA for bulk tensor transfers. ... ###### Figure 9-8. Using the asynchronous memory copy instruction (cp.async) to load data from global memory into shared memory without involving the register file and optionally the L1 cache ... CUTLASS stages tiles from global DRAM into SMEM using `cp.async` or TMA (`cp.async.bulk.tensor`). Tensor Core `tcgen05.mma` instructions then read operands from SMEM and accumulate the results implicitly into TMEM. This creates a software-managed staging area in shared memory, which is used for double-buffering. This way, while the Tensor Cores are processing the current tile, TMA is already fetching the next tile into shared memory."
  },
  {
    "file_name": "aisp_1401.png",
    "caption": "Diagram illustrating the components and flow of the PyTorch compiler stack, including stages like TorchDynamo, AOT Autograd, and integration with backends such as Inductor and Triton.",
    "description": "Diagram illustrating the components and flow of the PyTorch compiler stack, including stages like TorchDynamo, AOT Autograd, and integration with backends such as Inductor and Triton.",
    "chapter": "Chapter 14. PyTorch Compiler, OpenAI Triton, and XLA Backends",
    "page_context": "We also cover tools for debugging the compilation pipeline as well as libraries for scaling PyTorch across multi-GPU and multinode clusters. We will then explore how `torch.compile` works under the hood and how to handle dynamic shapes and variable sequence lengths efficiently. ... We will also examine the PyTorch compiler’s integration with the OpenAI Triton ecosystem. Our goal is to accelerate and scale our PyTorch models and applications without sacrificing the flexible, eager-execution development experience of PyTorch. ... ###### Figure 14-1. Overview of PyTorch compiler stack ... # PyTorch Compiler Deep Dive"
  },
  {
    "file_name": "aisp_1402.png",
    "caption": "Diagram of the PyTorch compiler pipeline showing stages of graph acquisition, graph lowering, and graph compilation leading to optimized kernel generation for GPUs.",
    "description": "Diagram of the PyTorch compiler pipeline showing stages of graph acquisition, graph lowering, and graph compilation leading to optimized kernel generation for GPUs.",
    "chapter": "PyTorch Compiler Deep Dive",
    "page_context": ") ... ``` ... This section breaks down the PyTorch compilation pipeline steps, including TorchDynamo’s graph capture, AOT Autograd’s combined forward/backward graph optimization, PrimTorch IR, and TorchInductor’s code generation. This pipeline is responsible for producing optimized kernels for the target GPU hardware and is shown in Figure 14-2. ... ###### Figure 14-2. PyTorch compiler pipeline (source: https://oreil.ly/55JDn) ... ## TorchDynamo for Bytecode Capture and Graph Extraction"
  },
  {
    "file_name": "aisp_1403.png",
    "caption": "Diagram comparing PyTorch's eager execution with compiled mode, highlighting how the compiler traces and compiles operations into a single kernel, improving performance by utilizing a cache and compiled graph.",
    "description": "Diagram comparing PyTorch's eager execution with compiled mode, highlighting how the compiler traces and compiles operations into a single kernel, improving performance by utilizing a cache and compiled graph.",
    "chapter": "TorchDynamo for Bytecode Capture and Graph Extraction",
    "page_context": "By capturing whole sequences of operations, TorchDynamo can perform whole-graph optimizations like kernel fusion to reduce launch overhead and minimize memory movement. It also eliminates Python-layer overhead for those fused operations. Figure 14-3 compares eager versus compiled modes, including the compiler cache. ... Instead of executing each small operation and kernel launch separately, the compiler can fuse many operations into a single kernel. This reduces CPU-GPU synchronization points and improves memory locality. Otherwise, the GPU would be bottlenecked with lots of fine-grained operations, heavy synchronization, and excessive global memory accesses. ... ###### Figure 14-3. PyTorch eager versus compiled modes ... TorchDynamo continues capturing until[]()[]() a graph break is needed. A graph break can be triggered by unsupported Python constructs (like certain control flows, e.g., an `if` statement using a Python bool instead of tensor operations) or by unsupported operations. When a graph break occurs, the current graph segment ends, and Dynamo falls back to eager mode for the code that can’t be captured. After that, TorchDynamo will start a new graph trace once it returns to traceable code."
  },
  {
    "file_name": "aisp_1404.png",
    "caption": "Diagram illustrating the PyTorch compilation and export process, showing how TorchInductor and AOTInductor integrate with torch.compile() and torch.export() for model deployment.",
    "description": "Diagram illustrating the PyTorch compilation and export process, showing how TorchInductor and AOTInductor integrate with torch.compile() and torch.export() for model deployment.",
    "chapter": "TorchInductor Backend Code Generation",
    "page_context": "Inductor also supports symbolic shapes, which allow dynamic dimensions. The IR is somewhat higher-level than raw CUDA, as it represents things like elementwise computations as loops over tensor indices. The IR is user-inspectable, which helps debugging and even extending. ... TorchInductor’s IR can also be used[]()[]() for ahead-of-time compilation flows with `torch.export()` and AOTInductor. AOTInductor compiles artifacts produced by `torch.export` for AOT use cases such as packaging and deployment. This allows saving and reusing the compiled code across runs. Export is highlighted in the context of `torch.compile()` in [Figure 14-4](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch14.html#ch14_figure_4_1757308073242653). ... ###### Figure 14-4. PyTorch compile and export (TorchDynamo → AOT Autograd → PrimTorch IR → TorchInductor → Triton/LLVM NVPTX); export via torch.export/AOTInductor; CUDA Graphs are used when shapes are static ... For NVIDIA GPU backends, TorchInductor uses OpenAI’s Triton JIT compiler to generate the actual GPU kernels. Triton is a CUDA-like domain-specific language (DSL) written in Python. Triton also includes a compiler for its DSL (we’ll cover Triton more in a bit)."
  },
  {
    "file_name": "aisp_1405.png",
    "caption": "Diagram showcasing graph breaks in PyTorch's FSDP strategy due to collective communication layers, highlighting all-gather and reduce-scatter operations across GPUs.",
    "description": "Diagram showcasing graph breaks in PyTorch's FSDP strategy due to collective communication layers, highlighting all-gather and reduce-scatter operations across GPUs.",
    "chapter": "Graph Breaks and TorchDynamo explain()",
    "page_context": "Graph breaks are the enemy of performance. Each break means an optimized graph is cut short—and more Python overhead is introduced. If you compile a model and see only modest speedups, it may be caused by frequent graph breaks that are preventing large, fused graphs. Ideally, we want as few breaks as possible—ideally one large graph for the whole model or whole training step. ... Complex graphs that involve collective communications (e.g., all-gather, reduce-scatter, etc.) often require graph breaks. Figure 14-5 shows the graph breaks in PyTorch’s FSDP strategy due to collective communication. ... ###### Figure 14-5. Graph breaks in PyTorch FSDP caused by communication layers (source: https://oreil.ly/TJW42) ... PyTorch provides `torch._dynamo.explain()`[]()[]()[]()[]() to help analyze and debug graph breaks. When invoking this debugging function with your model and example inputs, it will run the model within TorchDynamo and return a report of how many graphs were generated, where the breaks occurred, and why they happened, as shown here, followed by the detailed graph-break analysis and explanation:"
  },
  {
    "file_name": "aisp_1406.png",
    "caption": "Diagram illustrating OpenXLA's integration with machine learning frameworks like TensorFlow, JAX, and PyTorch, and its support for CPUs, GPUs, and xPUs.",
    "description": "Diagram illustrating OpenXLA's integration with machine learning frameworks like TensorFlow, JAX, and PyTorch, and its support for CPUs, GPUs, and xPUs.",
    "chapter": "PyTorch XLA Backend",
    "page_context": "# PyTorch XLA Backend ... While TorchInductor is the PyTorch default backend for GPUs and CPUs, PyTorch XLA is a separate backend-compilation option that targets Google Cloud TPUs and other accelerators. PyTorch XLA allows PyTorch models to run on these accelerators by mapping PyTorch operations into XLA’s graph IR and executing them using the target hardware’s runtime, as shown in Figure 14-6. ... ###### Figure 14-6. OpenXLA, the basis of the PyTorch XLA compiler backend ... To activate the XLA backend,[]() you can use `torch.compile(..., backend=\"openxla\")`, which activates PyTorch XLA based on [OpenXLA](https://openxla.org/). This backend string is supported by the PyTorch XLA project and activates OpenXLA-based compilation. Similar to TorchDynamo and TorchInductor, XLA captures the graph of computations. However, it compiles whole programs ahead of time because XLA is designed to generate static graphs."
  },
  {
    "file_name": "aisp_1501.png",
    "caption": "Diagram illustrating the overlap of KV cache transfers with computations during prefill and decode stages, showing that transfer affects tokens 1 and 2 only.",
    "description": "Diagram illustrating the overlap of KV cache transfers with computations during prefill and decode stages, showing that transfer affects tokens 1 and 2 only.",
    "chapter": "Disaggregated Prefill and Decode Architecture",
    "page_context": "Cross-vendor or cross-architecture deployments require that the KV cache layout and dtypes match across both sides. In practice, production systems should keep prefill and decode on compatible GPU families. This way, they use the same numeric formats to easily enable KV cache transfer and data reuse. ... In the prefill stage, the model processes the entire input prompt—often thousands, tens of thousands, or even millions of tokens—in a single forward pass to produce initial hidden states as calculated by the LLM. It then populates the attention key-value (KV) cache for all tokens in the input prompt. Figure 15-1 shows how disaggregated prefill and decode share the KV cache and overlap KV transfers with computations. ... ###### Figure 15-1. Disaggregated prefill and decode sharing the KV cache and overlapping KV transfers with computations ... In the decode stage, the model performs an autoregressive generation to predict each new token in the sequence. It does this by consuming the cached attention KV representations of all previously generated tokens."
  },
  {
    "file_name": "aisp_1502.png",
    "caption": "Diagram of disaggregated inference architecture showing separate prefill and decoding instances with GPU clusters, illustrating the transfer of KV cache between them.",
    "description": "Diagram of disaggregated inference architecture showing separate prefill and decoding instances with GPU clusters, illustrating the transfer of KV cache between them.",
    "chapter": "Scaling Prefill and Worker Nodes Independently",
    "page_context": "We can scale prefill and decode separately by dedicating one set of nodes to handle the prefill and another set of nodes to handle the decode. The two clusters communicate only when transferring the encoded prompt state, or attention KV cache, from the prefill workers to the decode workers, as shown in Figure 15-2. ... Here, you see separate GPU workers handling the prefill stage to process the input prompt—along with the decode stage to generate output tokens iteratively. The output of the prefill stage includes the KV cache for the prompt. It is transferred to the decode workers to generate the next tokens. ... ###### Figure 15-2. Disaggregated inference: Prefill pool (hidden state + KV) → KV handoff using NVLink/NVSwitch (intranode) or GPUDirect RDMA (internode) → Decode pool ... By dedicating separate GPU pools, the system keeps both prefill and decode pipelines busy in parallel. In practice, disaggregation has been shown to significantly improve throughput under strict latency constraints. Some studies show that large gains are possible, but results range from moderate improvements to several times higher goodput once the prefill and decode stages are separated. The results greatly depend on the workload and network fabric."
  },
  {
    "file_name": "aisp_1503.png",
    "caption": "Diagram depicting the integration of NIXL in the vLLM inference engine, showing data flow between proxy, decode worker, and prefill worker over various communication methods.",
    "description": "Diagram depicting the integration of NIXL in the vLLM inference engine, showing data flow between proxy, decode worker, and prefill worker over various communication methods.",
    "chapter": "KV Cache Data Transfer and NIXL",
    "page_context": "Specifically, LMCache and NIXL are integrated in vLLM’s disaggregated prefilling as the supported path. NIXL is also used by NVIDIA Dynamo and TensorRT-LLM to transport KV cache data using peer-to-peer GPU interconnects and RDMA. ... Within a node, NIXL performs device-to-device transfers over NVLink and NVSwitch without host staging. Across nodes, NIXL uses GPUDirect RDMA over InfiniBand or RoCEv2 to avoid host copies. These paths keep KV cache handoff latency low even for multigigabyte payloads. ... ###### Figure 15-3. KV cache data transfers with NIXL in the vLLM inference engine; Intranode: NVLink/NVSwitch (device-to-device); Internode: GPUDirect RDMA (InfiniBand/RoCEv2) using ConnectX-class NICs ... Placement of prefill and decode workers should follow the fabric. Same node placement keeps KV transfers on NVLink and NVSwitch via CUDA peer access, while cross-node placement should use GPUDirect RDMA over InfiniBand or RoCEv2. NVIDIA Dynamo integrates with NIXL to move KV cache between GPUs, CPU memory, and storage across nodes, and vLLM integrates through LMCache and NIXL for disaggregated prefilling."
  },
  {
    "file_name": "aisp_1504.png",
    "caption": "Diagram of a Kubernetes-based vLLM cluster showing separate prefill and decode workers using llm-d, with components like inference gateway, inference pool, variant autoscaler, and nodes from NVIDIA, Google, AMD, and Intel.",
    "description": "Diagram of a Kubernetes-based vLLM cluster showing separate prefill and decode workers using llm-d, with components like inference gateway, inference pool, variant autoscaler, and nodes from NVIDIA, Google, AMD, and Intel.",
    "chapter": "Deploying Disaggregated Prefill and Decode with Kubernetes",
    "page_context": "In contrast, if many users arrive requesting superlong outputs (e.g., long reasoning chains, “think step-by-step,” etc.), more GPUs can be shifted to the decode pool. In both cases, new instances can be scaled out for each worker type. ... Figure 15-4 shows a distributed, Kubernetes-based vLLM cluster of separate prefill and decode workers using the open source llm-d project. vLLM implements disaggregated prefilling by running two instances and handing off KV using LMCache and NIXL, but llm-d extends this with Kubernetes native orchestration for disaggregated serving and KV-aware routing. This diagram shows a component called the variant autoscaler, which is responsible for updating the number of replicas for the prefill and decode workers in the pool. ... ###### Figure 15-4. Kubernetes-based vLLM cluster of separate prefill and decode workers using the open source llm-d project; variant autoscaler tunes prefill/decode replica counts based on the prompt-response mix; KV moved using LMCache and NIXL ... In modern inference deployments, all nodes can perform both prefill and decode functionality since they all share the same runtime and code base (e.g., vLLM, SGLang, NVIDIA Dynamo, etc.). It’s up to the cluster orchestrator to assign them a specific role, either prefill or decode, statically upon startup—and dynamically throughout the worker’s lifecycle."
  },
  {
    "file_name": "aisp_1505.png",
    "caption": "Diagram illustrating the division of model weights and input data across GPUs, demonstrating different parallelism strategies and their combinations.",
    "description": "Diagram illustrating the division of model weights and input data across GPUs, demonstrating different parallelism strategies and their combinations.",
    "chapter": "Parallelism Strategies for Serving Massive MoE Models",
    "page_context": "For intranode TP on Blackwell NVL72, prefer keeping TP groups within a single NVSwitch domain; extend inter-rack only when topology permits to avoid extra hops. ... These parallelism strategies define how the model weights and data are split over the GPUs. Figure 15-5 shows how they are split up for the different parallelism strategies—as well as common combinations of strategies. ... ###### Figure 15-5. Model weights and input data split over the GPUs ... ## Tensor Parallelism"
  },
  {
    "file_name": "aisp_1506.png",
    "caption": "Diagram illustrating a mixture-of-experts (MoE) layer workflow, showing a gating network routing tokens to selected experts for processing and then combining the results.",
    "description": "Diagram illustrating a mixture-of-experts (MoE) layer workflow, showing a gating network routing tokens to selected experts for processing and then combining the results.",
    "chapter": "Expert Parallelism",
    "page_context": "Expert parallelism (EP) is specific to MoE architectures. In an MoE layer, there are many expert networks, or feed-forward sublayers. For each input token, only one or a few experts are activated. This naturally lends itself to distributing different experts on different GPUs. ... For instance, if an MoE layer has 16 experts and we have 4 GPUs, each GPU could host 4 experts. During inference, when a token arrives at that MoE layer, its internal gating network will choose the top two experts, for instance, for that token. The token’s data is then sent to whichever GPUs own those two experts for processing. Then the results are combined back to generate the next token, as shown in Figure 15-6. ... ###### Figure 15-6. Mixture-of-experts (MoE) communication (source: https://oreil.ly/pzn5t) ... Implementing this requires an all-to-all communication pattern such that tokens (technically, their activation vectors) are dynamically shuffled between GPUs so that each token lands on the GPU of its assigned expert. After each expert computes its output for its assigned tokens, another all-to-all is performed to return the token outputs back to their original order."
  },
  {
    "file_name": "aisp_1507.png",
    "caption": "Diagram illustrating a hybrid parallel configuration using four GPUs, showcasing a 4 × 2 EP hybrid parallel strategy with pipeline stages, tensor parallelism, and expert parallelism across expert modules.",
    "description": "Diagram illustrating a hybrid parallel configuration using four GPUs, showcasing a 4 × 2 EP hybrid parallel strategy with pipeline stages, tensor parallelism, and expert parallelism across expert modules.",
    "chapter": "Hybrid Parallelism",
    "page_context": "## Hybrid Parallelism ... In practice, serving massive MoE LLMs uses a combination of the previous parallelism strategies. Today’s LLM models are so large and complex that no single parallelization method is sufficient. Figure 15-7 shows a hybrid parallel configuration using four GPUs. ... ###### Figure 15-7. High-level diagram of a 4 × 2 × EP hybrid parallel combination (source: https://oreil.ly/q1AEf) ... Here, we are using four pipeline stages (one per GPU) and two-way tensor parallelism. The tokens are routed across two experts using expert parallelism. This is called a 4 × 2 EP hybrid parallel strategy."
  },
  {
    "file_name": "aisp_1508.png",
    "caption": "Diagram illustrating speculative decoding where a draft model predicts tokens which are then verified by a target model, highlighting a parallel verification process.",
    "description": "Diagram illustrating speculative decoding where a draft model predicts tokens which are then verified by a target model, highlighting a parallel verification process.",
    "chapter": "Two-Model, Draft-Based Speculative Decoding and EAGLE",
    "page_context": "Once the target model’s corrected token is used and decoding continues, a new speculative decoding cycle can start from that point. Figure 15-8 shows a small draft model predicting multiple tokens ahead. Then the target (big) model verifies these tokens one by one. ... Over time, speculative decoding reduces the overall number of one-by-one, sequential invocations needed by the large model. In theory, it provides a theoretical k× speedup, where k is the number of tokens generated by the draft model. In practice, with overhead and occasional speculative-token rejections, the gain is more like a 2× speedup. ... ###### Figure 15-8. Speculative decoding with a draft (small) for decoding and a target (large) model for multitoken verification ... The draft model must be chosen to have reasonably high fidelity to the large model’s distribution. This means that the draft model’s predictions should have high overlap with the large model’s likely outputs. If the draft frequently guesses wrong, speculative decoding provides little benefit and wastes compute. If it often predicts tokens that the larger target model would not, many speculative tokens will be rejected. This will waste compute time."
  },
  {
    "file_name": "aisp_1509.png",
    "caption": "Diagram illustrating how EAGLE-2 employs adaptive speculative decoding with dynamic draft trees to improve inference speed and efficiency compared to previous methods.",
    "description": "Diagram illustrating how EAGLE-2 employs adaptive speculative decoding with dynamic draft trees to improve inference speed and efficiency compared to previous methods.",
    "chapter": "Two-Model, Draft-Based Speculative Decoding and EAGLE",
    "page_context": "EAGLE-3 continues to improve on earlier versions, EAGLE-1 and EAGLE-2, by preferring direct token prediction and fusing multi-layer features. EAGLE-3 reports up to 1.4× improvements over EAGLE-2 in certain tasks—and up to 6.5× speedups over non-optimized baseline variants. In EAGLE-1 and EAGLE-2, the draft model predicts internal feature vectors which are then decoded to tokens. These earlier EAGLE methods worked by guessing internal features, essentially—and then mapping them to tokens. ... EAGLE-3 skips the feature-level prediction step and predicts the draft tokens more directly. However, it still uses internal features but fused into multiple layer representations (lower, middle, upper) to guide the draft predictions. This is in contrast to using just the top layer. This makes EAGLE-3 more streamlined and less constrained—allowing better scaling. ... ###### Figure 15-9. Speculative decoding with EAGLE-2 (source: https://oreil.ly/uG07b) ... Another technique is dynamic depth decoding, which can adaptively skip layers that minimize the impact on output quality. Other techniques that reduce computations include skipping every Nth transformer layer, using lower precision (e.g., FFP8 and NVFP4) for the draft model, and using a smaller hidden size temporarily for the draft stage."
  },
  {
    "file_name": "aisp_1510.png",
    "caption": "Diagram illustrating the Medusa framework for multitoken decoding, showing how multiple Medusa heads generate and verify token sequences in parallel, integrated with an original model and producing top-k predictions.",
    "description": "Diagram illustrating the Medusa framework for multitoken decoding, showing how multiple Medusa heads generate and verify token sequences in parallel, integrated with an original model and producing top-k predictions.",
    "chapter": "Multitoken Decoding with Medusa’s Multiple Heads",
    "page_context": "However, Medusa requires custom model training since it modifies the transformer-based LLM with additional decoder heads that branch off at certain layers—hence, the name Medusa. This lets the model propose several next tokens simultaneously. ... The multiple token candidates generated by the different Medusa heads are structured like a tree. For instance, Medusa can generate a binary tree of depth 2 in one pass to produce up to 4 tokens. It can then verify the sequence of multiple tokens in parallel, as shown in Figure 15-10. ... ###### Figure 15-10. Multitoken decoding with Medusa (source: https://oreil.ly/MJMOQ) ... Internally, Medusa uses a specialized, tree-based attention pattern to improve consistency among the parallel token predictions. The model learns to extend and verify the multiple-token sequences concurrently. With Medusa, the LLM essentially learns to “think ahead” by a few tokens—and output them all at once."
  },
  {
    "file_name": "aisp_1511.png",
    "caption": "Diagram illustrating the distribution of tokens across multiple GPUs, each hosting different experts, through a central router.",
    "description": "Diagram illustrating the distribution of tokens across multiple GPUs, each hosting different experts, through a central router.",
    "chapter": "Expert Communication Optimization",
    "page_context": "## Expert Communication Optimization ... During the all-to-all exchange of token activations across experts, an MoE shuffles a batch of tokens between the GPUs. Each GPU receives only the tokens it needs for the experts that it hosts, as shown in Figure 15-11. This happens for every layer in the MoE and is a costly operation, which can potentially dominate inference time if not handled efficiently. ... ###### Figure 15-11. Each GPU receives only the tokens it needs for the expert(s) that it hosts ... One strategy to reduce communication overhead is to use a hierarchical routing strategy for our GPU cluster by first routing tokens between GPUs within the same node using NVSwitch/NVLink (fast) and routing across nodes only for any remaining tokens that need nonlocal experts. This two-stage all-to-all can reduce the volume of internode traffic."
  },
  {
    "file_name": "aisp_1512.png",
    "caption": "Diagram comparing the distribution of tokens across experts under capacity factors of 1.0 and 1.5, illustrating how excess tokens are handled.",
    "description": "Diagram comparing the distribution of tokens across experts under capacity factors of 1.0 and 1.5, illustrating how excess tokens are handled.",
    "chapter": "Load Balancing, Capacity Factor, and Expert Replication",
    "page_context": "At inference time, however, specific input prompts or topics might still cause imbalance by concentrating on a subset of “hot” experts. One strategy to avoid inference hot spots is to use a capacity factor that triggers an overflow mechanism. ... By specifying a capacity factor, the model can be configured such that each expert can process only a maximum number of tokens (e.g., 32 tokens) at a given time. If an expert receives more than this capacity of tokens, the extra tokens can either be forwarded to a fallback expert with the next highest routing score or the tokens will be serialized and processed in a second pass. Figure 15-12 compares a capacity factor of 1.0 versus 1.5. ... ###### Figure 15-12. Comparing expert capacity factors of 1.0 versus 1.5 ... In practice, a capacity factor of 1.2 (20% overflow allowance) with top-2 gating is common. This means that each expert will take up to 120% of its average load. After that, it will send excess tokens to the next expert. This will effectively smooth out the load across experts in the system."
  },
  {
    "file_name": "aisp_1513.png",
    "caption": "Diagram illustrating an adaptive MoE routing strategy using biased gating scores to balance expert load, showing gating scores, expert bias updates, and feedback mechanisms.",
    "description": "Diagram illustrating an adaptive MoE routing strategy using biased gating scores to balance expert load, showing gating scores, expert bias updates, and feedback mechanisms.",
    "chapter": "Adaptive Expert Routing and Real-Time Monitoring",
    "page_context": "Inference engines rely on real-time metrics like per-GPU utilization and per-expert token counts to continuously measure expert load. If the system sees one expert’s GPU at 99% utilization while other experts’ GPUs are at 60%, the system could temporarily lower its load by routing some tokens to its expert replica—or to a different expert with a slightly lower expert-preference score. ... Figure 15-13 shows an adaptive MoE routing strategy that uses a biased gating score approach. While this approach was originally used in a training context, a simpler approach applies to inference. In this case, it would use a modified expert-bias algorithm to divert tokens to alternate experts when the primary experts are heavily loaded. ... ###### Figure 15-13. Adaptive MoE routing in action ... This approach can reduce unnecessary communication and balance the load more uniformly. However, it does incur some additional cost in the form of extra monitoring, decision making, complexity, configuration management, and logging. The benefits may or may not outweigh the cost. Every scenario and workload is unique, but it’s definitely something worth exploring."
  },
  {
    "file_name": "aisp_1601.png",
    "caption": "Diagram illustrating the lifecycle of a request in a disaggregated prefill and decode LLM inference system, showing interactions between the orchestrator and prefill instances for token management and cache handling.",
    "description": "Diagram illustrating the lifecycle of a request in a disaggregated prefill and decode LLM inference system, showing interactions between the orchestrator and prefill instances for token management and cache handling.",
    "chapter": "Profiling, Debugging, and Tuning Inference Performance",
    "page_context": "# Profiling, Debugging, and Tuning Inference Performance ... There are a lot of moving parts in modern LLM inference engines—especially with disaggregated prefill and decode. The lifecycle of a typical request involves many components, as shown in Figure 16-1. ... ###### Figure 16-1. Lifecycle of a typical request in a disaggregated prefill and decode LLM inference system ... Given such complexity, the workflow for tuning inference performance is very iterative. It requires careful tuning and continuous verification."
  },
  {
    "file_name": "aisp_1602.png",
    "caption": "Diagram comparing traditional microservice invocations as fast and uniform, with LLM requests as slow, nonuniform, and expensive, highlighting the need for LLM-aware load balancing.",
    "description": "Diagram comparing traditional microservice invocations as fast and uniform, with LLM requests as slow, nonuniform, and expensive, highlighting the need for LLM-aware load balancing.",
    "chapter": "Monitoring System Metrics and Counters",
    "page_context": "## Monitoring System Metrics and Counters ... Unlike traditional microservice invocations, which are relatively uniform and predictable in their execution time, LLM requests are nonuniform and can vary wildly in terms of latency. This difference is shown in Figure 16-2. ... ###### Figure 16-2. Difference between traditional microservice invocations and LLM invocations ... For ongoing monitoring in production, it’s common to use Prometheus to collect metrics from each GPU compute node—as well as Grafana dashboards to visualize them. Key GPU metrics to track include GPU utilization (percent of time the SMs are busy), GPU memory usage, copy engine utilization, PCIe and NVLink throughput, and GPU temperature and power (e.g., throttling). Note: Low-level counters such as L1 and L2 activity, occupancy, and instruction throughput can be collected with Nsight Compute or CUPTI rather than DCGM and Prometheus."
  },
  {
    "file_name": "aisp_1603.png",
    "caption": "Diagram illustrating how DCGM collects GPU metrics from Kubernetes pods and exports them to Prometheus, with components like service monitor and node-exporter shown in the process flow.",
    "description": "Diagram illustrating how DCGM collects GPU metrics from Kubernetes pods and exports them to Prometheus, with components like service monitor and node-exporter shown in the process flow.",
    "chapter": "Monitoring System Metrics and Counters",
    "page_context": "This way, when you deploy a new optimization to increase batching, for instance, Grafana will immediately show if GPU utilization on each GPU increases. You can also monitor to make sure p95/p99 latencies stay within the target. ... Counters are extremely useful to measure as well—especially with dynamic and adaptive systems. For instance, if your inference engine dynamically adapts the batch size to current conditions, you may want to increment a “batch size change” counter. ... ###### Figure 16-3. DCGM collects metrics from the Kubernetes GPU nodes and sends them to Prometheus ... The other option is to log the change in a logfile, but this would require a slow, text-based search/aggregation to analyze the logfile offline using Apache Spark, for instance. You would then need to manually correlate the result of the logfile analysis with the Prometheus metrics."
  },
  {
    "file_name": "aisp_1604.png",
    "caption": "Diagram illustrating the process flow of Nsight Compute’s CUDA Program Counter sampling feature, showing interactions between user, host process, application process, and CUDA components.",
    "description": "Diagram illustrating the process flow of Nsight Compute’s CUDA Program Counter sampling feature, showing interactions between user, host process, application process, and CUDA components.",
    "chapter": "Profiling with Nsight Systems and Nsight Compute",
    "page_context": "Nsight Compute lets us profile individual kernels to pinpoint inefficiencies. We can use the Nsight Compute’s section-based profiling feature to focus on specific parts of the kernel, such as memory transactions. ... Another super useful tool that isn’t well known is Nsight Compute’s CUDA Program Counter (PC) Sampling feature. This samples program counters and identifies hotspots without requiring full, heavyweight instrumentation, as shown in Figure 16-4. ... ###### Figure 16-4. Nsight Compute’s CUDA Program Counter (PC) sampling feature helps identify hotspots in a low-overhead manner (source: https://oreil.ly/DyKWR) ... Specifically, we can use this to profile live inference servers and pinpoint exactly which kernel instructions are taking the most time. And we can do this in a low-overhead manner. Now that we’ve covered profiling with Nsight Systems and Nsight Compute, let’s discuss some common troubleshooting recipes for inference."
  },
  {
    "file_name": "aisp_1605.png",
    "caption": "Diagram comparing static and dynamic batching over time; static batching shows fixed-size groupings, while dynamic batching adjusts group sizes for efficiency.",
    "description": "Diagram comparing static and dynamic batching over time; static batching shows fixed-size groupings, while dynamic batching adjusts group sizes for efficiency.",
    "chapter": "Dynamic Batching",
    "page_context": "With dynamic batching, the system accumulates incoming requests and dispatches “whatever has arrived” once either a target batch size is met or a short timeout (e.g., 2 ms) elapses. This bounds maximum latency to the timeout value that you specify. ... With its on-the-fly sizing, dynamic batching lets you amortize kernel-launch overheads across multiple sequences—while avoiding the worst‐case delays of static batching. This improves both GPU utilization and predictable latency under variable load. Figure 16-5 shows the difference between static batching and dynamic batching. ... ###### Figure 16-5. Difference between static and dynamic batching ... Dynamic batching lets the system automatically grow or shrink batch sizes based on actual request-arrival patterns and latency targets (e.g., max delay). The key is to batch intelligently in a way that increases overall throughput while maintaining latency within acceptable bounds. Modern inference engines implement microbatching, which accumulates requests for only a few milliseconds before dispatching the batch to the GPU. Typically, a delay of 2–10 ms is used, but this should be tuned to meet latency SLOs."
  },
  {
    "file_name": "aisp_1606.png",
    "caption": "Diagram illustrating continuous batching, showing sequences joining a batch mid-generation and efficiently filling compute slots to reduce idle time and improve throughput.",
    "description": "Diagram illustrating continuous batching, showing sequences joining a batch mid-generation and efficiently filling compute slots to reduce idle time and improve throughput.",
    "chapter": "Continuous Batching",
    "page_context": "Continuous batching, also known as in-flight batching or iteration-level scheduling, maintains high GPU utilization by refilling batches on every token-generation iteration rather than waiting for complete sequences to finish. It evicts completed requests and immediately pulls in new ones based entirely on GPU readiness. This technique is particularly important for low-latency use cases such as chat assistants. ... In contrast to timeout-driven approaches like dynamic batching, the event-driven continuous batching strategy eliminates idle compute slots and the padding overhead of these other approaches. By never relying on a fixed “max-delay” timer, continuous batching allows new requests to join an ongoing batch mid-generation—and without blocking on the longest sequence, as shown in Figure 16-6. ... ###### Figure 16-6. Continuous batching allows new sequences (requests) to join a batch mid-generation ... Here, continuous batching minimizes wasted slots in the inference compute pipeline. It eliminates the idle time caused by waiting for the longest response to finish for each batch. Instead of waiting for all sequences in a batch to finish (which is inefficient due to variable output lengths and leads to GPU underutilization), continuous batching replaces completed sequences with new ones at each iteration. This approach allows new requests to fill GPU slots immediately—resulting in higher throughput, reduced latency, and more efficient GPU utilization."
  },
  {
    "file_name": "aisp_1607.png",
    "caption": "Diagram illustrating NVIDIA's Transformer Engine using range analysis, scaling factors, and adaptive format conversion for precision transformation in a transformer layer.",
    "description": "Diagram illustrating NVIDIA's Transformer Engine using range analysis, scaling factors, and adaptive format conversion for precision transformation in a transformer layer.",
    "chapter": "Reducing Precision from FP16 to FP8 and FP4",
    "page_context": "NVIDIA’s TE automatically manages per-tensor scaling factors at these reduced precisions. At inference time, your inference server can load a model in FP16 but use FP8 matrix multiplies. ... The TE applies scaling to each tensor to maintain numerical stability using a scaling factor that is typically chosen one of two ways: a fixed, ahead-of-time calibration step using representative data during training, called static calibration—or a dynamically computed value that tracks the tensor’s maximum absolute value, called amax-based dynamic scaling. Figure 16-7 shows the TE’s using range analysis, scaling factor, and target format for the precision conversion. ... ###### Figure 16-7. NVIDIA Transformer Engine (TE) using range analysis, scaling factor, and target format for the precision conversion on a transformer layer ... For more compression, the FP4 format reduces model weight storage and traffic better than FP8 does. Accounting for scaling metadata and packing, the effective reduction is commonly around 1.8× compared with FP8 and about 3.5× compared with FP16. However, because FP4’s dynamic range is very limited, reliable inference at FP4 requires per-channel scaling—or other calibration such as NVIDIA’s per-block microscaling supported in the GPU’s TE. These techniques are needed to make FP4 usable for large networks by minimizing accuracy loss."
  },
  {
    "file_name": "aisp_1608.png",
    "caption": "Diagram illustrating a prefix cache for a system prompt using a trie data structure, showcasing the token sequence of \"You are ChatGPT, a friendly assistant designed to help users.\"",
    "description": "Diagram illustrating a prefix cache for a system prompt using a trie data structure, showcasing the token sequence of \"You are ChatGPT, a friendly assistant designed to help users.\"",
    "chapter": "Prefix Caching",
    "page_context": "The prefix cache is typically implemented as a token-sequence trie (pronounced “try”). A trie, often called a prefix tree, is a tree-based data structure in which each edge represents a single token and each node encodes the sequence of tokens from the root to that point. ... In a token-sequence trie, every observed prompt prefix is stored as its own path of tokens. This enables fast lookups of shared prefixes. When a new request arrives, the inference engine traverses the trie—token by token—from the root until it can no longer match the next token. The sequence of matching tokens lands at the node that completes the longest-cached prefix, as shown in Figure 16-8 for an example system prompt, “You are ChatGPT, a friendly assistant designed to help users…” ... ###### Figure 16-8. Prefix cache implemented as a trie data structure ... At this point, the system reuses the shared KV cache data (clones the pointers) and resumes decoding but only for the remaining tokens in the sequence. This avoids redundant self-attention computations for the already-cached prefix since the attention scores for this prefix have already been computed."
  },
  {
    "file_name": "aisp_1609.png",
    "caption": "Diagram illustrating the evolution of a prefix cache tree structure with nodes representing chat sessions and queries; green nodes are new, blue are accessed, and red are evicted under the least-recently-used policy.",
    "description": "Diagram illustrating the evolution of a prefix cache tree structure with nodes representing chat sessions and queries; green nodes are new, blue are accessed, and red are evicted under the least-recently-used policy.",
    "chapter": "cache each generated prefix as well",
    "page_context": "Knowing when to invalidate the cache is always a challenge. If the cache memory is needed for other things, you may need to evict some prefixes. Caching systems support different policies, such as “least recently used” (LRU), in which the least recently used prefix gets evicted first. SGLang’s RadixAttention, for instance, will lazily evict the least recently used radix-tree leaf when GPU memory is scarce, as shown in Figure 16-9. ... This is a prefix cache tree structure—and LRU eviction policy—used by SGLang for multiple incoming requests. This example is based on an awesome SGLang blog post from LMSys. ... ###### Figure 16-9. Prefix cache evolution for multiple requests (source: https://oreil.ly/7LBoC) ... Here, there are two chat sessions and multiple queries across those chat sessions. The label of each is a sequence of tokens (e.g., substring). Green nodes represent new nodes in the tree. Blue nodes are cached nodes that are currently being accessed. Red nodes have been evicted. Here is the breakdown of each step:"
  },
  {
    "file_name": "aisp_1610.png",
    "caption": "Diagram showing multiple user clicks triggering debouncing, with setTimeout delaying function execution to manage rapid inputs.",
    "description": "Diagram showing multiple user clicks triggering debouncing, with setTimeout delaying function execution to manage rapid inputs.",
    "chapter": "Debouncing and Request Coalescing",
    "page_context": "## Debouncing and Request Coalescing ... Many production systems also implement UX features called debouncing and request coalescing. By debouncing, or pausing, before responding, the system can recognize if a user sends multiple requests in quick succession—either by accident or from rage clicking, as shown in Figure 16-10. ... ###### Figure 16-10. Debouncing pauses a bit before performing an action ... In this case, the system can either coalesce the multiple queries into one query or drop all but the latest query. These types of application-level guardrails help reduce excessive, repeated, wasteful load on the backend."
  },
  {
    "file_name": "aisp_1701.png",
    "caption": "Diagram illustrating interference caused by colocated prefill and decode processes on the same GPU, showing wasted time during continuous batching for requests R1 through R4.",
    "description": "Diagram illustrating interference caused by colocated prefill and decode processes on the same GPU, showing wasted time during continuous batching for requests R1 through R4.",
    "chapter": "Why Prefill-Decode Disaggregation?",
    "page_context": "Under a simple FIFO scheduling strategy, long prompts can amplify tail latency for everyone. In general, long or compute-heavy prefills at the front of the queue will block shorter, lighter requests behind them. This is called head-of-line blocking, and it leads to poor utilization, latency outliers, and unhappy end users. ... In a flexible disaggregated architecture, it’s possible to send a large prompt prefill to the dedicated pool of compute-optimized prefill workers, while a lightweight prompt prefill can be sent to the decode workers directly—bypassing the prefill workers. This type of flexibility allows shorter tokens to not suffer from head-of-line blocking. This maximizes overall throughput and minimizes latency tail effects. ... ###### Figure 17-1. Interference caused by colocated prefill and decode running on the same GPU (source: https://oreil.ly/GRkHs) ... ## Advantages of Disaggregation"
  },
  {
    "file_name": "aisp_1702.png",
    "caption": "Diagram comparing colocated and disaggregated systems for prefill and decode, showing improved request rates per second in disaggregated setups, achieving higher goodput.",
    "description": "Diagram comparing colocated and disaggregated systems for prefill and decode, showing improved request rates per second in disaggregated setups, achieving higher goodput.",
    "chapter": "Reduced interference",
    "page_context": "With disaggregation, prefill tasks no longer contend with decode tasks on the same device. A busy decode worker, generating many tokens, won’t prevent another user’s prompt from being processed, and vice versa. ... Dedicated resources for each stage mean a long prompt’s computation won’t block another user’s token generation. In practice, this produces more predictable latency. Figure 17-2 shows the comparison between colocated and disaggregated prefill and decode. This experiment is described in more detail in the DistServe paper and subsequent blog post by the authors. ... ###### Figure 17-2. Comparison between colocated and disaggregated prefill and decode (source: https://oreil.ly/GRkHs) ... Here, the SLO is set to 0.4 seconds for P90 TTFT and 0.04 seconds for P90 TPOT (e.g., horizontal line in Figure 17-2). The colocated system can support only ~3 requests per second (RPS) of goodput within the given TTFT latency bounds. And it can sustain only 1.6 RPS within the given TPOT latency bounds. As such, the goodput of the colocated configuration is only 1.6 RPS since both the TTFT and TPOT latency SLOs need to be met."
  },
  {
    "file_name": "aisp_1703.png",
    "caption": "Diagram illustrating the workflow between decode worker, prefill queue, and prefill worker, detailing the process of managing prefill requests and KV block operations in a distributed system.",
    "description": "Diagram illustrating the workflow between decode worker, prefill queue, and prefill worker, detailing the process of managing prefill requests and KV block operations in a distributed system.",
    "chapter": "Decode workers design",
    "page_context": "If a request is initially routed to the decode worker, as in Figure 17-3, it must first decide if the prefill should be done locally or remotely using a disaggregated router. If it decides to prefill remotely, it will push the prefill request into a prefill queue to be picked up by the prefill worker. ... The prefill worker continuously pulls from the prefill queue, reads any KV blocks cached in the prefix cache, and computes the prefill operations. It then writes the KV blocks back to the decode worker, which completes the decoding. ... ###### Figure 17-3. Prefill worker design: read prefix cache → compute prefill → write KV cache for decode ... The decode worker design is focused on handling many concurrent sequence generations efficiently—as well as managing the memory footprint of the KV cache. In this section, we describe how decode servers achieve high throughput using techniques like continuous batching and clever memory management tricks. These help to reduce TPOT latency and increase scalability—especially for long sequences. Let’s start by detailing the KV cache transfer between the two stages."
  },
  {
    "file_name": "aisp_1704.png",
    "caption": "Diagram illustrating the sequence of interactions between decode workers and prefill workers for KV cache data transfer using NIXL, showcasing scheduling and remote memory operations.",
    "description": "Diagram illustrating the sequence of interactions between decode workers and prefill workers for KV cache data transfer using NIXL, showcasing scheduling and remote memory operations.",
    "chapter": "Decode workers design",
    "page_context": "High-performance disaggregation requires moving KV cache data as efficiently as possible between the prefill and decode workers. By using libraries like NIXL (described in Chapter 4) for direct GPU-to-GPU transfers, we can avoid CPU involvement and utilize nonblocking operations. This way, while one GPU is transferring KV data, it can also service other forward-pass requests without waiting for the transfer to complete. ... Consider a user request that arrives at a decode worker. In this case, the decode worker’s scheduler allocates the necessary KV blocks and adds a remote prefill request to the prefill queue. This prefill request contains the identifiers for those KV blocks. This interaction is shown in Figure 17-4. ... ###### Figure 17-4. Transfer of KV cache data between the prefill and decode workers using NIXL; coalesce multiple PagedAttention blocks into ~128-token payloads before RDMA (note: vLLM defaults to 16 tokens per block on CUDA) ... The prefill worker uses NIXL to perform direct remote GPU memory reads and writes over the selected transport. This avoids CPU copies and enables nonblocking progress. As soon as the prefill worker completes the prefill request, the decode worker’s scheduler adds a corresponding decode request to its own decode pipeline. This allows compute and data movement to overlap seamlessly. Make sure to use pre-registered peer memory with large pinned window sizes to minimize re-registration churn. You can verify zero-copy transfer overlap with the Nsight Systems timeline."
  },
  {
    "file_name": "aisp_1705.png",
    "caption": "Diagram illustrating how KV cache data is managed and reused between different requests, showing the process flow from pulling, encoding, and generating in the cache, stored across GPU, CPU, and SSD cache.",
    "description": "Diagram illustrating how KV cache data is managed and reused between different requests, showing the process flow from pulling, encoding, and generating in the cache, stored across GPU, CPU, and SSD cache.",
    "chapter": "Decode workers design",
    "page_context": "#### Memory management for the KV cache ... Because decoding involves attending to the entire sequence seen so far—including previously decoded tokens—KV cache memory is a critical resource for decode workers. Each sequence stores key and value tensors for each transformer layer—and each past token. Figure 17-5 shows an example KV cache being shared across different requests. ... ###### Figure 17-5. Managing and reusing KV cache data between requests ... For large models and long sequences, KV memory grows linearly with tokens and depends on attention layout and `dtype`. A practical estimate is `bytes_per_token = ​2⁠ x n_layers x n_kv_heads x head_dim x bytes_per_element`. (Note: the `2 x` accounts for keys and values per token per layer.)"
  },
  {
    "file_name": "aisp_1706.png",
    "caption": "Diagram illustrating KV-cache-aware request routing, showing the flow from chat/agent requests through analysis, tokenization, and cache matching, to selecting a worker node for optimized data processing.",
    "description": "Diagram illustrating KV-cache-aware request routing, showing the flow from chat/agent requests through analysis, tokenization, and cache matching, to selecting a worker node for optimized data processing.",
    "chapter": "Routing factors",
    "page_context": "Advanced routers like vLLM’s KV cache-aware router also consider cache locality. They will route a request to a decode worker that already holds some of its prefix in cache. Figure 17-6 shows how an example KV-cache-aware router moves a request through the system based on data received from KV cache events emitted by the workers. ... The goal is to route in a way that maximizes cache hits and balances load. Table 17-2 summarizes some key factors that influence the routing decision in a typical disaggregated design. ... ###### Figure 17-6. KV-cache-aware request routing based on data received from KV cache events emitted by the workers ... These factors will prefer to offload only requests that will benefit from the remote execution. These include long and compute-heavy prompts. Meanwhile, short and cache-hitting prompts are handled locally to minimize overhead. The thresholds are tunable. Each factor in Table 17-2 addresses a particular trade-off, as described next:"
  },
  {
    "file_name": "aisp_1707.png",
    "caption": "Diagram showing NVIDIA Dynamo GPU Planner reallocating GPUs from generation to context phase in response to increased summarization requests, optimizing system performance.",
    "description": "Diagram showing NVIDIA Dynamo GPU Planner reallocating GPUs from generation to context phase in response to increased summarization requests, optimizing system performance.",
    "chapter": "Capacity-aware routing",
    "page_context": "### Capacity-aware routing ... As mentioned in the previous section, NVIDIA Dynamo supports dynamic routing policy. An implementation of this dynamic routing capability is Dynamo GPU Planner. The planner uses metrics like TTFT, TPOT, and the estimated cost of KV cache transfer to decide to modify routing—or even reallocate/scale phase-specific GPUs—to reduce bottlenecks and adapt to shifts in the workload. This allows the system to maintain high performance during heavy surges in demand, as shown in Figure 17-7. ... ###### Figure 17-7. NVIDIA’s Dynamo GPU Planner decides how to handle incoming requests and allocate GPU workers to prefill and decode based on GPU utilization metrics ... Here, Dynamo’s Planner chooses to shift more GPUs to the prefill (context) phase because of an influx of large summarization prompts that require a heavy amount of prefill (large inputs) relative to decode (small summary outputs)."
  },
  {
    "file_name": "aisp_1801.png",
    "caption": "Chart comparing the arithmetic intensity and computational performance of different attention implementations on NVIDIA H100 architecture; MLA approaches the computational roof, showing its efficiency in maximizing GPU utilization.",
    "description": "Chart comparing the arithmetic intensity and computational performance of different attention implementations on NVIDIA H100 architecture; MLA approaches the computational roof, showing its efficiency in maximizing GPU utilization.",
    "chapter": "FlashMLA (DeepSeek)",
    "page_context": "FlashMLA (decode) is to inference what FlashAttention (prefill) is to training. It reduces memory access overhead and latency. With FlashMLA, you can achieve large latency reductions for the decode phase compared to standard kernels. ... FlashMLA increases arithmetic intensity by fusing multiple attention operations into one. This way, it can process multiple heads and multiple time steps in one fused kernel launch. This increases GPU utilization during the decode by keeping the math units busy despite small batch sizes. Figure 18-1 shows the improvement in arithmetic intensity for MLA compared to other attention implementations like grouped-query attention (GQA) and multiquery attention (MQA) on a Hopper H100 GPU. (Note: Blackwell shifts both rooflines upward with higher TFLOPs and HBM bandwidth.) ... ###### Figure 18-1. MLA approaches the compute-bound regime (measured on the NVIDIA Hopper H100 architecture) ... The introduction of FlashMLA was significant because it showed that the decode phase’s bottlenecks, memory bandwidth, and kernel-launch overhead can be reduced—even on suboptimal GPU hardware. It reduced the number of separate GPU kernel launches and optimized memory access patterns—squeezing as much performance out of constrained hardware as possible for decoding tasks."
  },
  {
    "file_name": "aisp_1802.png",
    "caption": "Diagram showing three variable-length sequences consolidated into a single nested jagged tensor with specified offsets, demonstrating FlexAttention's support for ragged batching.",
    "description": "Diagram showing three variable-length sequences consolidated into a single nested jagged tensor with specified offsets, demonstrating FlexAttention's support for ragged batching.",
    "chapter": "FlexDecoding (PyTorch)",
    "page_context": "Prefer `torch.compile(mode=\"max-autotune\")` for stable, latency-critical decode once recompilations are under control. Keep the capture boundary narrow (per-layer or attention block) to reduce graph invalidations from ragged batching. Prefer Transformer Engine FP8 (MXFP8) for prefill and decode. Consider FP4 (NVFP4) when accuracy permits and performance increases. As of this writing, FP4 support is still maturing and can underperform 8-bit and 16-bit formats in the near-term. Continue to set `torch.set_float32_matmul_precision(\"high\")` to enable TF32 fallback on remaining FP32 ops. FlexAttention’s decode backend supports common performance enhancements including grouped-query attention (GQA) and PagedAttention. ... A key feature of FlexAttention and FlexDecoding includes support for nested jagged-layout tensors (NJT). These allow ragged batching of variable-length sequences (common in LLM workloads) during decoding. A jagged tensor representation of various sequences is shown in Figure 18-2. ... ###### Figure 18-2. Ragged batch as a nested jagged tensor (offsets); three sequences (top) represented as a single nested jagged tensor representation with offsets (bottom); prefer PyTorch NJT for decode-time batching ... Additionally, FlexDecoding supports bias terms and integrates with PagedAttention by using a block mask conversion interface that maps logical blocks to the physical cache layout. This scatters logical KV blocks into the physical cache layout—without creating extra copies, as shown in Figure 18-3."
  },
  {
    "file_name": "aisp_1803.png",
    "caption": "Diagram illustrating the mapping of logical KV blocks from sequences A and B to the same physical KV cache blocks for optimal cache reuse.",
    "description": "Diagram illustrating the mapping of logical KV blocks from sequences A and B to the same physical KV cache blocks for optimal cache reuse.",
    "chapter": "FlexDecoding (PyTorch)",
    "page_context": "FlexDecoding leverages captured tensors to vary certain mask or bias values during each iteration—without requiring a recompile. And it integrates with PagedAttention. To use a global KV cache such as vLLM LMCache, map the cache’s page table to FlexAttention’s BlockMask. This will translate logical KV pages into physical memory addresses on the fly. ... With FlexDecoding, developers have full Python-level flexibility for custom attention sparsity patterns. This is particularly useful for MoE model inference. FlexDecoding allows you to achieve near-optimal performance without requiring you to write any custom CUDA kernels. Essentially, it allows arbitrary attention patterns to be optimized similarly to dense attention patterns. This becomes even more valuable as new inference techniques emerge. ... ###### Figure 18-3. PagedAttention scatters logical KV blocks into physical KV blocks for optimal cache reuse between sequences; align block sizes with LMCache page size—larger pages (e.g., 64–128 tokens) reduce RDMA overhead in disaggregated setups ... Many of these capabilities, such as fused attention for decoding and support for PyTorch’s nested jagged tensors (NJT) batching, are available in the core PyTorch library. This makes custom fusion less necessary for typical patterns."
  },
  {
    "file_name": "aisp_1804.png",
    "caption": "Diagram illustrating a disaggregated KV cache pool, showing how KV tensors are stored and managed across distributed computing nodes during prefill and decoding stages.",
    "description": "Diagram illustrating a disaggregated KV cache pool, showing how KV tensors are stored and managed across distributed computing nodes during prefill and decoding stages.",
    "chapter": "Disaggregated KV Cache Pool",
    "page_context": "The pool can also offload to CPU memory, including the unified CPU and GPU memory of the Grace Blackwell and Vera Rubin platforms. It can also offload to persistent storage like NVMe SSDs. ... Using a disaggregated KV cache pool, when a prefill computes the KV tensors for a prompt—or when a decode extends the KV tensors—the KV blocks are stored in a distributed manner across many compute nodes. This is shown in Figure 18-4, adapted from work on disaggregated KV pools. ... ###### Figure 18-4. Disaggregated (distributed) KV cache pool (source: https://oreil.ly/2xtK-) ... Consider a very long 250,000-token context (e.g., a chat session with many turns) using a 70 billion-parameter transformer model with 80 layers and 32 heads in which each head is dimension 128. This generates a huge KV cache footprint per token."
  },
  {
    "file_name": "aisp_1805.png",
    "caption": "Diagram illustrating SM-aware thread-block scheduling, showing how CTAs dynamically assign prefill and decode tasks across SMs to minimize memory movement.",
    "description": "Diagram illustrating SM-aware thread-block scheduling, showing how CTAs dynamically assign prefill and decode tasks across SMs to minimize memory movement.",
    "chapter": "Optimized KV Cache Memory Layout",
    "page_context": "This eviction/compression technique is safe when the model uses a sliding-window or other restricted-attention pattern. However, it should not be applied to layers that retain full attention over the full content window (or retrieval hooks) without careful evaluation. ... Another technique called POD-Attention similarly reorganizes attention computation to reduce HBM traffic. Specifically, it uses SM-aware thread-block (or cooperative thread array [CTA]) scheduling. This implements runtime operation binding to dynamically assign each CTA running on an SM to either perform a prefill or decode task. This is shown in Figure 18-5. ... ###### Figure 18-5. SM-aware thread-block (CTA) scheduling to match prefill tasks with decode tasks on SMs to minimize memory movement ... So rather than statically launching separate kernels for each phase, a single kernel launches enough CTAs to cover both workloads. At runtime, each CTA inspects which SM it’s on and uses per-SM counters to decide which operation (prefill or decode) should be run based on what else is running on that SM."
  },
  {
    "file_name": "aisp_1806.png",
    "caption": "Diagram illustrating the request lifecycle in NVIDIA Dynamo, showcasing the flow from HTTP client discovery to prefill processing and response delivery within the system architecture.",
    "description": "Diagram illustrating the request lifecycle in NVIDIA Dynamo, showcasing the flow from HTTP client discovery to prefill processing and response delivery within the system architecture.",
    "chapter": "Connector and Data Path Design",
    "page_context": "Building on the zero-copy optimization, let’s see how the prefill and decode nodes coordinate the transfer end to end—beyond just moving the bits. The prefill and decode workers often communicate using a scheduler or router. In practice, this scheduler is often implemented as a centralized component, as used in NVIDIA Dynamo, or a decentralized coordination approach, as used by SGLang. ... For instance, NVIDIA Dynamo implements a global scheduling queue in which the decode workers push new prompt tasks into a queue that prefill workers consume. In this design, a decode node enqueues a request for prompt processing, as shown in the “Put RemovePrefillRequest” (step 6) in Figure 18-6. ... ###### Figure 18-6. Request lifecycle in NVIDIA Dynamo; decode pulls prompts and prefill pushes KV using NIXL ... A prefill node picks up this request and, when done, knows exactly which decode node to send the results to since the request carries an origin or reply-to ID for the decode node. The KV is then transferred directly to that decode worker’s GPU using NIXL RDMA."
  },
  {
    "file_name": "aisp_1807.png",
    "caption": "Diagram comparing tensor parallelism and pipeline parallelism strategies for prefill and decode stages, highlighting the differences in all-reduce overhead and batching efficiency.",
    "description": "Diagram comparing tensor parallelism and pipeline parallelism strategies for prefill and decode stages, highlighting the differences in all-reduce overhead and batching efficiency.",
    "chapter": "Phase-specific model parallelism",
    "page_context": "The decode phase, on the other hand, is sequential and latency-sensitive per step. Using too many GPUs for one decode can actually hurt time-per-output-token (TPOT) latency, also called inter-token latency (ITL). This is because each token step would require additional multi-GPU communication overhead. As such, the potential for speedup is limited since there’s only one token’s worth of compute to split at a time (or a few tokens, if using speculative decoding). ... Disaggregation makes it possible to mix these approaches and use TP for one phase and PP for another—or use different degrees of each technique. For instance, you can run prefill with `TP=8` to span 8 GPUs and minimize prompt latency. You can then run decode with `TP=1`, or a single GPU, to maximize per-token throughput and minimize step latency. In this way, each phase’s throughput and latency can be tuned separately, as shown in [Figure 18-7](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch18.html#ch18_figure_7_1757308075827813). ... ###### Figure 18-7. Using different parallelism strategies for prefill and decode (source: https://oreil.ly/1-Ti0) ... Here, tensor parallelism’s additional all-reduce communication overhead is more prominent during the prefill stage since a large number of tokens are being processed in parallel. As such, we choose pipeline parallelism because it’s more efficient for our prefill workload."
  },
  {
    "file_name": "aisp_1808.png",
    "caption": "Diagram illustrating instance load management using early rejection strategies to optimize decoding and prefill request loads over time.",
    "description": "Diagram illustrating instance load management using early rejection strategies to optimize decoding and prefill request loads over time.",
    "chapter": "Early Rejection (Admission Control)",
    "page_context": "By doing so, you prevent a situation where the request sits in the queue and then takes so long that it surpasses its latency SLO and slows down all other requests by consuming scarce compute or memory bandwidth resources. This reinforces the fact that it’s better to return a quick “too busy” response than to silently accept and then fail the latency guarantee. ... This is analogous to how web servers shed load under extreme overload to keep serving the remaining requests with acceptable latency—the dreaded HTTP 503 error. This is better than timing out all requests. ... ###### Figure 18-8. Instance load when applying early rejection (source: https://oreil.ly/2xtK-) ... In LLM serving, because request sizes and durations vary widely, having a clear admission control step helps maintain performance for accepted requests. A well-behaved admission control keeps the system in a regime where it can meet both TTFT and TPOT targets for the load that it has accepted."
  },
  {
    "file_name": "aisp_1809.png",
    "caption": "Diagram comparing the execution timeline and architecture of existing systems to TetriInfer's work, illustrating improved load balancing and resource management through a two-level scheduler.",
    "description": "Diagram comparing the execution timeline and architecture of existing systems to TetriInfer's work, illustrating improved load balancing and resource management through a two-level scheduler.",
    "chapter": "TetriInfer’s two-level scheduler",
    "page_context": "Here, you see that by predicting resource usage through queue lengths, GPU utilization trends, etc., TetriInfer’s two-level scheduler smooths out the load across the cluster and prevents any single node from being overloaded. ... The name TetriInfer hints at “packing” requests like Tetris pieces to fill GPU time without interference. ... ###### Figure 18-9. Comparison between existing systems and TetriInfer’s architecture (source: https://oreil.ly/_3KGj) ... ### Arrow’s adaptive instance scaling"
  },
  {
    "file_name": "aisp_1810.png",
    "caption": "Diagram illustrating Arrow architecture, showing components like TTFT predictor, global scheduler, elastic instance, and local scheduler, detailing the dynamic allocation of prefill and decode tasks within the system.",
    "description": "Diagram illustrating Arrow architecture, showing components like TTFT predictor, global scheduler, elastic instance, and local scheduler, detailing the dynamic allocation of prefill and decode tasks within the system.",
    "chapter": "Arrow’s adaptive instance scaling",
    "page_context": "Another technique for dynamic resource scheduling is Arrow (not to be confused with the popular Arrow data format). This is an adaptive instance scaling technique that leverages the fact that disaggregated systems often have a lagging response to workload changes. For instance, if the distribution of input versus output changes, the static number of prefill versus decode workers doesn’t immediately adjust. This causes a temporary loss of goodput since one side becomes a bottleneck. ... Arrow continuously analyzes the workload by measuring input token rate versus output token rate—and the backlog in each worker pool in the cluster. It then dynamically adjusts the allocation of workers, as shown in Figure 18-10. ... ###### Figure 18-10. Arrow architecture ... In a cloud environment, this could mean spinning up additional decode instances when output load increases. You can also scale down decode in favor of more prefill instances if an input-heavy workload is detected."
  },
  {
    "file_name": "aisp_1901.png",
    "caption": "Diagram illustrating pipeline parallelism with devices processing different stages over time, highlighting sequential delays referred to as pipeline bubbles.",
    "description": "Diagram illustrating pipeline parallelism with devices processing different stages over time, highlighting sequential delays referred to as pipeline bubbles.",
    "chapter": "Adaptive Parallelism Strategies (TP Versus PP Versus Hybrid)",
    "page_context": "Pipeline parallelism (PP) is another form of model parallelism that splits the model as well. But instead of splitting individual model layers and matrices, it assigns whole layers to different GPUs to overcome memory limits—assuming the layers fit into a single GPU. PP incurs additional overhead in the form of sequential stage delays. These are called pipeline bubbles, as shown in Figure 19-1. ... Expert parallelism, used in mixture-of-experts (MoE) model architectures, assigns each expert subnetwork its own GPU. A lightweight gating network then directs each input request or token to only the top-k active experts identified by the router. In this case, each GPU processes just the subset of experts that it hosts. ... ###### Figure 19-1. Pipeline bubbles caused by PP ... By activating only a few experts per input, expert parallelism reduces per-device memory, inference time, and compute costs for models with a large number of experts, often called wide expert models. The conditional, router-based expert compute pattern scales efficiently as you add more experts. For instance, DeepSeek-R1 has 256 total experts, but only the top 9 experts (including 1 shared expert) are chosen by the router during inference."
  },
  {
    "file_name": "aisp_1902.png",
    "caption": "Diagram showing two hybrid-sharding strategies across eight GPUs: one with Tensor Parallelism (TP)=4 and Pipeline Parallelism (PP)=1, and another with TP=2 and PP=1.",
    "description": "Diagram showing two hybrid-sharding strategies across eight GPUs: one with Tensor Parallelism (TP)=4 and Pipeline Parallelism (PP)=1, and another with TP=2 and PP=1.",
    "chapter": "Adaptive Parallelism Strategies (TP Versus PP Versus Hybrid)",
    "page_context": "Meanwhile, short latency-sensitive requests would route to a TP-only model instance to avoid pipeline-stage overhead. To support this, your serving engine maintains multiple presharded model instances, each optimized for different workload profiles, and dynamically dispatches incoming queries to the model instance whose parallelism strategy best satisfies the job’s SLOs. ... You can also use a different number of shards. This is shown in Figure 19-2, which uses two different numbers of TP shards in two different hybrid TP + PP parallelism configurations across eight GPUs. ... ###### Figure 19-2. Preprovisioning two different hybrid-sharding pools (TP = 4, PP = 1 and TP = 2, PP = 1) for a given model across eight GPUs ... Using PP on the fly for long sequence inputs helps to avoid OOM errors caused by the large input sequence. Conversely, for short prompts and latency-sensitive queries, the system can instead route to a tensor-parallel model instance optimized for low latency. In this case, the request avoids the overhead of PP."
  },
  {
    "file_name": "aisp_1903.png",
    "caption": "Diagram illustrating speculative KV prefetching with SpeCache, showing data flow between LLMs and CPU RAM to optimize token generation, including cache hits, misses, and speculative tokens.",
    "description": "Diagram illustrating speculative KV prefetching with SpeCache, showing data flow between LLMs and CPU RAM to optimize token generation, including cache hits, misses, and speculative tokens.",
    "chapter": "Speculative KV Prefetching for Faster TTFT",
    "page_context": "On each subsequent step, the model decodes two tokens in parallel, including the actual output token and the speculative token. Both results are fed into the next step, and, before each step, the top-k most relevant 16-bit KV pairs for the speculative path are prefetched. This way, both paths have their required KV cache data ready. In short, SpeCache reports TTFT improvements by prefetching reduced-precision KV and overlapping with compute. ... Integrate speculative prefetch techniques only after validating your access patterns and storage tiers. ... ###### Figure 19-3. Speculative decoding with SpeCache (source: https://oreil.ly/b21E5) ... The KV cache can be extremely large due to the number of layers in modern LLMs, the increasing size of the LLM context window (effectively limitless at this point), and the large amount of reasoning chains generated by modern “thinking” models. Modern inference systems will often swap the KV cache between GPU, CPU memory, and SSD to better manage capacity—especially for extremely long contexts, which don’t fit in GPU memory."
  },
  {
    "file_name": "aisp_1904.png",
    "caption": "Diagram illustrating different KV cache algorithms, including full cache and selected cache strategies with varying fixed and dynamic configurations, highlighting approaches like Dense, Streaming LLM, SnapKV, and PyramidKV.",
    "description": "Diagram illustrating different KV cache algorithms, including full cache and selected cache strategies with varying fixed and dynamic configurations, highlighting approaches like Dense, Streaming LLM, SnapKV, and PyramidKV.",
    "chapter": "Real-Time KV Cache Compression and Policy Switching",
    "page_context": "KV cache is a good candidate for compression/quantization. Like any form of compression, KV cache compression reduces its memory footprint. Doing this in real time means performing compression on the fly during inference. ... Policy switching means that the compression strategy can change based on the current context. The goal is to free up memory and network bandwidth when needed—without impacting model accuracy or slowing down computations that involve the KV cache data. Figure 19-4 shows a few different types of KV cache compression algorithms. ... ###### Figure 19-4. Different KV cache algorithms, including no caching (e.g., dense) ... A straightforward and simple approach to KV compression is to just reduce its precision. Many frameworks default to FP16 or BF16 for KV cache since 16-bit is typically what the model uses for activations. However, one can often compress keys and values to 8-bit or even 4-bit with minimal impact on output quality—especially for tokens at the end of the LLM’s context."
  },
  {
    "file_name": "aisp_1905.png",
    "caption": "Diagram comparing vanilla attention and sliding window attention, illustrating how the latter focuses on recent tokens using a windowed approach for efficient context management.",
    "description": "Diagram comparing vanilla attention and sliding window attention, illustrating how the latter focuses on recent tokens using a windowed approach for efficient context management.",
    "chapter": "Example usage. Replace with a model that supports your hardware.",
    "page_context": "Having this flexibility is useful if, for instance, your service sometimes prioritizes maximum quality (no compression) for premium users versus maximum throughput (heavy compression) for free-tier users. The policy can switch based on request metadata as well, including the user’s subscription type. ... No discussion on caching is complete without considering eviction strategies, such as Least Recently Used (LRU) eviction. If context length becomes too long, some model architectures—like those with recency bias or sliding-window attention—might choose to discard or downsample very old tokens entirely. Sliding-window attention is shown in Figure 19-5. ... ###### Figure 19-5. Sliding-window attention uses the intuition that the most useful tokens are the most recent ... While LRU eviction of earlier tokens from the context is not exactly compression, it’s yet another type of policy that can be dynamically chosen at runtime. For instance, the system can decide that, beyond 2,048 tokens, the model likely won’t need the earlier tokens—based on some heuristics or a smaller LLM."
  },
  {
    "file_name": "aisp_1906.png",
    "caption": "Diagram illustrating a slab allocator managing memory objects, showing full, partially full, and free slabs for different object sizes.",
    "description": "Diagram illustrating a slab allocator managing memory objects, showing full, partially full, and free slabs for different object sizes.",
    "chapter": "Dynamic Memory-Allocation Switching (Slab Versus Caching Versus Stream-Ordered)",
    "page_context": "By default, PyTorch uses a variant[]()[]() of the buddy/best-fit memory allocator called <em>best-fit with coalescing</em>, or BFC. It grabs big chunks of GPU memory and subdivides the chunks to satisfy allocation requests. This reuses free space and avoids frequent calls to the relatively slow and synchronous `cudaMalloc` and `cudaFree`. ... A buddy allocator splits memory into blocks whose sizes are powers of two. A slab allocator works on top of the buddy system to efficiently manage small, fixed-size objects. It preallocates slabs, or collections of objects of a given type, and maintains a free list within each slab, as shown in Figure 19-6. ... ###### Figure 19-6. Slab allocator maintains a free list of memory objects within each preallocated slab ... A slab allocator allows fast reuse without fragmentation. A buddy allocator handles coarse-grained page allocation, while a slab allocator optimizes fine-grained object reuse."
  },
  {
    "file_name": "aisp_1907.png",
    "caption": "Diagram illustrating chunked prefill, showing how prefill (green) and decode tasks (blue) are scheduled efficiently over time across four requests, reducing processing gaps.",
    "description": "Diagram illustrating chunked prefill, showing how prefill (green) and decode tasks (blue) are scheduled efficiently over time across four requests, reducing processing gaps.",
    "chapter": "Adaptive Batching and Chunked Prefill Scheduling",
    "page_context": "You can interleave large prefill requests with latency-sensitive decode tasks by slicing the prefill into small chunks and piggybacking decodes between them. This keeps all pipeline stages busy and minimizes idle “bubbles” in your GPU schedule. ... Chunked prefill is a well-supported pattern used by all modern LLM inference engines to reduce pipeline bubbles. It effectively time-slices a big task (prefill) to create room for small tasks (decode) to execute in the pipeline gaps created by the chunks, as shown in Figure 19-7. ... ###### Figure 19-7. Benefits of chunked prefills for decode-maximal batching across four requests ... The SARATHI paper demonstrated that this type of chunked prefill and piggybacking can help you find the right level of decode-maximal batching, reduce bubbles, and improve throughput by ~1.3–1.9× compared to naive scheduling. The name SARATHI is a reference to a charioteer that intelligently steers both prefill and decode tasks together. Fun!"
  },
  {
    "file_name": "aisp_1908.png",
    "caption": "Diagram illustrating topology-aware routing to optimize inter-GPU communication and avoid bottlenecks in a multinode cluster setup.",
    "description": "Diagram illustrating topology-aware routing to optimize inter-GPU communication and avoid bottlenecks in a multinode cluster setup.",
    "chapter": "Congestion-Aware and Topology-Aware Scheduling with Multiple GPUs",
    "page_context": "Modern multi-GPU and multirack systems like Grace Blackwell GB200 NVL72 systems (72 Blackwell B200 GPUs with 180 GB HBM each) and the newer Grace Blackwell Ultra GB300 NVL72 (72 B300 GPUs with 288 GB each) connect 72 GPUs in a single high-bandwidth NVLink/NVSwitch fabric. These architectures create a unified 72-GPU domain and give each GPU up to ~1.8 TB/s of aggregate bidirectional NVLink throughput. This provides over 130 TB/s of aggregate cross-sectional bandwidth across the NVSwitch network. ... However, achieving peak performance for large-scale inference requires more than raw bandwidth. It needs intelligent and adaptive communication scheduling. Congestion-aware and topology-aware strategies make sure that data transfers avoid bottlenecks in real time, as shown in Figure 19-8. ... ###### Figure 19-8. Topology-aware routing to avoid bottlenecks across GPUs and multinode clusters ... To address these bottlenecks, let’s consider link utilization telemetry, dynamic message routing, and scheduling waves of collectives. Next are some key principles and techniques that enable efficient scheduling of inter-GPU communication while maintaining low latency and high throughput. To keep things concrete, we’ll do this in the context of an NVL72 rack environment."
  },
  {
    "file_name": "aisp_1909.png",
    "caption": "Diagram comparing default and NVTAGS process-to-GPU mapping, highlighting the efficiency of faster communication channels with NVTAGS.",
    "description": "Diagram comparing default and NVTAGS process-to-GPU mapping, highlighting the efficiency of faster communication channels with NVTAGS.",
    "chapter": "Adaptive Process-GPU Mapping",
    "page_context": "This is essentially a process-GPU placement-optimization problem, which requires mapping the graph of neural-network model layers onto GPU hardware that incurs the minimum amount of communication cost. If the original assignment of layers/processes to GPUs is naive, these tensors might travel over long, expensive, congested paths. This could include multiple NVSwitch hops—or even off the NVLink fabric entirely onto another rack or data center. This will definitely reduce throughput and overall performance. ... With adaptive process-GPU mapping, the system dynamically assigns processes to GPUs such that communication is kept as local (and balanced) as possible. For instance, consider our LLM layers (processes) partitioned across many GPUs in an NVL72 rack. If layer/process 0 on GPU 0 feeds layer/process 2 on GPU 2, but their GPUs are on opposite ends of the NVSwitch network, the data has to traverse more links. In this case, moving layer/process 2 to GPU 1 is the preferred process-GPU mapping, as shown in Figure 19-9, in the context of NVIDIA’s Topology-Aware GPU Selection (NVTAGS) system. ... ###### Figure 19-9. NVIDIA’s Topology-Aware GPU Selection (NVTAGS) process-to-GPU mapping ... Here, NVTAGS automatically assigns GPU affinity to processes based on the communication patterns between the GPUs. NVTAGS is a topology-aware GPU selection framework from NVIDIA that automates process-to-GPU mapping using fabric distances and link metrics. It actively profiles the topology and reassigns processes to GPUs with the fastest mutual links."
  },
  {
    "file_name": "aisp_1910.png",
    "caption": "Diagram illustrating distributed GEMM using multiple GPUs with NCCL all-reduce across NVLink, showing overlapping communication and computation stages.",
    "description": "Diagram illustrating distributed GEMM using multiple GPUs with NCCL all-reduce across NVLink, showing overlapping communication and computation stages.",
    "chapter": "Optimizing Collective Communication with NCCL",
    "page_context": "NVIDIA’s Collective Communications Library (NCCL) is the standard library managing these GPU collectives. It offers multiple algorithms and optimizations for multiple GPU environments. ... Many inference workloads involve collective communication patterns, such as gathering outputs from multiple experts, broadcasting parameters, or performing all-reduce operations, as shown in Figure 19-10. Here, NCCL communication (stream 1) overlaps with GEMM computations (stream 0). ... ###### Figure 19-10. Distributed GEMM using multiple GPUs and all-reduce across NVLink ... These NCCL optimizations can be applied dynamically by the scheduler in congestion-aware environments. By choosing the right collective algorithm and tuning it using the topology and congestion information, we can reduce communication overhead of our inference system. Next are some key considerations that the scheduler can use when tuning NCCL on the fly."
  },
  {
    "file_name": "aisp_1911.png",
    "caption": "Diagram comparing tree-based (chain) communication with ring-based communication among GPUs, highlighting different data paths and connections.",
    "description": "Diagram comparing tree-based (chain) communication with ring-based communication among GPUs, highlighting different data paths and connections.",
    "chapter": "Ring versus tree all-reduce",
    "page_context": "The ring approach maximizes bandwidth utilization on NVLink/NVSwitch by keeping all links busy, but it means the latency scales linearly with the number of GPUs. For instance, on a 72-GPU ring, the data makes 71 hops to complete one reduction. ... A tree algorithm, in contrast, reduces or broadcasts data in a logarithmic fashion since GPUs are organized into a logical binary tree where each step halves the number of participants. However, the GPUs are physically connected linearly, link-by-link, into what can be logically considered a tree-chain. Figure 19-11 compares tree-based and ring-based communication among GPUs. ... ###### Figure 19-11. Tree-based (chain) versus ring-based communication ... In practice, in a tree all-reduce, the GPUs first connect in a simple NVLink/NVSwitch “chain” within each node. Across nodes, they connect in a pipelined, dual-binary-tree topology. This is what allows the O(log N) latency of a tree-based algorithm."
  },
  {
    "file_name": "aisp_1912.png",
    "caption": "Diagram comparing data transfer paths with and without GPUDirect RDMA, highlighting more direct GPU memory access in GPUDirect RDMA scenarios.",
    "description": "Diagram comparing data transfer paths with and without GPUDirect RDMA, highlighting more direct GPU memory access in GPUDirect RDMA scenarios.",
    "chapter": "Multinode and Multirack Communication with GPUDirect RDMA",
    "page_context": "When scaling beyond a single node (e.g., NVL72 rack), additional challenges will start to surface since communication is traveling over relatively slow network interfaces, such as InfiniBand and Ethernet. In this case, NVLink and NVSwitch no longer directly connect all of the GPUs in the system. Instead, GPUs in different nodes exchange data using NICs and network switches. ... To maintain high performance in a multinode and multirack environment, modern AI systems use GPUDirect RDMA. As covered in Chapter 4, GPUDirect RDMA allows GPUs to directly send/receive data with remote GPUs’ memory—and without host CPU involvement, as shown in Figure 19-12. ... ###### Figure 19-12. Direct GPU-to-GPU memory transfers with GPUDirect RDMA—and without involving the host CPU memory (source: https://oreil.ly/445a9) ... Even with RDMA efficiency, however, network bandwidth is still lower and latency is still higher than intranode NVLink. As such, network congestion in the cluster fabric will become the limiting factor without a dynamic and adaptive routing schedule. A congestion-aware scheduler can intelligently route and balance internode traffic in addition to intranode traffic, as we discussed earlier."
  },
  {
    "file_name": "aisp_1913.png",
    "caption": "Diagram illustrating network-level packet routing strategies to avoid congestion, featuring routing paths and control signal exchanges between nodes labeled A to F, supported by queue information and a forwarding table.",
    "description": "Diagram illustrating network-level packet routing strategies to avoid congestion, featuring routing paths and control signal exchanges between nodes labeled A to F, supported by queue information and a forwarding table.",
    "chapter": "Dynamic Congestion-Aware Scheduling",
    "page_context": "While all the techniques we’ve mentioned can be configured at system startup or design time, the most robust and advanced systems use dynamic scheduling to respond to congestion as it happens. Dynamic congestion-aware scheduling means the system continuously monitors network conditions using the telemetry discussed earlier—and adjusts the scheduling of tasks or communications in real time. ... Congestion-aware scheduling and routing helps reduce bottlenecks and maintains high performance under dynamic conditions. This is analogous to network-level dynamic packet routing, as shown in Figure 19-13. ... ###### Figure 19-13. Network-level packet routing to avoid congestion ... In a multi-GPU inference context, dynamic strategies include throttling, rerouting, or reordering operations based on congestion feedback. For instance, suppose the scheduler detects that NVLink link 0, connecting two particular GPUs, is currently maxed out because it’s transferring data for a massive tensor during a large pipeline-parallel activation transfer, for instance."
  },
  {
    "file_name": "aisp_1914.png",
    "caption": "Diagram showing the connection of 72 GPUs in an NVL72 rack to NVSwitches, illustrating data transfer to a centralized parameter server and potential congestion points.",
    "description": "Diagram showing the connection of 72 GPUs in an NVL72 rack to NVSwitches, illustrating data transfer to a centralized parameter server and potential congestion points.",
    "chapter": "Coordinating NVSwitch Transfers with Fine-Tuned Scheduling",
    "page_context": "A proven technique is staggering communication waves. This is related to the wave scheduling strategy mentioned earlier for collectives, but it applies more broadly to any overlapping transfers. ... Consider all 72 GPUs in a NVL72 rack needing to send data to a specific peer, such as a central parameter server in which GPU 0 collects all the results from all 72 GPUs. If all 71 other GPUs send their data at the exact same time, GPU 0’s 18 NVLink links—and the NVSwitch that connects them—will experience a huge burst of 71 inputs, as shown in Figure 19-14. This will exceed the amount of bandwidth that can be delivered at that moment. ... ###### Figure 19-14. All 72 GPUs sending data to a centralized parameter server ... In this case, NVSwitch will need to buffer and serialize many of those transfers. This leads to latency spikes. Instead, a coordinated and optimized approach is to partition the senders into four groups: group 1 (GPUs 1–18) sends first, then a few microseconds later group 2 (GPUs 19–36) sends, and so on."
  },
  {
    "file_name": "aisp_2001.png",
    "caption": "Diagram illustrating a reinforcement learning process that uses interleaved code execution to optimize GPU kernel code, involving a policy model, code sandbox, and a flow of advantage and reward feedback.",
    "description": "Diagram illustrating a reinforcement learning process that uses interleaved code execution to optimize GPU kernel code, involving a policy model, code sandbox, and a flow of advantage and reward feedback.",
    "chapter": "Chapter 20. AI-Assisted Performance Optimizations and Scaling Toward Multimillion GPU Clusters",
    "page_context": "In a broader context, these examples demonstrate that algorithmic innovations, even in core operations, such as matrix multiplication, can produce performance gains similar to those achieved by acquiring new hardware. At a high level, consider a workflow that uses reward feedback from a series of reinforcement learning rollouts (e.g., iterations). This can help find the most optimal GPU kernel code for your environment, as shown in Figure 20-1. ... These AI-assisted approaches can help improve performance, reduce training time, and lower operating costs. They can also enable the efficient deployment of larger models on smaller systems, which will unlock future advances in AI. In other words, this is AI helping to create better AI. We love it! ... ###### Figure 20-1. Using reinforcement learning to find the most optimal GPU kernel code for your environment ... # AlphaTensor AI-Discovered Algorithms Boosting GPU Performance (Google DeepMind)"
  },
  {
    "file_name": "aisp_2002.png",
    "caption": "Diagram illustrating Strassen’s subquadratic algorithm for multiplying 2 × 2 matrices, showing matrix components involved in the computations.",
    "description": "Diagram illustrating Strassen’s subquadratic algorithm for multiplying 2 × 2 matrices, showing matrix components involved in the computations.",
    "chapter": "AlphaTensor AI-Discovered Algorithms Boosting GPU Performance (Google DeepMind)",
    "page_context": "The astonishing result was that it found formulas for multiplying matrices that proved better than any human-derived method in existence at the time. For instance, it rediscovered Strassen’s famous subquadratic algorithm for 2 × 2 matrices, as shown in Figure 20-2, but also improved it for larger matrix sizes. ... But the real proof came when those algorithms were tested on actual hardware. AlphaTensor discovered a method specific to the NVIDIA Volta V100 GPU generation, which multiplied large matrices 10%–20% faster than the standard NVIDIA V100-era cuBLAS library could at the time. A 10%–20% speedup in GEMM performance is huge. It’s like gaining an extra 10%–20% in free compute for every model’s forward and backward pass. ... ###### Figure 20-2. Strassen’s subquadratic algorithm for multiplying 2 × 2 matrices (source: https://oreil.ly/5jzLn) ... Such gains typically come from a new hardware generation—or months of low-level CUDA tuning. Yet, in this case, the AI found a better way mathematically in a relatively short amount of time."
  },
  {
    "file_name": "aisp_2003.png",
    "caption": "Diagram showing the iterative process of generating GPU-optimized kernels using DeepSeek-R1 on Hopper GPUs, involving prompts, verification, and refinement until criteria are met.",
    "description": "Diagram showing the iterative process of generating GPU-optimized kernels using DeepSeek-R1 on Hopper GPUs, involving prompts, verification, and refinement until criteria are met.",
    "chapter": "Automated GPU Kernel Optimizations with DeepSeek-R1 (NVIDIA)",
    "page_context": "... ... ``` ... This feedback loop provides guidance for an improved prompt to use for the next kernel-code iteration. The loop continues until the code meets the given criteria, as shown in Figure 20-3. ... ###### Figure 20-3. Inference-time scaling with DeepSeek-R1 on the NVIDIA Hopper platform (source: Automating GPU Kernel Generation with DeepSeek-R1 and Inference Time Scaling | NVIDIA Technical Blog) ... The following prompt was used:"
  },
  {
    "file_name": "aisp_2004.png",
    "caption": "Bar chart comparing performance speedup of automatically generated attention kernels using NVIDIA’s DeepSeek-R1 with PyTorch FlexAttention across various attention patterns, showing NVIDIA's kernels achieve up to 2.1× speedup.",
    "description": "Bar chart comparing performance speedup of automatically generated attention kernels using NVIDIA’s DeepSeek-R1 with PyTorch FlexAttention across various attention patterns, showing NVIDIA's kernels achieve up to 2.1× speedup.",
    "chapter": "Automated GPU Kernel Optimizations with DeepSeek-R1 (NVIDIA)",
    "page_context": "Please provide the complete updated kernel code that incorporates these changes, ensuring that the relative positional encoding is applied efficiently within the kernel operations. ... With this prompt, the AI produced a functionally correct CUDA kernel for attention. (Note that 1.44269504 = 1/ln(2). Using this value, the prompt scales the relative-position term accordingly when forming qk. In addition to correctness, the generated kernel also achieved a 1.1–2.1× speedup over the built-in PyTorch FlexAttention API. Figure 20-4 shows the performance comparison between the generated kernel and PyTorch’s optimized FlexAttention across various attention patterns, including causal masks and long-document masks. ... ###### Figure 20-4. Automatically generated attention kernels achieved 1.1×–2.1× speedups compared to PyTorch FlexAttention (source: Automating GPU Kernel Generation with DeepSeek-R1 and Inference Time Scaling | NVIDIA Technical Blog) ... Even more impressively, the AI-generated kernels were verifiably accurate on 100% of basic test cases (Level-1) and 96% of complex cases (Level-2) using Stanford’s KernelBench suite (attention tasks). This essentially matches the reliability of a human engineer."
  },
  {
    "file_name": "aisp_2005.png",
    "caption": "Diagram illustrating the process of assigning a reward in an RL-based system for generating efficient Triton kernels, highlighting steps to prevent reward hacking and ensure kernel correctness.",
    "description": "Diagram illustrating the process of assigning a reward in an RL-based system for generating efficient Triton kernels, highlighting steps to prevent reward hacking and ensure kernel correctness.",
    "chapter": "Reinforcement Learning Approach to Generating Optimized GPU Kernels (Predibase)",
    "page_context": "To do this, Predibase created a reward function to guide the model to continuously generate better code using reinforcement learning. Specifically, the LLM would first generate a candidate kernel. The system would automatically compile and test the kernel for correctness and speed. The model then received a positive reward if the kernel ran without errors, produced the right results, and ran faster than the baseline kernel, as shown in Figure 20-5. ... Through many iterations of this RL-based trial-and-error approach, the model steadily improved. Within a few days of training, the AI went from near-0% success to producing working kernels ~40% of the time after only 5,000 training steps. Some of the generated Triton kernels ran up to 3× faster than baseline. Additionally, the model continued to improve as training progressed. ... ###### Figure 20-5. Assigning an RL-based reward for generating correct and high-performing OpenAI Triton code (relative to a baseline) (source: https://oreil.ly/JBxdW) ... This outcome shows that an AI can optimize code by testing, observing feedback, and making adjustments. This is similar to how engineers iteratively refine their code. Reinforcement learning can align AI-generated code with real-world performance metrics by rewarding both correctness and speed. This prompts the AI to explore optimizations like using warp-level parallelism or minimizing global memory access to improve overall performance."
  },
  {
    "file_name": "aisp_2006.png",
    "caption": "Diagram comparing the compute required for training GPT-3 and GPT-4 with significantly greater compute anticipated for the future model Agent-1.",
    "description": "Diagram comparing the compute required for training GPT-3 and GPT-4 with significantly greater compute anticipated for the future model Agent-1.",
    "chapter": "Self-Improving AI Agents (AI Futures Project)",
    "page_context": "According to the AI Futures Project scenario, Agent-1 is envisioned as a self-improving model that can generate and optimize code in real time. By automating coding tasks ranging from routine debugging to complex kernel fusion, this frontier AI system reduces time-to-insight and expands the creative horizon for research engineers all across the world. Automated coding acts as a force multiplier that enables rapid iteration and allows researchers to explore more ambitious ideas with less manual overhead. ... These massive AI systems are expected to allow continuous model fine-tuning and improvement. The follow-up model, Agent-2, might be an always-learning AI that never actually finishes training. So instead of checkpointing and deploying a static model, Agent-2 is designed to update its weights every day based on freshly generated synthetic data. ... ###### Figure 20-6. Amount of compute needed to train GPT-3 and GPT-4 compared to the expected compute for the “next-generation” model called Agent-1 by the researchers at the AI Futures Project (source: https://ai-2027.com) ... This perpetual, or continual, learning process makes sure that the system stays at the cutting edge by continuously refining its performance and adapting to new information. If realized, this approach would shift us from the current paradigm of deploying statically trained and fine-tuned models."
  },
  {
    "file_name": "aisp_2007.png",
    "caption": "Diagram showing how CUDA Graphs reduce CPU-GPU synchronization overhead by launching a sequence of GPU operations more efficiently.",
    "description": "Diagram showing how CUDA Graphs reduce CPU-GPU synchronization overhead by launching a sequence of GPU operations more efficiently.",
    "chapter": "Smart Compilers and Automated Code Optimizations",
    "page_context": "Already, many optimizations that used to be coded by hand are being automated by compilers, and this trend is accelerating. Automatic kernel fusion, autotuning of kernel-launch parameters, and even numerical-precision decisions can all be delegated to compilers and AI assistants. ... Beyond kernel generation, modern frameworks are getting smarter about execution graphs and scheduling. Graph execution helps to reduce CPU-GPU synchronization overhead and opens the door to global optimizations across the whole graph. Technologies like NVIDIA’s CUDA Graphs allow capturing a sequence of GPU <span class=\"keep-together\">operations—along</span> with their dependencies—as a static graph that can then be instantiated and launched with minimal CPU overhead using the `cudaGraphInstantiate()` and `cudaGraphLaunch()` APIs, as shown in [Figure 20-7](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch20.html#ch20_figure_7_1757308079223018). ... ###### Figure 20-7. Graph execution in CUDA reduces overhead when launching multiple kernels in a sequence (source: https://oreil.ly/kxSDm) ... We’re seeing AI frameworks automatically capturing training loops and other repetitive patterns into graphs to reduce overhead. Even if the execution graph is dynamic instead of static, the framework can trace it once and then run the trace repeatedly."
  },
  {
    "file_name": "aisp_2008.png",
    "caption": "Illustration of an AI assistant monitoring a training job, highlighting a diverging loss and suggesting corrective actions.",
    "description": "Illustration of an AI assistant monitoring a training job, highlighting a diverging loss and suggesting corrective actions.",
    "chapter": "AI-Assisted Real-Time System Optimizations and Cluster Operations",
    "page_context": "In short, AI gives us an always-fresh pair of eyes for every running job and every inference server. It can monitor them, advise them, and adjust in real time. Traditional utilization metrics can be misleading. For instance, a GPU 100% busy on redundant data transfers isn’t productive. These AI-driven schedulers instead aim to maximize goodput and make sure that when a GPU is busy, it’s doing useful neural compute. This directly improves cost efficiency. ... In an AI cluster, for instance, you can use a metrics pipeline based on Prometheus to feed an LLM-based assistant that alerts when GPU memory suddenly drops due to either a potential memory leak or data stall. It can even identify likely root causes. This is the kind of tedious work that AI can help automate and run 24/7 without interruption and distraction. ... ###### Figure 20-8. AI assistant monitoring a long-running training job and suggesting actions to fix an anomaly ... Another powerful use of AI is in automated debugging and failure analysis for AI systems. When a training job fails halfway through its three-month run, a human has to read through error logs, device statistics, and perhaps even memory dumps to figure out what went wrong. Was it a hardware fault? A numerical overflow? A networking hiccup?"
  }
]